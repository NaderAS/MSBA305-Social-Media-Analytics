{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d54bb9d",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26a49c3",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7563436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from Reddit API\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "def setup_reddit_api(client_id, client_secret, user_agent):\n",
    "    \"\"\"Set up Reddit API client.\"\"\"\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "    return reddit\n",
    "\n",
    "def get_subreddit_posts(reddit, subreddit_name, limit=100):\n",
    "    \"\"\"Get posts from a subreddit.\"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "\n",
    "    for post in subreddit.hot(limit=limit):\n",
    "        posts.append({\n",
    "            'post_id': post.id,\n",
    "            'title': post.title,\n",
    "            'text': post.selftext,\n",
    "            'author': str(post.author),\n",
    "            'created_utc': post.created_utc,\n",
    "            'score': post.score,\n",
    "            'num_comments': post.num_comments,\n",
    "            'upvote_ratio': post.upvote_ratio\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(posts)\n",
    "\n",
    "def get_post_comments(reddit, post_id, limit=100):\n",
    "    \"\"\"Get comments for a specific post.\"\"\"\n",
    "    post = reddit.submission(id=post_id)\n",
    "    post.comments.replace_more(limit=0)  # Only get top-level comments\n",
    "    comments = []\n",
    "\n",
    "    for comment in post.comments[:limit]:\n",
    "        comments.append({\n",
    "            'comment_id': comment.id,\n",
    "            'post_id': post_id,\n",
    "            'author': str(comment.author),\n",
    "            'text': comment.body,\n",
    "            'score': comment.score,\n",
    "            'created_utc': comment.created_utc\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052def9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = \"YI3yiSbD9yctHZ2NMIQoZA\"\n",
    "client_secret = \"vZ1MFPXuDEXCe-NaEwEyKnDrKyaPyg\"\n",
    "user_agent = \"script : data_collection :v1 .0 (by/u/data_collection)\"\n",
    "reddit = setup_reddit_api( client_id , client_secret , user_agent )\n",
    "posts_df_reddit = get_subreddit_posts(reddit , \"datascience\" , limit =50)\n",
    "posts_df_reddit.to_csv('posts_df_reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3436924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>upvote_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1jyq1tk</td>\n",
       "      <td>Weekly Entering &amp; Transitioning - Thread 14 Ap...</td>\n",
       "      <td>\\n\\nWelcome to this week's entering &amp; transit...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>1.744603e+09</td>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1i5inrb</td>\n",
       "      <td>Weekly Entering &amp; Transitioning - Thread 20 Ja...</td>\n",
       "      <td>\\n\\nWelcome to this week's entering &amp; transit...</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>1.737349e+09</td>\n",
       "      <td>13</td>\n",
       "      <td>46</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1k26920</td>\n",
       "      <td>How do you go about memorizing all the ML algo...</td>\n",
       "      <td>I’ve been preparing for interviews lately, but...</td>\n",
       "      <td>Lamp_Shade_Head</td>\n",
       "      <td>1.744986e+09</td>\n",
       "      <td>65</td>\n",
       "      <td>36</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1k26kp3</td>\n",
       "      <td>What’s your 2025 data science coding stack + A...</td>\n",
       "      <td>Curious how others are working these days. Wha...</td>\n",
       "      <td>Zuricho</td>\n",
       "      <td>1.744987e+09</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1k2a8t6</td>\n",
       "      <td>Forecasting: Principles and Practice, the Pyth...</td>\n",
       "      <td></td>\n",
       "      <td>Sampo</td>\n",
       "      <td>1.744997e+09</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                              title  \\\n",
       "0  1jyq1tk  Weekly Entering & Transitioning - Thread 14 Ap...   \n",
       "1  1i5inrb  Weekly Entering & Transitioning - Thread 20 Ja...   \n",
       "2  1k26920  How do you go about memorizing all the ML algo...   \n",
       "3  1k26kp3  What’s your 2025 data science coding stack + A...   \n",
       "4  1k2a8t6  Forecasting: Principles and Practice, the Pyth...   \n",
       "\n",
       "                                                text           author  \\\n",
       "0   \\n\\nWelcome to this week's entering & transit...    AutoModerator   \n",
       "1   \\n\\nWelcome to this week's entering & transit...    AutoModerator   \n",
       "2  I’ve been preparing for interviews lately, but...  Lamp_Shade_Head   \n",
       "3  Curious how others are working these days. Wha...          Zuricho   \n",
       "4                                                               Sampo   \n",
       "\n",
       "    created_utc  score  num_comments  upvote_ratio  \n",
       "0  1.744603e+09      9            43          0.91  \n",
       "1  1.737349e+09     13            46          0.94  \n",
       "2  1.744986e+09     65            36          0.93  \n",
       "3  1.744987e+09     41            16          0.88  \n",
       "4  1.744997e+09     10             1          0.92  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df_reddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00749389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieved 18 comments for post: 1jyq1tk\n",
      "✅ Retrieved 24 comments for post: 1i5inrb\n",
      "✅ Retrieved 23 comments for post: 1k26920\n",
      "✅ Retrieved 12 comments for post: 1k26kp3\n",
      "✅ Retrieved 1 comments for post: 1k2a8t6\n",
      "✅ Retrieved 3 comments for post: 1k2ax74\n",
      "✅ Retrieved 9 comments for post: 1k1wu9o\n",
      "✅ Retrieved 17 comments for post: 1k22cd4\n",
      "✅ Retrieved 11 comments for post: 1k1mjok\n",
      "✅ Retrieved 3 comments for post: 1k1x464\n",
      "✅ Retrieved 1 comments for post: 1k1vo23\n",
      "✅ Retrieved 1 comments for post: 1k1lh3r\n",
      "✅ Retrieved 5 comments for post: 1k20azb\n",
      "✅ Retrieved 0 comments for post: 1k1ohsp\n",
      "✅ Retrieved 20 comments for post: 1k0zcye\n",
      "✅ Retrieved 11 comments for post: 1k0v0dc\n",
      "✅ Retrieved 68 comments for post: 1k0c459\n",
      "✅ Retrieved 21 comments for post: 1k0mdr3\n",
      "✅ Retrieved 1 comments for post: 1k0vdku\n",
      "✅ Retrieved 9 comments for post: 1k082ij\n",
      "✅ Retrieved 26 comments for post: 1jzml32\n",
      "✅ Retrieved 5 comments for post: 1jz0h1y\n",
      "✅ Retrieved 28 comments for post: 1jz4teg\n",
      "✅ Retrieved 26 comments for post: 1jyu503\n",
      "✅ Retrieved 25 comments for post: 1jyicx6\n",
      "✅ Retrieved 11 comments for post: 1jyloqi\n",
      "✅ Retrieved 40 comments for post: 1jy2pe0\n",
      "✅ Retrieved 13 comments for post: 1jxl18x\n",
      "✅ Retrieved 10 comments for post: 1jxtzs1\n",
      "✅ Retrieved 25 comments for post: 1jxdlfg\n",
      "✅ Retrieved 4 comments for post: 1jygakg\n",
      "✅ Retrieved 5 comments for post: 1jxk5za\n",
      "✅ Retrieved 3 comments for post: 1jxe7rg\n",
      "✅ Retrieved 41 comments for post: 1jwbevk\n",
      "✅ Retrieved 10 comments for post: 1jwlf3f\n",
      "✅ Retrieved 9 comments for post: 1jx5k15\n",
      "✅ Retrieved 4 comments for post: 1jwduc6\n",
      "✅ Retrieved 6 comments for post: 1jw7i9l\n",
      "✅ Retrieved 3 comments for post: 1jvrgr5\n",
      "✅ Retrieved 6 comments for post: 1jvlqx7\n",
      "✅ Retrieved 15 comments for post: 1jvcz3t\n",
      "✅ Retrieved 11 comments for post: 1jvav77\n",
      "✅ Retrieved 52 comments for post: 1juo7ue\n",
      "✅ Retrieved 17 comments for post: 1jvwexo\n",
      "✅ Retrieved 5 comments for post: 1juzclh\n",
      "✅ Retrieved 21 comments for post: 1jv4xqf\n",
      "✅ Retrieved 47 comments for post: 1jtyyc0\n",
      "✅ Retrieved 16 comments for post: 1jtoul7\n",
      "✅ Retrieved 9 comments for post: 1ju139m\n",
      "✅ Retrieved 0 comments for post: 1juvgek\n",
      "💾 All comments saved to reddit_post_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# Fetch comments for all posts in posts_df_reddit\n",
    "all_comments = []\n",
    "\n",
    "for post_id in posts_df_reddit['post_id']:\n",
    "    try:\n",
    "        comments_df = get_post_comments(reddit, post_id, limit=100)\n",
    "        all_comments.append(comments_df)\n",
    "        print(f\"✅ Retrieved {len(comments_df)} comments for post: {post_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to fetch comments for {post_id}: {e}\")\n",
    "\n",
    "# Combine all comments into a single DataFrame\n",
    "all_comments_df = pd.concat(all_comments, ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "all_comments_df.to_csv(\"reddit_post_comments.csv\", index=False)\n",
    "print(\"💾 All comments saved to reddit_post_comments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa23535d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mn0k755</td>\n",
       "      <td>1jyq1tk</td>\n",
       "      <td>Norse_af</td>\n",
       "      <td>Here is the roadmap I am starting to prep for ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.744607e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mn2syxi</td>\n",
       "      <td>1jyq1tk</td>\n",
       "      <td>Formal-Degree-1578</td>\n",
       "      <td>Hi everyone, I’m working on a project to forec...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.744645e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mndey84</td>\n",
       "      <td>1jyq1tk</td>\n",
       "      <td>tejjm9</td>\n",
       "      <td>Hi guys, I have work experience in operations ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.744788e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mn0jtkc</td>\n",
       "      <td>1jyq1tk</td>\n",
       "      <td>Norse_af</td>\n",
       "      <td>Starting a Master's Program soon.\\n\\nI applied...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.744607e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mn0vcwx</td>\n",
       "      <td>1jyq1tk</td>\n",
       "      <td>Complete-Sandwich564</td>\n",
       "      <td>\\nNew here, this may be long winded but any gu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.744614e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id  post_id                author  \\\n",
       "0    mn0k755  1jyq1tk              Norse_af   \n",
       "1    mn2syxi  1jyq1tk    Formal-Degree-1578   \n",
       "2    mndey84  1jyq1tk                tejjm9   \n",
       "3    mn0jtkc  1jyq1tk              Norse_af   \n",
       "4    mn0vcwx  1jyq1tk  Complete-Sandwich564   \n",
       "\n",
       "                                                text  score   created_utc  \n",
       "0  Here is the roadmap I am starting to prep for ...      2  1.744607e+09  \n",
       "1  Hi everyone, I’m working on a project to forec...      2  1.744645e+09  \n",
       "2  Hi guys, I have work experience in operations ...      2  1.744788e+09  \n",
       "3  Starting a Master's Program soon.\\n\\nI applied...      1  1.744607e+09  \n",
       "4  \\nNew here, this may be long winded but any gu...      1  1.744614e+09  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c00a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MaisoonAboFakher\\AppData\\Local\\Temp\\ipykernel_21360\\875011887.py:162: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(insert_to_neo4j)\n",
      "C:\\Users\\MaisoonAboFakher\\AppData\\Local\\Temp\\ipykernel_21360\\875011887.py:163: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(recreate_engagements, all_comments_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Full Reddit knowledge graph recreated in Neo4j, including ENGAGED_WITH links.\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- CONFIG --- #\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"12345678\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# --- Generate Fake User Info --- #\n",
    "def generate_users(df, posts_df):\n",
    "    usernames = pd.concat([df['author'], posts_df['author']]).dropna().unique()\n",
    "    users = []\n",
    "    for name in usernames:\n",
    "        users.append({\n",
    "            'user_id': f\"U_{name}\",\n",
    "            'username': name,\n",
    "            'join_date': (datetime.today() - timedelta(days=random.randint(100, 3000))).strftime('%Y-%m-%d'),\n",
    "            'follower_count': random.randint(0, 10000),\n",
    "            'verified': random.choice([True, False])\n",
    "        })\n",
    "    return pd.DataFrame(users)\n",
    "\n",
    "# --- Assign Topics Based on Post Title --- #\n",
    "def assign_topics(posts_df):\n",
    "    keywords = {\n",
    "        \"book\": \"Books\",\n",
    "        \"transition\": \"Career Advice\",\n",
    "        \"engineer\": \"Engineering\",\n",
    "        \"forecast\": \"Forecasting\",\n",
    "        \"data\": \"Data Science\"\n",
    "    }\n",
    "    topic_map = {}\n",
    "    posts_df['topic'] = \"Other\"\n",
    "    for idx, row in posts_df.iterrows():\n",
    "        for k, v in keywords.items():\n",
    "            if k in row['title'].lower():\n",
    "                posts_df.at[idx, 'topic'] = v\n",
    "                if v not in topic_map:\n",
    "                    topic_map[v] = {\n",
    "                        'topic_id': f\"T_{len(topic_map)+1}\",\n",
    "                        'name': v,\n",
    "                        'popularity_score': random.randint(1, 100)\n",
    "                    }\n",
    "                break\n",
    "    return posts_df, pd.DataFrame(topic_map.values())\n",
    "\n",
    "# --- Recreate ENGAGED_WITH relationships --- #\n",
    "def recreate_engagements(tx, comments_df):\n",
    "    for _, row in comments_df.iterrows():\n",
    "        tx.run(\"\"\"\n",
    "            MATCH (u:User {username: $author})\n",
    "            MATCH (p:Post {post_id: $post_id})\n",
    "            MERGE (u)-[:ENGAGED_WITH {\n",
    "                engagement_type: 'comment',\n",
    "                engagement_date: date(datetime({epochSeconds: toInteger($created_utc)}))\n",
    "            }]->(p)\n",
    "        \"\"\", {\n",
    "            \"author\": row[\"author\"],\n",
    "            \"post_id\": row[\"post_id\"],\n",
    "            \"created_utc\": int(row[\"created_utc\"])\n",
    "        })\n",
    "\n",
    "# --- Generate Final DataFrames --- #\n",
    "users_df = generate_users(all_comments_df, posts_df_reddit)\n",
    "posts_df_reddit, topics_df = assign_topics(posts_df_reddit)\n",
    "\n",
    "# --- Neo4j Insertion Function --- #\n",
    "def insert_to_neo4j(tx):\n",
    "    # Platform node\n",
    "    tx.run(\"\"\"\n",
    "        MERGE (p:Platform {platform_id: 'reddit'})\n",
    "        SET p.name = 'Reddit',\n",
    "            p.monthly_active_users = 430000000\n",
    "    \"\"\")\n",
    "\n",
    "    # Users\n",
    "    for _, row in users_df.iterrows():\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (u:User {user_id: $user_id})\n",
    "            SET u.username = $username,\n",
    "                u.join_date = date($join_date),\n",
    "                u.follower_count = $follower_count,\n",
    "                u.verified = $verified\n",
    "        \"\"\", **row)\n",
    "\n",
    "    # Topics\n",
    "    for _, row in topics_df.iterrows():\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (t:Topic {topic_id: $topic_id})\n",
    "            SET t.name = $name,\n",
    "                t.popularity_score = $popularity_score\n",
    "        \"\"\", **row)\n",
    "\n",
    "    # Posts and Relationships\n",
    "    for _, row in posts_df_reddit.iterrows():\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (post:Post {post_id: $post_id})\n",
    "            SET post.timestamp = datetime({epochSeconds: toInteger($timestamp)}),\n",
    "                post.content_type = 'text',\n",
    "                post.like_count = $score,\n",
    "                post.share_count = $num_comments\n",
    "\n",
    "            WITH post\n",
    "            MATCH (u:User {username: $author})\n",
    "            MERGE (u)-[:CREATED {creation_date: date(datetime({epochSeconds: toInteger($timestamp)}))}]->(post)\n",
    "\n",
    "            WITH post\n",
    "            MATCH (t:Topic {name: $topic})\n",
    "            MERGE (post)-[:TAGGED_WITH {relevance_score: 0.9}]->(t)\n",
    "\n",
    "            WITH post\n",
    "            MATCH (p:Platform {platform_id: 'reddit'})\n",
    "            MERGE (post)-[:POSTED_ON]->(p)\n",
    "        \"\"\", {\n",
    "            'post_id': row['post_id'],\n",
    "            'timestamp': int(row['created_utc']),\n",
    "            'score': int(row['score']),\n",
    "            'num_comments': int(row['num_comments']),\n",
    "            'author': row['author'],\n",
    "            'topic': row['topic']\n",
    "        })\n",
    "\n",
    "    # Comments (CREATED and COMMENTED_ON)\n",
    "    for _, row in all_comments_df.iterrows():\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (c:Comment {comment_id: $comment_id})\n",
    "            SET c.text = $text,\n",
    "                c.score = $score,\n",
    "                c.timestamp = datetime({epochSeconds: toInteger($created_utc)})\n",
    "\n",
    "            WITH c\n",
    "            MATCH (u:User {username: $author})\n",
    "            MERGE (u)-[:CREATED]->(c)\n",
    "\n",
    "            WITH c\n",
    "            MATCH (p:Post {post_id: $post_id})\n",
    "            MERGE (c)-[:COMMENTED_ON]->(p)\n",
    "        \"\"\", {\n",
    "            'comment_id': row['comment_id'],\n",
    "            'text': row['text'],\n",
    "            'score': int(row['score']),\n",
    "            'created_utc': int(row['created_utc']),\n",
    "            'post_id': row['post_id'],\n",
    "            'author': row['author']\n",
    "        })\n",
    "\n",
    "    # LIKES_SIMILAR_TOPICS\n",
    "    for topic in topics_df['name']:\n",
    "        authors = posts_df_reddit[posts_df_reddit['topic'] == topic]['author'].unique()\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                tx.run(\"\"\"\n",
    "                    MATCH (a:User {username: $u1}), (b:User {username: $u2})\n",
    "                    MERGE (a)-[:LIKES_SIMILAR_TOPICS]->(b)\n",
    "                \"\"\", {'u1': authors[i], 'u2': authors[j]})\n",
    "\n",
    "# --- Run All Insertions --- #\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(insert_to_neo4j)\n",
    "    session.write_transaction(recreate_engagements, all_comments_df)\n",
    "\n",
    "print(\"✅ Full Reddit knowledge graph recreated in Neo4j, including ENGAGED_WITH links.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4245c3ae",
   "metadata": {},
   "source": [
    "#### 📊 CYTHER ANALYSIS QUERIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631dfe46",
   "metadata": {},
   "source": [
    "1. Who are the most influential users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fd7f44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MaisoonAboFakher\\AppData\\Local\\Temp\\ipykernel_21360\\2596408958.py:14: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  results = session.read_transaction(most_influential_users)\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\n    MATCH (u:User)\\n    OPTIONAL MATCH (u)-[:CREATED]->(p:Post)\\n    OPTIONAL MATCH (p)<-[:ENGAGED_WITH]-(e:User)\\n    WITH u.username AS user, u.follower_count AS followers, COUNT(DISTINCT e) AS total_engagement\\n    RETURN user, followers, total_engagement, (followers + total_engagement) AS influence_score\\n    ORDER BY influence_score DESC\\n    LIMIT 10\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record user='Trungyaphets' followers=9994 total_engagement=0 influence_score=9994>\n",
      "<Record user='crazyeddie_farker' followers=9963 total_engagement=0 influence_score=9963>\n",
      "<Record user='Beneficial_Phase2366' followers=9963 total_engagement=0 influence_score=9963>\n",
      "<Record user='Vampy04' followers=9961 total_engagement=0 influence_score=9961>\n",
      "<Record user='forbiscuit' followers=9926 total_engagement=0 influence_score=9926>\n",
      "<Record user='wang-bang' followers=9919 total_engagement=5 influence_score=9924>\n",
      "<Record user='PhitPhil' followers=9917 total_engagement=0 influence_score=9917>\n",
      "<Record user='Aromatic-Box683' followers=9902 total_engagement=0 influence_score=9902>\n",
      "<Record user='MyKo101' followers=9883 total_engagement=4 influence_score=9887>\n",
      "<Record user='next-choken' followers=9842 total_engagement=0 influence_score=9842>\n"
     ]
    }
   ],
   "source": [
    "def most_influential_users(tx):\n",
    "    query = \"\"\"\n",
    "    MATCH (u:User)\n",
    "    OPTIONAL MATCH (u)-[:CREATED]->(p:Post)\n",
    "    OPTIONAL MATCH (p)<-[:ENGAGED_WITH]-(e:User)\n",
    "    WITH u.username AS user, u.follower_count AS followers, COUNT(DISTINCT e) AS total_engagement\n",
    "    RETURN user, followers, total_engagement, (followers + total_engagement) AS influence_score\n",
    "    ORDER BY influence_score DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    return list(tx.run(query))\n",
    "\n",
    "with driver.session() as session:\n",
    "    results = session.read_transaction(most_influential_users)\n",
    "    for r in results:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceceab9",
   "metadata": {},
   "source": [
    "📌 2. Topics with Highest Engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2bff798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MaisoonAboFakher\\AppData\\Local\\Temp\\ipykernel_21360\\419821096.py:12: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  results = session.read_transaction(top_engaging_topics)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record topic='Data Science' engagement_count=243>\n",
      "<Record topic='Engineering' engagement_count=232>\n",
      "<Record topic='Career Advice' engagement_count=55>\n",
      "<Record topic='Books' engagement_count=32>\n",
      "<Record topic='Forecasting' engagement_count=21>\n"
     ]
    }
   ],
   "source": [
    "def top_engaging_topics(tx):\n",
    "    query = \"\"\"\n",
    "    MATCH (t:Topic)<-[:TAGGED_WITH]-(p:Post)<-[:ENGAGED_WITH]-(u:User)\n",
    "    WITH t.name AS topic, COUNT(u) AS engagement_count\n",
    "    RETURN topic, engagement_count\n",
    "    ORDER BY engagement_count DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    return list(tx.run(query))\n",
    "\n",
    "with driver.session() as session:\n",
    "    results = session.read_transaction(top_engaging_topics)\n",
    "    for r in results:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466f8d0",
   "metadata": {},
   "source": [
    "📌 3. Best Content Types by Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092a7cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record platform='Reddit' content_type='text' avg_engagement=118.16666666666667>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MaisoonAboFakher\\AppData\\Local\\Temp\\ipykernel_21360\\439653468.py:11: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  results = session.read_transaction(best_content_types)\n"
     ]
    }
   ],
   "source": [
    "def best_content_types(tx):\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Post)-[:POSTED_ON]->(platform:Platform)\n",
    "    WITH platform.name AS platform, p.content_type AS content_type, p.like_count + p.share_count AS total_engagement\n",
    "    RETURN platform, content_type, AVG(total_engagement) AS avg_engagement\n",
    "    ORDER BY avg_engagement DESC\n",
    "    \"\"\"\n",
    "    return list(tx.run(query))\n",
    "\n",
    "with driver.session() as session:\n",
    "    results = session.read_transaction(best_content_types)\n",
    "    for r in results:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354870ae",
   "metadata": {},
   "source": [
    "📌 4. User Communities via Shared Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b43ea44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Record user1='Emuthusiast' user2='khaili109'>\n",
      "<Record user1='FilmIsForever' user2='Particular_Reality12'>\n",
      "<Record user1='FilmIsForever' user2='SingerEast1469'>\n",
      "<Record user1='FilmIsForever' user2='Starktony11'>\n",
      "<Record user1='FilmIsForever' user2='Suspicious_Jacket463'>\n",
      "<Record user1='FilmIsForever' user2='chrisgarzon19'>\n",
      "<Record user1='FilmIsForever' user2='etherealcabbage72'>\n",
      "<Record user1='FilmIsForever' user2='guna1o0'>\n",
      "<Record user1='FilmIsForever' user2='vintagefiretruk'>\n",
      "<Record user1='Particular_Reality12' user2='chrisgarzon19'>\n",
      "<Record user1='Particular_Reality12' user2='guna1o0'>\n",
      "<Record user1='Particular_Reality12' user2='vintagefiretruk'>\n",
      "<Record user1='Sampo' user2='Admirable_Creme1276'>\n",
      "<Record user1='SingerEast1469' user2='Particular_Reality12'>\n",
      "<Record user1='SingerEast1469' user2='chrisgarzon19'>\n",
      "<Record user1='SingerEast1469' user2='guna1o0'>\n",
      "<Record user1='SingerEast1469' user2='vintagefiretruk'>\n",
      "<Record user1='Starktony11' user2='Particular_Reality12'>\n",
      "<Record user1='Starktony11' user2='SingerEast1469'>\n",
      "<Record user1='Starktony11' user2='chrisgarzon19'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MaisoonAboFakher\\AppData\\Local\\Temp\\ipykernel_21360\\903457670.py:11: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  results = session.read_transaction(user_communities)\n"
     ]
    }
   ],
   "source": [
    "def user_communities(tx):\n",
    "    query = \"\"\"\n",
    "    MATCH (u1:User)-[:LIKES_SIMILAR_TOPICS]->(u2:User)\n",
    "    RETURN u1.username AS user1, u2.username AS user2\n",
    "    ORDER BY user1, user2\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "    return list(tx.run(query))\n",
    "\n",
    "with driver.session() as session:\n",
    "    results = session.read_transaction(user_communities)\n",
    "    for r in results:\n",
    "        print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
