post_id,title,text,author,created_utc,score,num_comments,upvote_ratio,content_type
1k44mgg,"Weekly Entering & Transitioning - Thread 21 Apr, 2025 - 28 Apr, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1745208103.0,7,27,1.0,text
1i5inrb,"Weekly Entering & Transitioning - Thread 20 Jan, 2025 - 27 Jan, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1737349304.0,13,46,1.0,text
1k6tz9y,Leadership said they doesn‚Äôt understand what we do,"Our DS group was moved under a traditional IT org that is totally focused on delivery. We saw signs that they didn‚Äôt understand prework required to do the science side of the job, get the data clean, figure out the right features and models, etc. 

We have been briefing leadership on projects, goals, timelines. Seemed like they got it. Now they admit to my boss they really don‚Äôt understand what our group does at all. 

Very frustrating. Anyone else have this situation",DeepNarwhalNetwork,1745505817.0,149,63,0.94,text
1k6wi45,"What are some universities that you believe are ""Cash-Cows""",,Voldemort57,1745511916.0,61,111,0.84,other
1k6za0y,Signs of burnout?,"Hey all,

I posted a little bit about my current job situation in a previous post: [https://www.reddit.com/r/datascience/comments/1javfus/do\_you\_deal\_with\_unrealistic\_expectations\_from/](https://www.reddit.com/r/datascience/comments/1javfus/do_you_deal_with_unrealistic_expectations_from/)

Ever since the year started, I've just been looped into tasks where I have no context what it's supposed to do, don't have the requirements clear, frequently have my boss try to get something out without clear requirements and then us fixing it after the fact with another co-worker constantly expressing dissapointment and frustration for things not churning out sooner.

For the past month, I've been working several 12-14 hour shifts. On days when I don't have quick turnaround times, I've noticed myself losing focus, losing interest in the work overall. I signed up for a bunch of Udemy classes in the beginning of the year and feel like my headspace isn't there to upskill even though I had a lot of enthusiasm before. 

Has anybody gone through this situation and have advice? I want to change my job eventually in a few months, but I want to spend time preparing rather than just jump ship at the moment, esp in this market. ",thro0away12,1745518614.0,25,12,0.84,text
1k76c0v,Step in the right or wrong direction long term?,"I‚Äôm a sophomore double majoring in Data Analytics and Data Engineering with a minor in Computer Science. (It sounds like a lot, but I came in with an associate‚Äôs degree from high school, so it‚Äôs honestly not a ton)

My end goal is to become a Data Scientist, ideally specializing in time-series forecasting or recommendation systems. I plan to go straight into a Master‚Äôs in Data Science after undergrad.

Today, I just got an offer for a Business Analyst Internship. The role focuses heavily on SQL and Power BI, but doesn‚Äôt involve any Python, machine learning, or advanced statistics. It‚Äôs a great opportunity and I‚Äôd be working with a Business Analytics team at a credit union, but I‚Äôm a bit torn.

Will having ‚ÄúBusiness Analyst Intern‚Äù on my resume make me look less competitive for future data science internships or full-time roles‚Äîespecially compared to students who land internships with ‚ÄúData Scientist‚Äù or ‚ÄúData Science Intern‚Äù in the title?

I know I‚Äôm only a sophomore, and I don‚Äôt want to overthink it, but I also don‚Äôt want to unintentionally steer myself toward an analyst-only path.

Any advice or insight would be appreciated!",LilParkButt,1745536561.0,2,32,0.54,text
1k6sdkm,"Does anyone here do Data Science/Machine Learning at Walgreens? If so, what's it like?","My parents live in the Chicagoland area and I'm considering moving back home. I've been a data scientist at my current company for about 1.5 years now, primarily doing either ML builds (but not deployment, that's another role at my company) or more classical statistical analyses to aid in decision making. I have a location requirement where I work currently, and while I've been given feedback that I'm a strong performer, I don't anticipate being granted permission to work remotely.

I've been looking into the companies in the area and Walgreens is one of the ones I'm considering, but in addition to the current acquisition they're undergoing, I'm hearing some odd things about their data science group - however it looks like there's ML roles open in the area. I'm wondering if there's anyone who works there that would be open to just a quick conversation about how those roles look there so I can better understand if it's a viable option for me.",SkipGram,1745501744.0,8,7,0.65,text
1k6rj0y,Deep Analysis‚Ää‚Äî‚Ääthe analytics analogue to deep research,,phicreative1997,1745499402.0,9,0,0.85,link
1k6pqem,Polars: what is the status of compatibility with other Python packages?,,AMGraduate564,1745493737.0,7,4,0.89,other
1k63zii,"To Interviewers who ask product metrics  cases study, what makes you say yes or no to a candidate, do you want complex metrics? Or basic works too?","Hi, 
I was curious to know if you are an interviewer, lest say at faang or similar big tech, what makes you feel yes this is good candidate and we can hire, what are the deal breakers or something that impress you or think that a red flag? 

Like you want them to think about out of box metrics, or complex metrics or even basic engagement metrics like DAUs, conversions rates, view rates, etc are good enough? Also, i often see people mention a/b test whenever the questions asked so do you want them to go on deep in it?  Or anything you look them to answer? Also, how long do you want the conversation to happen?

Edit- also anything you think that makes them stands out or topics they mention make them stands out? ",Starktony11,1745426843.0,46,11,0.86,text
1k60gey,How can I come up with better feature ideas?,"I'm currently working on a credit scoring model. I have tried various feature engineering approaches using my domain knowledge, and my manager has also shared some suggestions. Additionally, I‚Äôve explored several feature selection techniques. However, the model's performance still isn't meeting my manager‚Äôs expectations.

At this point, I‚Äôve even tried manually adding and removing features step by step to observe any changes in performance. I understand that modeling is all about domain knowledge, but I can't help wishing there were a magical tool that could suggest the best feature ideas.",guna1o0,1745418250.0,17,15,0.84,text
1k5ikzd,How is your teaming using AI for DS?,"I see a lot of job posting saying ‚Äúleverage AI to add value‚Äù. What does this actually mean? Using AI to complete DS work or is AI is an extension of DS work?

I‚Äôve seen a lot of cool is cases outside of DS like content generation or agents but not as much in DS itself. Mostly just code assist of document creation/summary which is a tool to help DS but not DS itself.",Trick-Interaction396,1745358495.0,66,46,0.85,text
1k4geso,Ever met a person you think lied about working in Data Science?,"You ever get the feeling someone online or in-person just straight up lied to you about having a Data Science job (Data Scientist, Data Analyst, Data Engineer, Machine Learning Engineer, Data Architect, etc.)?

I was recently talking to someone at a technical meet-up for working professionals and one person was saying some really weird stuff. It was like they had heard of the technical terms before, but didn't actually have the experience working with the technologies/skills. For example, they mentioned that they had ""All sorts of experience with Kafka"" but didn't know that it is a tool that Data Engineers and related professionals could use for their workflows. They also mixed up the definitions of common machine learning models, what said models could do for a business, NoSQL & SQL, etc. It was jarring.

Also, sometimes I get the impression that a minority of people on this subreddit come on and lie about ever having a Data Science job. The more obvious examples are those who post the Chat-GPT answers to post questions. No shade thrown to anyone here. I encounter many qualified people here and have learned new stuff just reading through posts.

Any of you ever had an experience like that?

**Edit:** Hello all. Thank you for all of the responses on this post. I have gotten some good perspective, some hilarious comments, and some cool advice. I appreciate all of you on this sub-reddit.

I do want to say that I do not believe that all Data Scientists need to know Kafka (or any other specific tech. I don't know a bunch of stuff). I brought up the Kafka example because it was the most egregious (the person claimed to have all these years of experience, but didn't know a bunch of stuff including the basics). The conversation was 35 minutes, so I only wanted to bring up the outliers/notable examples.

And I want to emphasize that I was talking about **all** Data Science jobs (Data Scientist, Data Analyst, Data Engineer, Machine Learning Engineer, Data Architect, etc.). Because I think that these are all valid roles and that we all have unique experiences, skills, and knowledge to bring to this field.

Anyways, I appreciate all the comments and I will read through them after work.",NerdyMcDataNerd,1745249434.0,257,153,0.9,text
1k4q8b8,In an effort to keep learning,"I have a new DS starting soon...modalities change and all of that, more importantly, for those of you hired in the last year, what are some things you wish were presented earlier than they were ( or things done in general)? Looking to make this a very positive experience for the new employee.",zangler,1745273542.0,23,18,0.85,text
1k4u3dp,Any experience with Incrmntal for marketing studies?,"My firm was contacted by a marketing measurement company called Incrmntal. Their product is an MMM that uses interrupted time series (i.e. synthetic control) with a reinforcement learning step. Their documentation is very light. There are no simulation studies and just a handful of comparisons with A/B tests. It's not clear what the reinforcement learning process is, if it's there at all, and the time series model is similarly opaque. The whole thing seems pretty scammy. The marketing materials are fairly aggressive and make repeatedly inaccurate claims.

Has anyone used them? Any insights into what they're doing? How well did it work for you?",Lanky-Question2636,1745284402.0,7,3,0.82,text
1k52w1u,Request for Review,,essenkochtsichselbst,1745317399.0,0,6,0.37,link
1k3nxj7,"Pandas, why the hype?","I'm an R user and I'm at the point where I'm not really improving my programming skills all that much, so I finally decided to learn Python in earnest. I've put together a few projects that combine general programming, ML implementation, and basic data analysis. And overall, I quite like python and it really hasn't been too difficult to pick up. And the few times I've run into an issue, I've generally blamed it on R (e.g . the day I learned about mutable objects was a frustrating one). However, basic analysis - like summary stats - feels impossible.

All this time I've heard Python users hype up pandas. But now that I am actually learning it, I can't help think why? Simple aggregations and other tasks require so much code. But more confusng is the syntax, which seems to be odds with itself at times.  Sometimes we put the column name in the parentheses of a function, other times be but the column name in brackets before the function. Sometimes we call the function normally (e.g.mean()), other times it is contain by quotations. The whole thing reminds me of the Angostura bitters bottle story, where one of the brothers designed the bottles and the other designed the label without talking to one another.

Anyway, this wasn't really meant to be a rant. I'm sticking with it, but does it get better? Should I look at polars instead?

To R users, everyone needs to figure out what Hadley Wickham drinks and send him a case of it.",gonna_get_tossed,1745159796.0,388,214,0.88,text
1k3e4nb,Unit tests,Serious question: Can anyone provide a real example of a series of unit tests applied to an MLOps flow? And when or how often do these unit tests get executed and who is checking them? Sorry if this question is too vague but I have never been presented an example of unit tests in production data science applications.,genobobeno_va,1745121827.0,39,28,0.86,text
1k32lrl,"Python users, which R packages do you use, if any?","I'm currently writing an R package called [rixpress](https://github.com/b-rodrigues/rixpress) which aims to set up reproducible pipelines with simple R code by using Nix as the underlying build tool. Because it uses Nix as the build tool, it is also possible to write targets that are built using Python.
[Here is an example of a pipeline that mixes R and Python.](https://github.com/b-rodrigues/rixpress_demos/blob/master/python_r/gen-pipeline.R)

I think rixpress can be quite useful to Python users as well (and I might even translate the package to Python in the future), and I'm looking for examples of Python users that need to also work with certain R packages. These examples would help me make sure that passing objects from and between the two languages can be as seamless as possible.

So Python data scientists, which R packages do you use, if any? 
",brodrigues_co,1745086504.0,105,76,0.9,text
1k3jt7b,Is there something similar tailored for Data Science interviews? | asking on behalf of my friend,,guna1o0,1745145771.0,4,1,0.7,other
1k2y84g,Data science content gap,"I‚Äôm trying to get back into the habit of writing data science articles. I can cover a wide range of topics, including A/B testing, causal inference, and model development and deployment. I‚Äôd love to hear from this community‚Äîwhat kinds of articles or posts would be most valuable to you? I know there‚Äôs already a lot of content out there, and I‚Äôm to understand I‚Äôm writing something people find valuable. 

Edit thanks for the response: 

I‚Äôve learned that people want to see more real-world data science applications. Here are a few topics I could write about:

	‚Ä¢	Using time series forecasting to determine the best location for building a hydro power plant
	‚Ä¢	Developing top-line KPI metrics to track product or business health
	‚Ä¢	Modeling CLV for B2B businesses, especially where most revenue comes from a few accounts
	‚Ä¢	Applying quasi-experiments to measure the impact of marketing campaigns
	‚Ä¢	Prioritizing different GenAI opportunities 
	‚Ä¢	Detecting survey fraud by analyzing mouse movement
      - developing a full end-to- end modeling. ",da_chosen1,1745074906.0,55,36,0.87,text
1k33k6t,Finally releasing the Bambu‚ÄØTimelapse Dataset ‚Äì open video data for print‚Äëfailure ML (sorry for the delay!),"Hey everyone!

I know it‚Äôs been a **long minute** since my original call‚Äëfor‚Äëclips ‚Äì life got hectic and the project had to sit on the back burner a bit longer than I‚Äôd hoped. üòÖ Thanks for bearing with me!

# What‚Äôs new?

* **The dataset is live** on Hugging‚ÄØFace and ready for download or contribution.
* **First models are on the way** (starting with **build‚Äëplate identification**) ‚Äì but I can‚Äôt promise an exact release timeline yet. Life still throws curveballs!

üîó **Dataset page:** [https://huggingface.co/datasets/v2thegreat/bambu-timelapse-dataset](https://huggingface.co/datasets/v2thegreat/bambu-timelapse-dataset)

# What‚Äôs inside?

* **627 timelapse videos** from P1/X1 printers
* **81 full‚Äëlength camera recordings** straight off the printer cam
* Thumbnails + CSV metadata for quick indexing
* CC‚ÄëBY‚Äë4.0 license ‚Äì free for hobby, research, and even commercial use with proper attribution

# Why bother?

* It‚Äôs the **first fully open corpus** of Bambu timelapses; most prior failure‚Äëdetection work never shares raw data.
* Bambu‚ÄØLab printers are everywhere, so the footage mirrors real‚Äëworld conditions.
* Great sandbox for manufacturing / QA projects‚Äîfailure classification, anomaly detection, build‚Äëplate detection, and more.

# Contribute your clips

1. Open a **Pull Request** on the repo (`originals/timelapses/<your_id>/`).
2. If PRs aren‚Äôt your jam, DM me and we‚Äôll arrange a transfer link.
3. Please crop or blur anything private; aim for bed‚Äëonly views.

# Skill level

If you know some Python and basic ML, this is a perfect **intermediate** project to dive into computer vision. Total beginners can still poke around with the sample code, but training solid models will take a bit of experience.

Thanks again for everyone‚Äôs patience and for the clips already shared‚Äîcan‚Äôt wait to see what the community builds with this!",v2thegreat,1745089085.0,19,5,0.96,text
1k2igce,What SWE/AI Engineer skills in 2025 can I learn to complement Data Science?,"At my company currently - the hype is to use LLMs and GenAI at every intersection.

I have seen this means that a lot of DS work is now instead handed to SWEs, and the 'modelling' is all a GPT/API call.

Maybe this is just a feature of my company and the way they look at their tech stack, but I feel that DS is not getting as many projects and things are going to the SWEs only, as they can quickly build, and rapidly deploy into product.

I want to better learn how to integrate GenAI features/apps in our JavaScript based product, so that I can also build and integrate, and build working PoCs, rather than being trapped in notebooks. 

I'm not sure if I should just learn raw JS, because I'd even want to know how to put things into a silent test as an example, where predictions are made but no prediction is shown to the user.

Maybe the more apt title is going from a DS -> AI Engineer, and what skills to learn to get there?",sg6128,1745018155.0,78,29,0.93,text
1k2u4nd,Leverage Points for a Design Matrix with Mainly Categorial Features,"Hello! I hope this is a stupid question and gets quickly resolved. As per title, I have a design matrix with a high amount of categorial features. I am applying a linear regression model on the data set (mainly for training myself to get familiarity with linear regression). The model has a high amount of categorial features that I have one-hot encoded.

Now I try to figure out high leverage points for the design matrix. After a couple of attempts I was wondering if that would even make sense and how to evaluate if determining high leverage points would generally make sense in this scenario.

After asking ChatGPT (which provided a weird answer I know is incorrect) and searching a bit I found nothing explaining this. So, I thought I come here and ask:

* In how far does it make sense to compute/check for leverage values given that there is a high amount of categorial features?
* How to compute them? Would I use the diagonal of the HAT matrix or is there eventually another technique?

  
I am happy about any advise or hint, explanation or approach that gives me some clarity in this scenario. Thank you!!

 ",essenkochtsichselbst,1745062061.0,7,1,0.78,text
1k26kp3,What‚Äôs your 2025 data science coding stack + AI tools workflow?,"Curious how others are working these days. What‚Äôs your current setup?

IDE / notebook tools? (VS Code, Cursor, Jupyter, etc.)

Are you using AI tools like Cursor, Windsurf, Copilot, Cline, Roo?

How do they fit into your workflow? (e.g., prompting style, tasks they‚Äôre best at)

Any wins, limitations, or tips?",Zuricho,1744987285.0,172,67,0.94,text
1k2a8t6,"Forecasting: Principles and Practice, the Pythonic Way",,Sampo,1744996521.0,98,5,0.98,other
1k26920,How do you go about memorizing all the ML algorithms details for interviews?,"I‚Äôve been preparing for interviews lately, but one area I‚Äôm struggling to optimize is the ML depth rounds. Right now, I‚Äôm reviewing ISLR and taking notes, but I‚Äôm not retaining the material as well as I‚Äôd like. Even though I studied this in grad school, it‚Äôs been a while since I dove deep into the algorithmic details.  

Do you have any advice for preparing for ML breadth/depth interviews? Any strategies for reinforcing concepts or alternative resources you‚Äôd recommend? ",Lamp_Shade_Head,1744986429.0,149,65,0.94,text
1k2ax74,What does a good DS manager look like to you? How does one manage a DS project?,"Hi all, 

I have found myself numerous times in leadership roles for data science projects. I never feel that I am doing a sufficient job. I find that I either end have up doing a lot of the work on my own and failing to split up task in the data science realm. A lot of these projects, and I hate to say it like this without sounding cocky, I feel that I can do on my own from end to end. Maybe some minimal support from other teams in helping with data flow issues, etc. I'm not a manager by any means, I am individual contributor. 

For those in this subreddit who are managers, what are some ways you found success in managing data science teams and projects? For those as individual contributors, what are some things that you like to have in a data science manager?",throwaway69xx420,1744998234.0,55,20,0.96,text
1k22cd4,Working with distance,"I'm super curious about the solutions you're using to calculate distances. 

I can't share too many details, but we have data that includes two addresses and the GPS coordinates between these locations. While the results we've obtained so far are interesting, they only reflect the straight-line distance.

Google has an API that allows you to query travel distances by car and even via public transport. However, my understanding is that their terms of service restrict storing the results of these queries and the volume of the calls.

Have any of you experts explored other tools or data sources that could fulfill this need? This is for a corporate solution in the UK, so it needs to be compliant with regulations.

Edit: thanks, you guys are legends ",oryx_za,1744974637.0,16,30,0.77,text
1k1wu9o,Forecasting models for small data in operations,"Hi, I work in a company that provides a weekly service to our customers.

One of the most important things for our operations is to know 1 to 5 weeks in advance how many customers we expect to have for each of those future weeks.

Company is operating for about 4 years so there are roughly 200 historical data points.

I wonder, which data science, ML models are best for small data with some seasonal trends?

Facebook prophet, Arima and Sarima are the ones we use but it feels like we are missing some.

Any thoughts?
",Admirable_Creme1276,1744952011.0,35,43,0.9,text
1k1mjok,Lead DS book suggestions,"Ive landed my first role as a lead DS. My responsibilities outside actual DS work is upskilling the analytics team in Python, R and powerBI which I've got 5+ experience with. However, this is the first role where I'm mentoring/coaching/leading a team. I would welcome any suggestions for reading materials that would help me in this new leadership role. Thank you for your time!",citizenofme,1744920922.0,84,22,0.96,text
1k1x464,What is the difference between DiD and incremental testing? I did search online and gpt but didn‚Äôt find convincing difference,"Hi 

What is the difference between DiD and incremental testing? I did search online and gpt but didn‚Äôt find convincing difference, i don‚Äôt get it as both are basically difference between control and treatment group. If anyone could explain then would be great help. Thanks!",Starktony11,1744953050.0,13,8,0.88,text
1k1vo23,Advice before getting data engineer fellowship position,"Hey everybody,

I need some advice. I have an MsC in Data Science and have really struggled to find jobs. I got an average paying, ‚Äúdata science adjacent but not data science enough‚Äù quantitative analyst job in a bank. In fact , I feel like I get dumber every day I‚Äôm there and I‚Äôm miserable. None of the skills or achievements there are noteworthy : no model building, no big analyses, no data engineering or Gen ai work, just model validation work (helping other people fix their modeling solutions).

Long story short, I‚Äôm interviewing for a fellowship position to be a data engineer in a nonprofit. It lasts for one year and exposes me to many clients that I will aid. At most I can extend the fellowship for one additional year. It sounds exciting. It pays 10K less, but it‚Äôs a step in the right direction. It gets me closer to what I actually studied.

The reason I write this post is because I want to know if it will negatively impact my resume or future chances. If I take this job, my resume will look like this : data analyst job (3 years) with a bit of sql and excel, two data science internships (one 3 months and one 8 months) at the university, quantitative analyst (6months), data engineer fellowship (1 year). Will this make companies look at me like a problem and not give me a chance to even interview? Thanks in advance, everybody.
",Emuthusiast,1744947829.0,8,2,0.79,text
1k1lh3r,Experiences from past Open Data Science Conferences (ODSC)?,"I have an opportunity to attend ODSC East (https://odsc.com/boston/) and want to see if this is worth it as a M.S. CS graduate looking for networking and employment opportunities.

  
I am less interested in tutorials and workshops than in networking and employment. Is it worth it to show up with a resume and portfolio links looking to network?

  
I searched this sub and reviews are mixed but fairly old. Anyone gone recently?",FilmIsForever,1744918223.0,8,4,0.76,text
1k20azb,Have a lot of experience but not getting any interviews - help,"Hi,

I was here a few weeks back and you helped me to cut down my CV and demo more impact.  I have applied to jobs all over and get only rejections.

I know the market is hard right now, but I would think that I would at least get invited to have at least initial conversations.  This makes me think, there must be something really missing.  Could you tell me what you think it could be?

Due to AI hype there are a lot of postings with LLMs.  I don't have corporate experience there but I plan to do projects to learn & demo it.

This week I have lowered my salary requirements by 10k and still get rejections.

I have 2 versions - a 2 pager and a 1 pager.  Have been applying with the 2 pager mostly until now.

Am grateful for your feedback and any help you can give me

https://preview.redd.it/e4pubfms4kve1.png?width=1414&format=png&auto=webp&s=853c4ae00db446784cb42ff17048611e5fb03a81

https://preview.redd.it/mzsfifmv4kve1.png?width=1414&format=png&auto=webp&s=ca35aeac336eb834a54b55008efc51936c26658d

https://preview.redd.it/l9jz6b6w4kve1.png?width=1414&format=png&auto=webp&s=802f98f4dfdb7cc5d39346c6d1a91cf6b08b95b6

",SonicBoom_81,1744966399.0,0,19,0.44,text
1k1ohsp,Website that allow comparing VLMs and LLMs?,"I am trying to initiate a project in which I will describe images (then the descriptions will go through another pipeline). I already tested ChatGPT and saw that it was successful in giving me the description I needed. However, it is expensive and infeasible for my project (there are going to be billions of images). 

I am searching for an online platform that enables comparison of various VLM outputs. 

  
Thanks!",David202023,1744925962.0,2,1,0.76,text
1k0zcye,Data Engineer trying to understand data science to provide better support.,"I work as a data engineer who mainly builds & maintains data warehouses but now I‚Äôm starting to get projects assigned to me asking me to build custom data pipelines for various data science projects and I‚Äôm assuming deployment of Data Science/ML models to production. 

Since my background is data engineering, how can I learn data science in a structured bottom up manner so that I can best understand what exactly the data scientists want? 

This may sound like overkill to some but so far the data scientist I‚Äôm working with is trying to build a data science model that requires enriched historical data for the training of the data science model. Ok no problem so far. 

However, they then want to run the data science model on the data as it‚Äôs collected (before enrichment) but the problem is this data science model is trained on enriched historical data that wont have the exact same schema as the data that‚Äôs being collected real time?

What‚Äôs even more confusing is some data scientists have said this is ok and some said it isn‚Äôt.

I don‚Äôt know which person is right. So, I‚Äôd rather learn at least the basics, preferably through some good books & projects so that I can understand when the data scientists are asking for something unreasonable.

I need to be able to easily speak the language of data scientists so I can provide better support and let them know when there‚Äôs an issue with the data that may effect their data science model in unexpected ways. ",khaili109,1744848145.0,65,33,0.93,text
1k0v0dc,"Does anyone here work for DoorDash, Discover, Home Depot, or Liberty Mutual?",Why do you keep posting the same jobs over and over again?,Trick-Interaction396,1744836414.0,53,31,0.77,text
1k0c459,Data science is not about...,"There's a lot of posts on LinkedIn which claim:
- Data science is not about Python
- It's not about SQL
- It's not about models
- It's not about stats
...

But it's about storytelling and business value. 

There is a huge amount of people who are trying to convince everyone else in this BS, IMHO. It's just not clear why... 

Technical stuff is much more important. It reminds me of some rich people telling everyone else that money doesn't matter.  ",Suspicious_Jacket463,1744778309.0,710,164,0.87,text
1k0mdr3,Did great in the coding round but still never heard back from the HR,"I had a python and sql coding round last week. I managed to do all the questions within the given time, interviewer had to provide hint for a syntax in one of the questions but everything except that I was able to do on my own, even spoke out loud about my thought process.

At the end, the interviewer said I passed both SQL and Python and to expect to hear from HR on the next steps. To my surprise I never heard back from anyone. I can‚Äôt seem to understand what could I have done better, was requiring hint for syntax a deal breaker? It feels a bit disappointing as I don‚Äôt even know what to improve going forward. 

Based on your experience, is this a normal scenario?",Lamp_Shade_Head,1744815217.0,51,36,0.81,text
1k0vdku,Quick question regarding nested resampling and model selection workflow,"EDIT!!!!!! Post wording is confusing, when I refer to models I mean one singular model tuned N number of ways. E.g. random Forrest tuned to 4 different depths would be model a,b,c,d in my diagram.

Just wanted some feedback regarding my model selection approach.

The premise:  
Need to train dev a model and I will need to perform nested resmapling to prevent against spatial and temporal leakage.  
Outer samples will handle spatial leakage.  
Inner samples will handle temporal leakage.  
I will also be tuning a model.

Via the diagram below, my model tuning and selection will be as follows:  
\-Make inital 70/30 data budget  
\-Perfrom some number of spatial resamples (4 shown here)  
\-For each spatial resample (1-4), I will make N (4 shown) spatial splits  
\-For each inner time sample i will train and test N (4 shown) models and mark their perfromance  
\-For each outer samples' inner samples - one winner model will be selected based on some criteria  
\--e.g Model A out performs all models trained innner samples 1-4 for outer sample #1  
\----Outer/spatial #1 -- winner model A  
\----Outer/spatial #2 -- winner model D  
\----Outer/spatial #3 -- winner model C  
\----Outer/spatial #4 -- winner model A  
\-I take each winner from the previous step and train them on their entire train sets and validate on their test sets  
\--e.g train model A on outer #1 train and test on outer #1 test  
\----- train model D on outer #2 train and test on outer #2 test  
\----- and so on  
\-From this step the model the perfroms the best is then selected from these 4 and then trained on the entire inital 70% train and evalauated on the inital 30% holdout.

Should I change my method up at all?  
I was thinking that I might be adding bias in to the second modeling step (training the winning models on the outer/spatial samples) because there could be differences in the spatial samples themselves.   
Potentially some really bad data ends up exclusively in the test set for one of the outer folds and by default make one of the models not be selected that otherwise might have. 

https://preview.redd.it/kw6ogyygg9ve1.png?width=1080&format=png&auto=webp&s=7d6ac472bd91269bd9790ee3b8111b053cffa351

  
",showme_watchu_gaunt,1744837338.0,2,4,0.67,text
1k082ij,Is TimeSeriesSplit appropriate for purchase propensity prediction?‚Äù,"I have a dataset of price quotes for a service, with the following structure: client ID, quote ID, date (daily), target variable indicating whether the client purchased the service, and several features.

I'm building a model to predict the likelihood of a client completing the purchase after receiving a quote.

Does it make sense to use TimeSeriesSplit for training and validation in this case?
Would this type of problem be considered a time series problem, even though the prediction target is not a continuous time-dependent variable?",nirvana5b,1744765107.0,20,14,0.86,text
1jzml32,Is Agentic AI remotely useful for real business problems?,"Agentic AI is the latest hype train to leave the station, and there has been an explosion of frameworks, tools etc. for developing LLM-based agents. The terminology is all over the place, although the definitions in the Anthropic blog ‚ÄòBuilding Effective Agents‚Äô seem to be popular (I like them). 

Has anyone actually deployed an agentic solution to solve a business problem? Is it in production (i.e more than a PoC)? Is it actually agentic or just a workflow? I can see clear utility for open-ended web searching tasks (e.g. deep research, where the user validates everything) - but having agents autonomously navigate the internal systems of a business (and actually being useful and reliable) just seems fanciful to me, for all kinds of reasons. How can you debug these things? 

There seems to be a vast disconnect between expectation and reality, more than we‚Äôve ever seen in AI. Am I wrong?",Prize-Flow-3197,1744705044.0,88,53,0.87,text
1jz0h1y,*Saw Greg pinged me & logged off immediately*,,ElectrikMetriks,1744641544.0,489,19,0.98,image
1jz4teg,Why won‚Äôt they let you run your code!?,"So I just got done with a SQL zoom screen. I practiced for a long time on mediums and hards. One thing that threw me off was I was not allowed to run the query to see the result. The problems were medium and hard often requiring multiple joins and CTEs. 2 mediums 2 hards. 25 mins. Only got done with 3 and they wouldn‚Äôt even tell me if I was right or wrong. Just ‚Äúlogic looks sound‚Äù 

All the practice resources like leetcode and data lemur allow you to run your code. I did not expect this. Is this common practice? Definitely failed and feel totally dejected üòû ",Suspicious_Coyote_54,1744652198.0,191,40,0.94,text
1jyu503,PowerBI but not PowerBI,"Figured this was the best community to ask this question:

I have a bunch of personal data (think personal finance spreadsheet type stuff), and I'd love to build a dashboard for it - purely for me. I have access to Power BI through my work so I know how to build the sort of thing I want.

However

I obviously can't use my work account to create a personal dashboard with my personal data etc, so I'm trying to find alternative solutions.

To set up a personal PBI account seems to need a lot of hoops like owning your own domain for an email address etc, so I'm wondering if anyone in this community might use any other dashboard tools that they reccomend and that would have similar basic functionality and be a bit less faff to try and set up a personal account?",vintagefiretruk,1744620386.0,28,39,0.78,text
1jyicx6,Why are methods like forward/backward selection still taught?,"When you could just use lasso/relaxed lasso instead?


https://www.stat.cmu.edu/~ryantibs/papers/bestsubset.pdf",Loud_Communication68,1744579181.0,84,92,0.85,text
1jyloqi,Reputed Graduate Certificates?,"Since finishing my Master's in Stats 4+ years ago the field has changed a lot. I feel like my education had a lot of useless classes and missed things like bayesian, graphs, DL, big data, etc.

Stanford seems to have some good graduate certs with classes I'm interested in and my employer will cover 2/3 the costs. Are these worth taking or is there a better way to get this info online? I have 3 YOE as DS at well known companies, so will these graduate certs from reputed unis improve my resume or is it similar to coursera?",LeaguePrototype,1744588866.0,31,15,0.85,text
1jyq1tk,"Weekly Entering & Transitioning - Thread 14 Apr, 2025 - 21 Apr, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1744603309.0,9,61,0.85,text
1jy2pe0,Is a Master‚Äôs Still Necessary?,"Can I break into DS with just a bachelor‚Äôs? I have 3 YOE of relevant experience although not titled as ‚Äúdata scientist‚Äù. I always come across roles with bachelor‚Äôs as a minimum requirement but master‚Äôs as a preferred. However, I have not been picked up for an interview at all. 

I do not want to take the financial burden of a masters degree since I already have the knowledge and experience to succeed. But it feels like I am just putting myself at a disadvantage in the field. Should I just get an online degree for the masters stamp? ",Feeling_Bad1309,1744531456.0,117,120,0.81,text
1jxl18x,Ace The Interview - SQL Intuitively and Exhaustively Explained,"SQL is easy to learn and hard to master. Realistically, the difficulty of the questions you get will largely be dictated by the job role you're trying to fill.

From it's highest level, SQL is a ""declarative language"", meaning it doesn't define a set of operations, but rather a desired end result. This can make SQL incredibly expressive, but also a bit counterintuitive, especially if you aren't fully aware of it's declarative nature.

SQL expressions are passed through an SQL engine, like PostgreSQL, MySQL, and others. Thes engines parse out your SQL expressions, optimize them, and turn them into an actual list of steps to get the data you want. While not as often discussed, for beginners I recommend SQLite. It's easy to set up in virtually any environment, and allows you to get rocking with SQL quickly. If you're working in big data, I recommend also brushing up on something like PostgreSQL, but the differences are not so bad once you have a solid SQL understanding.

In being a high level declaration, SQL‚Äôs grammatical structure is, fittingly, fairly high level. It‚Äôs kind of a weird, super rigid version of English. SQL queries are largely made up of:

* **Keywords:**¬†special words in SQL that tell an engine what to do. Some common ones, which we‚Äôll discuss, are¬†`SELECT, FROM, WHERE, INSERT, UPDATE, DELETE, JOIN, ORDER BY, GROUP BY`¬†. They can be lowercase or uppercase, but usually they‚Äôre written in uppercase.
* **Identifiers:**¬†Identifiers are the names of database objects like tables, columns, etc.
* **Literals:**¬†numbers, text, and other hardcoded values
* **Operators:**¬†Special characters or keywords used in comparison and arithmetic operations. For example¬†`!=`,¬†`<`¬†,`OR`,¬†`NOT`¬†,¬†`*`,¬†`/`,¬†`%`¬†,¬†`IN`,¬†`LIKE`¬†. We‚Äôll cover these later.
* **Clauses:**¬†These are the major building block of SQL, and can be stitched together to combine a queries general behavior. They usually start with a keyword, like 
   * `SELECT`¬†‚Äì defines which columns to return 
   * `FROM`¬†‚Äì defines the source table 
   * `WHERE`¬†‚Äì filters rows 
   * `GROUP BY`¬†‚Äì groups rows etc.  

By combining these clauses, you create an SQL query

There are a ton of things you can do in SQL, like create tables:

    CREATE TABLE People(first_name, last_name, age, favorite_color)

Insert data into tables:

    INSERT INTO People
    VALUES
        ('Tom', 'Sawyer', 19, 'White'),
        ('Mel', 'Gibson', 69, 'Green'),
        ('Daniel', 'Warfiled', 27, 'Yellow')

Select certain data from tables:

    SELECT first_name, favorite_color FROM People

Search based on some filter

    SELECT * FROM People WHERE id = 3

And Delete Data

    DELETE FROM People WHERE age < 30 

What was previously mentioned makes up the cornerstone of pretty much all of SQL. Everything else builds on it, and there is a lot.

**Primary and Foreign Keys**  
A *primary key* is a unique identifier for each record in a table. A *foreign key* references a primary key in another table, allowing you to relate data across tables. This is the backbone of relational database design.

**Super Keys and Composite Keys**  
A *super key* is any combination of columns that can uniquely identify a row. When a unique combination requires multiple columns, it‚Äôs often called a *composite key* ‚Äî useful in complex schemas like logs or transactions.

**Normalization and Database Design**  
Normalization is the process of splitting data into multiple related tables to reduce redundancy. First Normal Form (1NF) ensures atomic rows, Second Normal Form (2NF) separates logically distinct data, and Third Normal Form (3NF) eliminates derived data stored in the same table.

**Creating Relational Schemas in SQLite**  
You can explicitly define tables with `FOREIGN KEY` constraints using `CREATE TABLE`. These relationships enforce referential integrity and enable behaviors like cascading deletes. SQLite enforces `NOT NULL` and `UNIQUE` constraints strictly, making your schema more robust.

**Entity Relationship Diagrams (ERDs)**  
ERDs visually represent tables and their relationships. Dotted lines and cardinality markers like `{0,1}` or `0..N` indicate how many records in one table relate to another, which helps document and debug schema logic.

**JOINs**  
JOIN operations combine rows from multiple tables using foreign keys. `INNER JOIN` includes only matched rows, `LEFT JOIN` includes all from the left table, and `FULL OUTER JOIN` (emulated in SQLite) combines both. Proper JOINs are critical for data integration.

**Filtering and LEFT/RIGHT JOIN Differences**  
JOIN order affects which rows are preserved when there‚Äôs no match. For example, using `LEFT JOIN` ensures all left-hand rows are kept ‚Äî useful for identifying unmatched data. SQLite lacks `RIGHT JOIN`, but you can simulate it by flipping the table order in a `LEFT JOIN`.

**Simulating FULL OUTER JOINs**  
SQLite doesn‚Äôt support `FULL OUTER JOIN`, but you can emulate it with a `UNION` of two `LEFT JOIN` queries and a `WHERE` clause to catch nulls from both sides. This approach ensures no records are lost in either table.

**The WHERE Clause and Filtration**  
`WHERE` filters records based on conditions, supporting logical operators (`AND`, `OR`), numeric comparisons, and string operations like `LIKE`, `IN`, and `REGEXP`. It's one of the most frequently used clauses in SQL.

**DISTINCT Selections**  
Use `SELECT DISTINCT` to retrieve unique values from a column. You can also select distinct combinations of columns (e.g., `SELECT DISTINCT name, grade`) to avoid duplicate rows in the result.

**Grouping and Aggregation Functions**  
With `GROUP BY`, you can compute metrics like `AVG`, `SUM`, or `COUNT` for each group. `HAVING` lets you filter grouped results, like showing only departments with an average salary above a threshold.

**Ordering and Limiting Results**  
`ORDER BY` sorts results by one or more columns in ascending (`ASC`) or descending (`DESC`) order. `LIMIT` restricts the number of rows returned, and `OFFSET` lets you skip rows ‚Äî useful for pagination or ranked listings.

**Updating and Deleting Data**  
`UPDATE` modifies existing rows using `SET`, while `DELETE` removes rows based on `WHERE` filters. These operations can be combined with other clauses to selectively change or clean up data.

**Handling NULLs**  
`NULL` represents missing or undefined values. You can detect them using `IS NULL` or replace them with defaults using `COALESCE`. Aggregates like `AVG(column)` ignore NULLs by default, while `COUNT(*)` includes all rows.

**Subqueries**  
Subqueries are nested `SELECT` statements used inside `WHERE`, `FROM`, or `SELECT`. They‚Äôre useful for filtering by aggregates, comparisons, or generating intermediate results for more complex logic.

**Correlated Subqueries**  
These are subqueries that reference columns from the outer query. Each row in the outer query is matched against a custom condition in the subquery ‚Äî powerful but often inefficient unless optimized.

**Common Table Expressions (CTEs)**  
CTEs let you define temporary named result sets with `WITH`. They make complex queries readable by breaking them into logical steps and can be used multiple times within the same query.

**Recursive CTEs**  
Recursive CTEs solve hierarchical problems like org charts or category trees. A base case defines the start, and a recursive step extends the output until no new rows are added. Useful for generating sequences or computing reporting chains.

**Window Functions**  
Window functions perform calculations across a set of table rows related to the current row. Examples include `RANK()`, `ROW_NUMBER()`, `LAG()`, `LEAD()`, `SUM() OVER ()`, and moving averages with sliding windows.

These all can be combined together to do a lot of different stuff.

In my opinion, this is too much to learn efficiently learn outright. It requires practice and the slow aggregation of concepts over many projects. If you're new to SQL, I recommend studying the basics and learning through doing. However, if you're on the job hunt and you need to cram, you might find this breakdown useful: [https://iaee.substack.com/p/structured-query-language-intuitively](https://iaee.substack.com/p/structured-query-language-intuitively)",Daniel-Warfield,1744474855.0,217,14,0.88,text
1jxtzs1,Which topics or questions frequently asked for a data science role in traditional banks? Or for fraud detection/risk modeling topics?,"Hi,

I am proficient with statistics(causal inference , parametric non parametric tests) and ML models, but i don‚Äôt what models, statistical techniques are used in fraud detection and risk modeling, especially in finance industry. So, could anyone suggest FAQs? Or topics i should focus more on? Or any not common topic you ask to candidates that are crucial to know? Role requires 3+ years of experience.

Also, would like to know what techniques you work on in your day to work in fraud detection. It would help me great how it works in industry and prepare for a potential interview. Thanks! 


Edit-
Would you consider it to be similar like anomaly detection in time series? If so what methods you use in your company, i know concept of a few methods like z-score, arima, sarima, med and other but would like to know in practice what you use as well

Edit 2- i am interested more on the topics that i could learn, like i know sql and python will be there",Starktony11,1744499468.0,23,16,0.8,text
1jxdlfg,Marketing Mix Models - are they really a good idea?,"hi,

I've seen a prior thread on this, but my question is more technical...

A prior company got sold a Return on Marketing Invest project by one of the big 4 consultancies.  The basis of it was build a bunch of MMMs, pump the budget in, and it automatically tells what you where to spend the budget to get the most bang for you buck.  Sounds wonderful.

I was the DS shadowing the consultancy to learn the models, so we could do a refresh.  The company had an annual marketing budget of 250m‚Ç¨ and its revenue was between 1.5 and 2bn ‚Ç¨.

Once I got into doing the refresh, I really felt the process was never going to succeed.  Marketing thought ""there's 3 years of data, we must have a good model"", but in reality 3\*52 weeks is a tiny amount of data, when you try to fit in TV, Radio, Press, OOH, Whitemail, Email, Search, Social, and then include prices from you and comp, and seasonal variables.

You need to adstock each media to take affect for lags - and finding the level of adstock requires experimentation. The 156 weeks need to have a test and possibly a validation set given the experiments.

The business is then interested in things like what happens when we do TV and OOH together, which means creating combined variables.  More variables on very little data.

I am a practical Data Scientist.  I don't get hung up on the technical details and am focused on generating value, but this whole process seemed a crazy and expensive waste of time.  

The positive that came out of it was that we started doing AB testing in certain areas where the initial models suggested there was very low return, and those areas had previously been very resistant to any kind of testing.  

This feels a bit like a rant, but I'm genuinely interested if people think it can work.  It feels like its a over promising in the worst way.",SonicBoom_81,1744450729.0,112,49,0.94,text
1jygakg,Features you would love,If someone were to create a new cloud based data system. What features would you love it to have? What features do other services lack?,MyKo101,1744573736.0,0,5,0.25,text
1jxk5za,Building a Reliable Text-to-SQL Pipeline: A Step-by-Step Guide pt.1,,phicreative1997,1744472558.0,10,30,0.59,link
1jxe7rg,[Help] Modeling Tariff Impacts on Trade Flow,"
I'm working on a trade flow forecasting system that uses the RAS algorithm to disaggregate high-level forecasts to detailed commodity classifications. The system works well with historical data, but now I need to incorporate the impact of new tariffs without having historical tariff data to work with.

Current approach:
- Use historical trade patterns as a base matrix
- Apply RAS to distribute aggregate forecasts while preserving patterns

Need help with:
- Methods to estimate tariff impacts on trade volumes by commodity
- Incorporating price elasticity of demand
- Modeling substitution effects (trade diversion)
- Integrating these elements with our RAS framework

Any suggestions for modeling approaches that could work with limited historical tariff data? Particularly interested in econometric methods or data science techniques that maintain consistency across aggregation levels.

Thanks in advance!",levenshteinn,1744453493.0,5,5,0.67,text
1jwbevk,What technical skills should young data scientists be learning?,"Data science is obviously a broad and ill-defined term, but most DS jobs today fall into one of the following flavors:

- Data analysis (a/b testing, causal inference, experimental design)

- Traditional ML (supervised learning, forecasting, clustering) 

- Data engineering (ETL, cloud development, model monitoring, data modeling)

- Applied Science (Deep learning, optimization, Bayesian methods, recommender systems, typically more advanced and niche, requiring doctoral education)

The notion of a ‚Äúfull stack‚Äù data scientist has declined in popularity, and it seems that many entrants into the field need to decide one of the aforementioned areas to specialize in to build a career. 

For instance, a seasoned product DS will be the best candidate for senior product DS roles, but not so much for senior data engineering roles, and vice versa. 

Since I find learning and specializing in everything to be infeasible, I am interested in figuring out which of these ‚Äúpaths‚Äù will equip one with the most employable skillset, especially given how fast ‚ÄúAI‚Äù is changing the landscape. 

For instance, when I talk to my product DS friends, they advise to learn how to develop software and use cloud platforms since it is essential in the age of big data, even though they rarely do this on the job themselves. 

My data engineer friends on the other hand say that data engineering tools are easy to learn, change too often, and are becoming increasingly abstracted, making developing a strong product/business sense a wiser choice. 

Is either group right? 

Am I overthinking and would be better off just following whichever path interests me most? 

EDIT: I think the essence of my question was to assume that candidates have solid business knowledge. Given this, which skillset is more likely to survive in today and tomorrow‚Äôs job market given AI advancements and market conditions. Saying all or multiple pathways will remain important is also an acceptable answer. ",etherealcabbage72,1744326138.0,387,75,0.97,text
1jwlf3f,Causal Inference Casework,Hii All. My team currently has a demand forecasting model in place. Though it answers a lot of questions but isnt very good. I did a one day research on casual inference and from a brief understanding I feel it can be something worth looking at. I am a junior data scientist. How can I go forward and put this case forward to the principal data scientist from whom I need a sign off essentially. Should I create a POC on my own without telling anyone and present it with the findings or are there better ways ?? Thanks in advance :),NervousVictory1792,1744362266.0,21,28,0.8,text
1jx5k15,Any good classification datasets‚Ä¶,‚Ä¶that are comprised primarily of categorical features? Looking to test some segmentation code. Real world data preferred.,SingerEast1469,1744419806.0,0,23,0.28,text
1jwduc6,Predicting with anonymous features: How and why?,,chiqui-bee,1744333496.0,4,5,0.67,other
1jw7i9l,Seeking advice fine-tuning,"Hello, i am still new to fine tuning trying to learn by doing projects.

Currently im trying to fine tune a model with unsloth, i found a dataset in hugging face and have done the first project, the results were fine (based on training and evaluation loss).

So in my second project i decided to prepare my own data, i have pdf files with plain text and im trying to transform them into a question answer format as i read somewhere that this format is necessary to fine tune models. I find this a bit odd as acquiring such format could be nearly impossible.

So i came up with two approaches, i extracted the text from the files into small chnuks. First one is to use some nlp technics and pre trained model to generate questions or queries based on those chnuks results were terrible maybe im doing something wrong but idk. Second one was to only use one feature which is the chunks only 215 row . Dataset shape is (215, 1) I trained it on 2000steps and notice an 
overfitting by measuring the loss of both training and testing test loss was 3 point something and traing loss was 0.00‚Ä¶somthing.

My questions are:
- How do you prepare your data if you have pdf files with plain text my case (datset about law)
- what are other evaluation metrics you do
- how do you know if your model ready for real world deployment ",Gold-Artichoke-9288,1744315889.0,9,9,0.85,text
1jvrgr5,Fixing the Agent Handoff Problem in LlamaIndex's AgentWorkflow System,"[Fixing the Agent Handoff Problem in LlamaIndex's AgentWorkflow System](https://preview.redd.it/shjbjpxccyte1.png?width=1280&format=png&auto=webp&s=3338b6859f2cc9e4852d1ec0a3ffd59e3511c3e3)

# The position bias in LLMs is the root cause of the problem

I've been working with LlamaIndex's AgentWorkflow framework - a promising multi-agent orchestration system that lets different specialized AI agents hand off tasks to each other. But there's been one frustrating issue: when Agent A hands off to Agent B, Agent B often fails to continue processing the user's original request, forcing users to repeat themselves.

This breaks the natural flow of conversation and creates a poor user experience. Imagine asking for research help, having an agent gather sources and notes, then when it hands off to the writing agent - silence. You have to ask your question again!

[The receiving agent doesn't immediately respond to the user's latest request - the user has to repeat their question.](https://preview.redd.it/ucl76xnmcyte1.png?width=883&format=png&auto=webp&s=4fc975569f3bda5238ebb5ed1e5b08ff7cc86049)

**Why This Happens: The Position Bias Problem**

After investigating, I discovered this stems from how large language models (LLMs) handle long conversations. They suffer from ""position bias"" - where information at the beginning of a chat gets ""forgotten"" as new messages pile up.

[Different positions in the chat context have different attention weights. Arxiv 2407.01100](https://preview.redd.it/ugtqdq2tdyte1.png?width=519&format=png&auto=webp&s=cf9978aef461521633c8e20786ed48d8a106a2de)

In AgentWorkflow:

1. User requests go into a memory queue first
2. Each tool call adds 2+ messages (call + result)
3. The original request gets pushed deeper into history
4. By handoff time, it's either buried or evicted due to token limits

[FunctionAgent puts both tool\_call and tool\_call\_result info into ChatMemory, which pushes user requests to the back of the queue.](https://preview.redd.it/ypd4caewdyte1.png?width=786&format=png&auto=webp&s=240629c41c2f581dd7c3c8917912827358db5525)

Research shows that in an 8k token context window, information in the first 10% of positions can lose over 60% of its influence weight. The LLM essentially ""forgets"" the original request amid all the tool call chatter.

**Failed Attempts**

First, I tried the developer-suggested approach - modifying the handoff prompt to include the original request. This helped the receiving agent see the request, but it still lacked context about previous steps.

[The original handoff implementation didn't include user request information.](https://preview.redd.it/lbnm2laxcyte1.png?width=681&format=png&auto=webp&s=261eb162385f7f471c92a7812c188404ed682548)

[The output of the updated handoff now includes both chat history review and user request information.](https://preview.redd.it/u5eukjkycyte1.png?width=681&format=png&auto=webp&s=2956e9aa2f88ce7aa65da0f09fbdb93a7930aa27)

Next, I tried reinserting the original request after handoff. This worked better - the agent responded - but it didn't understand the full history, producing incomplete results.

[After each handoff, I copy the original user request to the queue's end. ](https://preview.redd.it/j5irsta0dyte1.png?width=807&format=png&auto=webp&s=f4cbaf58ca093a06938e0ccf9cd7ea9164def92d)

**The Solution: Strategic Memory Management**

The breakthrough came when I realized we needed to work with the LLM's natural attention patterns rather than against them. My solution:

1. **Clean Chat History**: Only keep actual user messages and agent responses in the conversation flow
2. **Tool Results to System Prompt**: Move all tool call results into the system prompt where they get 3-5x more attention weight
3. **State Management**: Use the framework's state system to preserve critical context between agents

[Attach the tool call result as state info in the system\_prompt.](https://preview.redd.it/yj1wmx06eyte1.png?width=634&format=png&auto=webp&s=96272c1d5ead65d83881780ae6ee4d92d7c0e7aa)

This approach respects how LLMs actually process information while maintaining all necessary context.

**The Results**

After implementing this:

* Receiving agents immediately continue the conversation
* They have full awareness of previous steps
* The workflow completes naturally without repetition
* Output quality improves significantly

For example, in a research workflow:

1. Search agent finds sources and takes notes
2. Writing agent receives handoff
3. It immediately produces a complete report using all gathered information

[ResearchAgent not only continues processing the user request but fully perceives the search notes, ultimately producing a perfect research report.](https://preview.redd.it/1hw8vza8dyte1.png?width=671&format=png&auto=webp&s=b721645671c5639c2e0b7990395ed077a992900f)

**Why This Matters**

Understanding position bias isn't just about fixing this specific issue - it's crucial for anyone building LLM applications. These principles apply to:

* All multi-agent systems
* Complex workflows
* Any application with extended conversations

The key lesson: LLMs don't treat all context equally. Design your memory systems accordingly.

[In different LLMs, the positions where the model focuses on important info don't always match the actual important info spots. ](https://preview.redd.it/ex69ri8cdyte1.png?width=575&format=png&auto=webp&s=d680659f6e9889775c4d24b650e06ac9791945df)

**Want More Details?**

If you're interested in:

* The exact code implementation
* Deeper technical explanations
* Additional experiments and findings

Check out the full article on

[https://www.dataleadsfuture.com/fixing-the-agent-handoff-problem-in-llamaindexs-agentworkflow-system/](https://www.dataleadsfuture.com/fixing-the-agent-handoff-problem-in-llamaindexs-agentworkflow-system/)

I've included all source code and a more thorough discussion of position bias research.

Have you encountered similar issues with agent handoffs? What solutions have you tried? Let's discuss in the comments!",qtalen,1744267056.0,21,5,0.82,text
1jvlqx7,"Is Agentic AI a Generative AI + SWE, or am I missing a thing?","Basically I just started doing hands-on around the Agentic AI. However, it all felt like creating multiple functions/modules powered with GenAI, and then chaining them together using SWE skills such as through endpoints. 

Some explanation said that Agentic AI is proactive and GenAI is reactive. But then, I also thought that if you have a function that uses GenAI to produce output, then run another code to send the result somewhere else, wouldn't that achive the same thing as Agentic AI?

Or am I missing something?

Thank you!

Note: this is an oversimplification of a scenario.",Upstairs-Deer8805,1744246800.0,37,10,0.87,text
1jvcz3t,GenAI and LLM preparation for technical rounds,"From technical rounds perspective, can anyone suggest resources or topics to study for GenAI and LLMs? I have had some experience with them, but then in interviews they go into the depth (eg. Attention mechanism, Q-learning, chunking strategies, case studies etc.). Honestly, most of what I can see in YouTube is just in surface level. If it's just about calling an API and feeding your documents, then it's too simple, but that's not how interviews happen. ",alpha_centauri9889,1744223233.0,100,20,0.95,text
1jvav77,"just took a new job in supply chain optimization, what do i need to learn to be effective?",I am new to supply chain and need to know what resources/concepts I should be familiar with.,fridchikn24,1744218154.0,33,23,0.85,text
1juo7ue,Absolutely BOMBED Interview,"I landed a position 3 weeks ago, and so far wasn‚Äôt what I expected in terms of skills. Basically, look at graphs all day and reboot IT issues. Not ideal, but I guess it‚Äôs an ok start. 

Right when I started, I got another interview from a company paying similar, but more aligned to my skill set in a different industry. I decided to do it for practice based on advice from l people on here.

First interview went well, then got a technical interview scheduled for today and ABSOLUTELY BOMBED it. It was BAD BADD. It made me realize how confused I was with some of the basics when it comes to the field and that I was just jumping to more advanced skills, similar to what a lot of people on this group do. It was literally so embarrassing and I know I won‚Äôt be moving to the next steps. 

Basically the advice I got from the senior data scientist was to focus on the basics and don‚Äôt rush ahead to making complex models and deployments. Know the basics of SQL, Statistics (linear regression, logistic, xgboost) and how you‚Äôre getting your coefficients and what they mean, and Python. 

Know the basics!!",MightGuy8Gates,1744145436.0,523,68,0.98,text
1jvwexo,Do professionals in the industry still refer to online sources or old code for solutions?,"Hey everyone,  
I‚Äôm currently studying and working on improving my skills in data science, and I‚Äôve been wondering something:

Do professionals‚Äîthose already working in the industry‚Äîstill take reference from online sources like Stack Overflow, old GitHub repos, documentation, or even their previous Jupyter notebooks when they‚Äôre coding?

Sometimes I feel like I‚Äôm ‚Äúcheating‚Äù when I google things I forgot or reuse snippets from old work. But is this actually a normal part of professional workflows?

For example, take this small code block below:

  
`# 1. Instantiate the random forest classifier`

`rf = RandomForestClassifier(random_state=42)`



`# 2. Create a dictionary of hyperparameters to tune`

`cv_params = {'max_depth': [None],`

`'max_features': [1.0],`

`'max_samples': [1.0],`

`'min_samples_leaf': [2],`

`'min_samples_split': [2],`

`'n_estimators': [300],`

`}`



`# 3. Define a list of scoring metrics to capture`

`scoring = ['accuracy', 'precision', 'recall', 'f1']`



`# 4. Instantiate the GridSearchCV object`

`rf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='recall')`

  
Would professionals be able to code this entire thing out from memory, or is referencing docs and previous code still common?",LimpInvite2475,1744287252.0,0,18,0.32,text
1juzclh,Familiar matchmaking in gaming; to match players with players they like and have played with before,"I've seen the classic MMRs before based on skill level in many different games. 

But the truth is gaming is about fun, and playing with people you already like or who are similar to people you like is a massive fun multiplier

So the challenge is how would you design a method to achieve that? Multiple algorithms, or something simpler?  
  
My initial idea is raw, and ripe for improvement

During or after a game session is over you get to thumbs up or thumbs down players you enjoyed playing with. 

Later on if you are in a matchmaking queue the list of players you've thumbed up is consulted and the party that has players with the greatest total thumbs up points at the top of that list gets matched to your party if there is free space, and if you are at the top of the available people on their end too.

The end goal here is to make public matchmaking more fun, and feel more familiar as you get to play repeatedly with players you've enjoyed playing with before.

The main issue with this type of matchmaking is that over time it would be difficult for newer players to get enough thumbs up to get higher on the list. Harder to get to play with the people who already have a large pool of people they like to play with. I don't know how to solve that issue at the moment.",wang-bang,1744180578.0,23,7,0.95,text
1jv4xqf,"Hi, I‚Äôm a junior in high school and I am interested in Data Science. What‚Äôs steps should I take to get there (from now to the end of high school)?","Picture will be referenced later

For some background all I‚Äôve done related to data science is a harvard edx python course which I took twice (first time I got all the way to the final project then quit, the second time I wasn‚Äôt able to finish all the lectures). Though I know I have the skills, I really need a refresher on the language.

Some questions I have are:
1. Is it good to take certifications in this field. For example, in the computer networking role, the CCNA is an extremely important certification and can easily get you hired for an entry level position. Is there anything similar in data science?

2. Any way to find data science internships? Idk why but it‚Äôs kinda hard to find data science internships. I did manage to find a few, but idk which ones the best use of my time. Any help here?

3. In the picture I put a roadmap that i found online. The words are kinda small; to clarify, first they say to learn python, then R, then GIT, then data structures and algorithms, after that they recommend learning SQL, then math/statistics, then data processing and visualization, machine learning, deep learning, and finally big data. Is this a good path to follow? If so how should I approach going down this route? Any resources I can use to start learning?

Any other tips would be greatly appreciated, thank you all for reading I really appreciate it.",Particular_Reality12,1744202852.0,0,31,0.45,image
1jtyyc0,Do remote data science jobs still exsist?,"Evry time I search remote data science etc jobs i exclusively seem to get hybrid if anything results back and most of them are 3+ days in office a week.

Do remote data science jobs even still exsist, and if so, is there some in the know place to look that isn't a paid for site or LinkedIn which gives me nothing helpful?",vintagefiretruk,1744066951.0,103,66,0.82,text
1jtoul7,Data Science Projects for 1 Year of Experience,"Hello senior/lead/manager data scientist,  
What kind of data science projects do you typically expect from a candidate with 1 year of experience?",guna1o0,1744041752.0,133,38,0.95,text
1ju139m,Career Crossroads: DS Manager (Retail) w/ Finance Background -> Head of Finance Analytics Offer - Seeking Guidance & Perspectives,"
Hey r/datascience,

Hoping to tap into the collective wisdom here regarding a potential career move. I'd appreciate any insights or perspectives you might have.

My Background:

Current Role: Data Science Manager at a Retail company.

Experience: ~8 years in Data Science (started as IC, now Manager).

Prior Experience: ~5 years in Finance/M&A before transitioning into data science.
The Opportunity:

I have an opportunity for a Head of Finance Analytics role, situated within (or closely supporting) the Financial Planning & Analysis (FP&A) function.

The Appeal: 
This role feels like a potentially great way to merge my two distinct career paths (Finance + Data Science). It leverages my domain knowledge from both worlds. The ""Head of"" title also suggests significant leadership scope.

The Nature of the Work:
The primary focus will be data analysis using SQL and BI tools to support financial planning and decision-making.
Revenue forecasting is also a key component.
However, it's not a traditional data science role. Expect limited exposure to diverse ML projects or building complex predictive models beyond forecasting.
The tech stack is not particularly advanced (likely more SQL/BI-centric than Python/R ML libraries).


My Concerns / Questions for the Community:

Career Trajectory - Title vs. Substance? 
Moving from a ""Data Science Manager"" to a ""Head of Finance Analytics"" seems like a step up title-wise. However, is shifting focus primarily to SQL/BI-driven analysis and forecasting, away from broader ML/DS projects and advanced techniques, a potential functional downstep or specialization that might limit future pure DS leadership roles?

Technical Depth vs. Seniority: 
As you move towards Head of/Director/VP levels, how critical is maintaining cutting-edge data science technical depth versus deep domain expertise (finance), strategic impact through analysis, and leadership? Does the type of technical work (e.g., complex SQL/BI vs. complex ML) become less defining at these senior levels?

Compensation Outlook: 
What does the compensation landscape typically look like for senior analytics leadership roles like ""Head of Finance Analytics,"" especially within FP&A or finance departments, compared to pure Data Science management/director tracks in tech or other industries? Trying to gauge the long-term financial implications.

I'm essentially weighing the unique opportunity to blend my background and gain a significant leadership title (""Head of"") against the trade-offs in the type of technical work and the potential divergence from a purely data science leadership path.

Has anyone made a similar move or have insights into navigating careers at the intersection of Data Science and Finance/FP&A, particularly in roles heavy on analysis and forecasting? Any perspectives on whether this is a strategic pivot leveraging my unique background or a potential limitation for future high-level DS roles would be incredibly helpful.

Thanks in advance for your thoughts!

TL;DR: DS Manager (8 YOE DS, 5 YOE Finance) considering ""Head of Finance Analytics"" role. Opportunity to blend background + senior title. Work is mainly SQL/BI analysis + forecasting, less diverse/advanced DS. Worried about technical ""downstep"" vs. pure DS track & long-term compensation. Seeking advice.
",mad_e_y_e,1744073148.0,28,15,0.88,text
1juvgek,Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour,"# FREE Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour

[https://www.youtube.com/watch?v=8XH2vTyzL7c](https://www.youtube.com/watch?v=8XH2vTyzL7c)",chrisgarzon19,1744166203.0,0,0,0.38,text
1jti77o,I created a basic playground to help people familiarise themselves with copulas,"Hi guys,  
  
So, this app allows users to select a copula family, specify marginal distributions, and set copula parameters to visualize the resulting dependence structure.  
  
A standalone calculator is also included to convert a given Kendall‚Äôs tau value into the corresponding copula parameter for each copula family. This helps users compare models using a consistent level of dependence.

The motivation behind this project is to gain experience deploying containerized applications.

https://preview.redd.it/nd4nhhr65ete1.png?width=2554&format=png&auto=webp&s=edd9c4b7add7df49ee2e3d55d91a030d4204f80e

Here's is the link if anyone wants ton interact with it, it was build with desktop view in mind but later I realised that it's very likely people will try to access via phone, it still works but it doesn‚Äôt look tidy.

[https://copula-playground-app-n7fioequfq-lz.a.run.app](https://copula-playground-app-n7fioequfq-lz.a.run.app)",Emergency-Agreeable,1744022413.0,52,11,0.91,text
1jtmrxz,We built a framework for building SQL bots and automations!,"Hey folks! We recently released Oxy, an open-source framework for building SQL bots and automations:¬†[https://github.com/oxy-hq/oxy](https://github.com/oxy-hq/oxy)

In short, Oxy gives you a simple YAML-based layer over LLMs so they can write accurate SQL with the right context. You can also build with these agents by combining them into workflows that automate analytics tasks.

The whole system is modular and flexible thanks to Jinja templates - you can easily reference or reuse results between steps, loop through data from previous operations, and connect everything together.

We have a few folks using us in production already, but would love to hear what you all think :)",ryime,1744036610.0,10,2,0.63,image
1jtdrvr,MSCS Admit; Preparing for 2026 Summer Internship Recruitement,"I got admitted to a top MSCS program for Fall 2025! I want to be ready for Data Science recruitement for Summer 2026.

I have 3 YOE as a data scientist in a FinTech firm with a mix of cross-functional production-grade projects in NLP, GenAI, Unsupervised learning, Supervised learning with high proficiency in Python, SQL, and AWS. 

Unfortunately, do not have experience with big data technologies (Spark, Snowflake, Big Query, etc), experimentation (A/B Testing), or deployment due to the nature of my job. 

No recent personal projects. 

Lastly, I did my undergrad from a top school with majors in data science and business. Had some comprehensive projects from classes currently listed on my resume. 

Would highly appreciate advice on the best course of action in the comming 4-8 months to maximize my chances in landing a good internship in 2026. I recognize my weaknesses but would like to determine how I can prioritize them. Have not recruited/interviewed in a while. 

Add info: I am also an international working under an
n H-1B. 

Update: Many of you have flagged that I should not be seeking data science internships with 3 YOE. However, my current title is Quant analyst and is a bit more geared towards finance. Yes the skills are transferable but the problems and the approach are very different.",Feeling_Bad1309,1744003142.0,23,20,0.74,text
1jtcjlc,"Weekly Entering & Transitioning - Thread 07 Apr, 2025 - 14 Apr, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1743998503.0,7,35,0.89,text
1jtn8oj,"If SNL can go live every week, why can't our models go live in 6 months?","""The show doesn't go on because it's ready. It goes because it's 11:30.""

I love this quote from Saturday Night Live's creator, Lorne Michaels. It holds a lot of wisdom about how projects should be planned and executed.

In data science, it perfectly captures the idea of shaping a project with fixed time and flexible scope. Too often, we get stuck in PoC hell. When every new project is treated as an experiment, requirements tend to be vague, definitions of done unclear. We fall into the rabbit hole of endlessly tweaking hyperparameters, convinced that the right combination will solve all our problems.

We end up running in circles, with yet another PoC that never makes it to production.

Lorne understood back in 1975 that to make people laugh every Saturday, they had to work with a fixed time and flexible scope. If they‚Äôve managed to do that every week for nearly 50 years, why can't we get a model into production in less than six months?",santiviquez,1744037775.0,0,14,0.46,image
1jrx6qg,What do you think about the blog 'Towards Data Science' breaking free from Medium ? Is it the best blog about Data Science out there ? What are your favourites ?,"I have been following Towards Data Science for years. It was one of the main reasons I considered and took a Medium subscription in the past. However, it recently decided to off-board Medium and launch their own independent blog. I was wondering about the reasons for this move. 

It is a loss for Medium since it was Medium's largest publication. I also imagine it could possibly be worse for Towards Data Science since they have to get readers to their independent website instead of take advantage of Medium's user base. 

  
I also wanted to know if it is the best data science blog out there since it is now independent. What are your favourites ? Here are some of mine. 

* Data Skeptic - A weekly email newsletter every Wednesday 
* Deep Dive - Amazon's monthly newsletter focused on data science and machine learning 
* Quanta - It is a popular science blog and not strictly about data science, though some articles have an intersection with it. 

  
This is my first post on this subreddit. I really like it. I notice this subreddit is much more motivating and positive compared to some other subreddits on computer science. ",MagicalEloquence,1743834530.0,189,48,0.93,text
1js1sgj,How to deal with medium data,"I recently had a problem at work that dealt with what I‚Äôm coining as ‚Äúmedium‚Äù data which is not big data where traditional machine learning greatly helps and it wasn‚Äôt small data where you can really only do basic counts and means and medians. What I‚Äôm referring to is data that likely has a relationship that can be studied based on expertise but falls short in any sort of regression due to overfitting and not having the true variability based on the understood data.

The way I addressed this was I used elasticity as a predictor. Where I divided the percentage change of each of my inputs by my percentage change of my output which allowed me to calculate this elasticity constant then used that constant to somewhat predict what I would predict the change in output would be since I know what the changes in input would be. I make it very clear to stakeholders that this method should be used with a heavy grain of salt and to understand that this approach is more about seeing the impact across the entire dataset and changing inputs in specific places will have larger effects because a large effect was observed in the past.

So I ask what are some other methods to deal with medium sized data where there is likely a relationship but your ML methods result in overfitting and not being robust enough?

Edit: The main question I am asking is how have you all used basic statistics to incorporate them into a useful model/product that stakeholders can use for data backed decisions?",cptsanderzz,1743854176.0,37,41,0.86,text
1js5jby,Is ongoing part time degree considered a red flag during job hunting?,"Is ongoing part time degree considered a red flag on your resume during job hunt?

I‚Äôm pursuing a part time MBA on weekends to upskill myself. This doesn‚Äôt affect my productivity at work. I am currently considering switching jobs. 

I want to understand if this should be listed on my resume. I plan to inform the hiring manager during final stages of the interview. Let me know if I‚Äôm thinking about this wrong. ",IMightBYourDad,1743865648.0,21,12,0.78,text
1jrz1k5,DS seeking development into SWE,"Hi community, 

I‚Äôm a data scientist that‚Äôs worked with both parametric and non parametric models. Quite experienced with deploying locally on our internal systems. 

Recently I‚Äôve been needing to develop client facing systems for external systems. However I seem to be out of my depth.

Are there recommendations on courses that could help a DS with a core in pandas, scikit learn, keras and TF develop skills on how endpoints and API works? Development of backend applications in Python. I‚Äôm guessing it will be a major issue faced by many data scientists. 

I‚Äôd appreciate if you could help with recommendations of courses you‚Äôve taken in this regard. ",NoteClassic,1743842539.0,36,12,0.82,text
1jr1tsv,I dare someone to drop this into a stakeholder presentation,"From source: https://ustr.gov/issue-areas/reciprocal-tariff-calculations

> ‚ÄúParameter values for Œµ and œÜ were selected. The price elasticity of import demand, Œµ, was set at 4‚Ä¶ The elasticity of import prices with respect to tariffs, œÜ, is 0.25.‚Äú",brianckeegan,1743735580.0,1680,136,0.98,image
1jrdrpx,ML Engineer GenAI @ Amazon,"I'll be having technical ML Engineer interview @ Amazon on Thursday and was researching what can I expect to be asked about. All online resources talk about ML concepts, system design and leadership rules, but they seem to omit job description.

IMO it doesn't make any sense for interviewer to ask about PCA, K-means, linear regression, etc. when the role is mostly relating to applying GenAI solutions, LLM customization and fine tuning. Also data structures & algos seem to me close to irrelevant in that context.

Does anyone have any prior experience applying to this department and know if it's better to focus on prioritizing more on GenAI related concepts or keep it broad? Or maybe you've been interviewing to different department and can tell how closely the questions were relating to job description?",Grapphie,1743778269.0,111,21,0.85,text
1jrr35h,"How do you calculate your hourly rate, if you were to consider contract over FTE?!","I have always been an FTE in this field, receiving compensations and benefits that extend far beyond the base salary.

For many years now, every contract opportunity a recruiter presented never made financial sense to me, regardless of the level, and even for top FAANG employers known for generous pay packages. Is this really the case and contract workers are scammed in this field? or is it just my luck? Or is it the recruiters robbing us?

For reference, I take my annual TC, divide it by 48 √ó 40 (weeks times hours), because there will be at least 4 unpaid vacation weeks if I contract, to estimate my hourly rate, which isn't even fair to me because I am not factoring benefits. Anyway, the value I get is always multiples more than the best contract offer a recruiter presented. So am I doing it wrong?!


t",DieselZRebel,1743813058.0,7,17,0.65,text
1jr680q,Getting back to Data Science after 4 years out,"Hi,

I left the corporate world to try to build my own apps.  They have not been successful and so I am trying to get hired back as a Data Scientist.  I have not yet heard anything from the applications I have sent so I would greatly appreciate your feedback on my CV.

I've anonymised where I can.  Re the picture, in Germany it is very normal and even expected that you add a picture, so this is why there is a placeholder there.

Cloud computing has become much more prevalent in the posts I see, so I am working my way through various Azure qualifications.

My current thoughts are:

* Add in LinkedIn Recommendations
* Somehow rewrite the key achievements to show monetary impact - current focus is on showing range of skills and impact
* Add Git - maybe add specific links to the different elements I've done for my own app development

Greatly appreciate your feedback



https://preview.redd.it/6q650rwytrse1.png?width=1414&format=png&auto=webp&s=8a509d15b782dbb272198e1a0a5fababcf9632a3

https://preview.redd.it/3kzliowytrse1.png?width=1414&format=png&auto=webp&s=c6ac751ff2cae83cb015db47b6fb995ebf0c04d0

https://preview.redd.it/31dqcnwytrse1.png?width=1414&format=png&auto=webp&s=e980bc60e07751e7cdaa23fae4672d5a383eec05",SonicBoom_81,1743751824.0,67,27,0.83,text
1jqpm9u,Data Scientist quiz from Unofficial Google Data Science Blog,https://www.unofficialgoogledatascience.com/2025/03/quantifying-statistical-skills-needed.html,FlyMyPretty,1743703706.0,143,34,0.96,text
1jqjinm,Ace the Interview: Graphs,"A solid grasp of graph theory can give you an edge in technical interviews, especially when the problem at hand is less about code and more about the structure beneath it.

At their core, graphs are about relationships. Each node represents an entity, and each edge represents a relationship. This simple abstraction lets you model remarkably complex systems. What matters most in interviews is not memorizing jargon, but understanding what these structures mean and how to work with them intuitively.

A graph doesn‚Äôt care where things are laid out‚Äîit only matters who connects to whom. That‚Äôs why there are countless ways to visualize the same graph. This property reminds us that graph algorithms don‚Äôt depend on visuals but on connectivity.

You should also get comfortable with the flavors of graphs. Some have direction (like a tweet being retweeted), some allow duplicate edges (multigraphs), and some are fully connected (cliques and complete graphs). Understanding when to use each form lets you frame problems properly, which is half the battle in any interview.

One of the most powerful concepts is the subgraph‚Äîa way to isolate parts of a system for focused analysis. It‚Äôs useful when troubleshooting a bug, analyzing a subset of users, or designing modular systems.

Key graph metrics like degree, centrality, and shortest path help you quantify structure. They reveal which nodes are ‚Äúimportant,‚Äù how information flows, and how efficient routes can be. These aren‚Äôt just for theory‚Äîthey appear constantly in ranking algorithms, search engine logic, and network analysis.

And don‚Äôt overlook concepts like bridges, which are edges whose removal splits the graph, or graph coloring, which underpins classic scheduling and resource allocation problems. Questions about exam scheduling, register allocation, or task assignment often reduce to ‚Äúcoloring‚Äù graphs efficiently.

Ultimately, the interview isn‚Äôt testing whether you know the name of every centrality metric. It‚Äôs testing whether you can recognize a graph problem when you see one‚Äîand whether you can think in terms of connections, constraints, and traversals.

I noticed the top posts on r/datascience tend to be about getting a job. I'd love to hear about what other topics you think I should cover! Also, I wrote an educational piece on graphs if you want to learn more: [https://iaee.substack.com/p/graphs-intuitively-and-exhaustively](https://iaee.substack.com/p/graphs-intuitively-and-exhaustively)",Daniel-Warfield,1743689570.0,122,31,0.94,text
1jr4rwq,Explain Complex Interactions Beyond Univariate Insights,"I‚Äôm analyzing a complex process where the outcome is client conversion rate, influenced by both numerical and categorical variables about client profile, product features, sales service, for instance.

So far, only univariate analyses have been used, but they fail to explain the variations effectively. I‚Äôve already applied traditional multivariable models like decision trees and SHAP, but they haven‚Äôt provided clear or actionable insights to explain the changes in conversion. 

I‚Äôm now looking for creative, multivariable approaches (possibly involving dimensionality reduction or latent structure) to better explain what‚Äôs driving conversion. Any advice on how to approach this differently?",gomezalp,1743745844.0,3,7,0.61,text
1jq3j72,Is there an unspoken glass ceiling for professionals in AI/ML without a PhD degree?,"I've been on the job hunt for MLE roles but it seems like a significant portion of them (certainly not all) prefer a PhD over someone with a master's.. If I look at the applicant profiles via Linkedin Premium, it seems like anywhere from 15-40% of applicants have PhDs as well. I work for a large organization and many of the leads and managers have PhD's, too.

So now, this got me worried about whether there's an unspoken glass ceiling for ML practitioners without a PhD. I'm not even talking about research/applied scientist positions, either, but just ML engineers and regular data scientists.

Do you find that this is true? If so, why is this?",Illustrious-Pound266,1743637436.0,164,121,0.9,text
1jq1lwz,select typical 10? select unusual 10? select comprehensive 10?,"Hi group, I'm a data scientist based in New Zealand.

Some years ago I did some academic work on non-random sampling - selecting points that are 'interesting' in some sense from a dataset. I'm now thinking about bringing that work to a wider audience.

I was thinking in terms of implementing as SQL syntax *(although* r/snowflake *suggests it may work better as a stored procedure).* This would enable some powerful exploratory data analysis patterns without stepping out of SQL.

We might propose queries like:

* *select typical 10...*¬†(finds 10 records that are ""average"" or ""normal"" in some sense)
* *select unusual 10...*¬†(finds the 10 records that are most 'different' from the rest of the dataset in some sense)
* *select comprehensive 10...*¬†(finds a group of 10 records that, between them, represent as much as possible of the dataset)
* *select representative 10...*¬†(finds a group of 10 records that, between them, approximate the distribution of the full dataset as closely as possible)

I've implemented a bunch of these 'select-adjectives' in R as a first step. Most of them work off a difference matrix using a generic metric using [Gower's distance](https://en.wikipedia.org/wiki/Gower%27s_distance). For example, 'select unusual 10' finds the ten records with the least RMS distance from all records in the dataset.

For demonstration purposes, I applied these methods to a [test dataset](https://www.rdocumentation.org/packages/poliscidata/versions/2.3.0/topics/world) of 'countries \[or territories\] of the world' containing various economic and social indicators, and found:

* five *typical* countries are the Dominican Republic, the Philippines, Mongolia, Malaysia, Thailand *(generally middle-income, quite democratic countries with moderate social development)*
* the most *unique* countries are Afghanistan, Cuba, Fiji, Botswana, Tunisia and Libya *(none of which is very like any other country)*
* a *comprehensive* list of seven countries, spanning the range of conditions as widely as possible, is Mauritania *(poor, less democratic)*, Cote d'Ivoire *(poor, more democratic)*, Kazakhstan *(middle income, less democratic)*, Dominican Republic *(middle income, more democratic)*, Kuwait *(high income, less democratic)*, Slovenia *(high income, more democratic)*, Germany *(very high income)*
* the six territories that are most *different* from each other are Sweden, the USA, the Democratic Republic of the Congo, Palestine and Taiwan
* the six countries that are most *similar* to each other are Denmark, Finland, Germany, Sweden, Norway and the Netherlands.

*(Please don't be offended if I've mischaracterised a country you love. Please also don't be offended if I've said a region is a country that, in your view, is not a country. The blame doubtless rests with my rather out-of-date test dataset.)*

So - any interest in hearing more about this line of work?",Majestic-Influence-2,1743632319.0,25,16,0.94,text
1jpq0x1,Tensorflow/Keras vs PyTorch for industry?,"I have used both Keras and PyTorch but only at the surface level. I am thinking to learn one in depth keeping DS/MLE positions in mind. I have heard that big companies use Tensorflow since it is more flexible in production while PyTorch is much more used in academia and research. I can't learn both at the same time, so want to know which one would be worth my time given that I am working in industry.

Note: By Tensorflow/Keras I meant starting with Keras and eventually evolving to Tensorflow.


PS: From the comments, I can see a lot of preferences for PyTorch. It's a clear winner.",alpha_centauri9889,1743604286.0,66,58,0.92,text
1jpy5qs,Robbery prediction on retail stores,"Hi, just looking for advice. I have a project in which I must predict probability of robbery on retail stores. I use robbery history of the stores, in which I have 1400 robberies in the last 4 years. Im trying to predict this monthly, So I add features such as robbery in the area in the last 1, 2, 3, 4 months behind, in areas for 1, 2, 3, 5 km. I even add month and if it is a festival day on that month. I am using XGboost for binary classification, wether certain store would be robbed that month or not. So far results are bad, predicting even 300 robberies in a month, with only 20 as true robberies actually, so its starting be frustrating.

Anyone has been on a similar project?",chris_813,1743623847.0,21,38,0.83,text
1jqa0yn,Does moving between domains a thing?,"Hi,
Just started a DS role at a financial company, and I was curious to know whether transitioning to a medical/biological/any-other-based company later is possible/common in the field.
Do companies care about domain specific knowledge or only about the actual soft and hard skills required for a data scientist?

Initially, I started studying DS from the motivation to use data to help people, but I grew up and understood that my noble ideas at a young age aren‚Äôt always realistic. But the idea it is possible since there are data scientists in these domains really encourages me to try and work with them sometime in the future.

Thanks, learned a lot from this sub.",indie-devops,1743657114.0,0,11,0.47,text
1jobolz,Tired of AI,"One of the reasons I wanted to become an AI engineer was because I wanted to do cool and artsy stuff in my free time and automate away the menial tasks. But with the continuous advancements I am finding that it is taking away the fun in doing stuff. The sense of accomplishment I once used to have by doing a task meticulously for 2 hours can now be done by AI in seconds and while it's pretty cool it is also quite demoralising. 

The recent 'ghibli style photo' trend made me wanna vomit, because it's literally nothing but plagiarism and there's nothing novel about it. I used to marvel at the art created by Van Gogh or Picasso and always tried to analyse the thought process that might have gone through their minds when creating such pieces as the Starry night (so much so that it was one of the first style transfer project I did when learning Machine Learning). But the images now generated while fun seems soulless.

And the hypocrisy of us using AI for such useless things. Oh my god. It boils my blood thinking about how much energy is being wasted to do some of the stupid stuff via AI, all the while there is continuously increasing energy shortage throughout the world.

And the amount of job shortage we are going to have in the near future is going to be insane! Because not only is AI coming for software development, art generation, music composition, etc. It is also going to expedite the already flourishing robotics industry. Case in point look at all the agentic, MCP and self prompting techniques that have come out in the last 6 months itself.

I know that no one can stop progress, and neither should we, but sometimes I dread to imagine the future for not only people like me but the next generation itself. Are we going to need a universal basic income? How is innovation going to be shaped in the future? 

Apologies for the rant and being a downer but needed to share my thoughts somewhere.

PS: I am learning to create MCP servers right now so I am a big hypocrite myself. ",akshayb7,1743449093.0,591,138,0.88,text
1jo4g14,It's important work.,,ElectrikMetriks,1743431151.0,1278,50,0.99,image
1joot5w,High quality time series data sources (with realtime)?,"Are there any services or offerings that make high-quality time series data public? Perhaps with the option of ingesting data from it in real time?

Ideally a service like this would have anything-over-time available - from weather to stock prices to air quality to country migration patterns - unified under an easy to use interface which would allow you to explore these data sources and potentially subscribe to them. Does anything like this exist? If not, is there any use or demand for anything like this? ",endgamer42,1743487907.0,12,7,0.81,text
1jo2gxt,"I have tested all the popular coding assistant for data science, here's what I found","Recently I feel like much less productive when doing data science work when I do more software development. I think it is because I use AI effectively when building software. So I setup a test to find the best AI coding assistant to help with Data Science task. 

The result is a bit surprising for me: None of the popular AI agent works for data science. Although the demo looks gorgeous, Google Gemini in Colab fail pretty bad. But there are some tools that has potential and some are already a bit useful. 

Check article for more detailed analysis. ",SummerElectrical3642,1743425592.0,95,45,0.84,link
1jo82j8,Struggling to understand A/B Test,"Hi, 

today I tried to understand the a/b testing, expecially in ML domain (for example, when a new recommendation system is better than another). I losed hours just to understand null hypotesis, alpha factor and t-test only to find out that I completely miss a lot of things (power? MDE? why t-test vs z.test vs person's chi test??

Do you know a resource to understand all of these things (written resources preferred)?? Thank you so much ",juvegimmy_,1743440340.0,43,53,0.68,text
1jnwt7l,Getting High Information Value on a credit scoring model,"I'm working on a credit scoring model.   
  
For a few features (3 out of 15), I'm getting high Information Values (IV) such as 1.0, 1.2, and 1.5. However, according to the theory, the maximum threshold should be 0.5. anything above this requires severe investigation as it might indicate data leakage.   
  
I've checked the features and the pipeline several times, but I couldn't find any data leakage.   
  
  
Is it normal to have high IV values, or should I investigate further?",guna1o0,1743402075.0,12,9,0.74,text
1jnh32k,Why you should use RMSE over MAE,"I often see people default to using MAE for their regression models, but I think on average most people would be better suited by MSE or RMSE.

Why? Because they are both minimized by different estimates!

You can prove that MSE is minimized by the conditional expectation (mean), so E(Y | X).

But on the other hand, you can prove that MAE is minimized by the conditional median. Which would be Median(Y | X).

It might be tempting to use MAE because it seems more ""explainable"", but you should be asking yourself what you care about more. Do you want to predict the expected value (mean) of your target, or do you want to predict the median value of your target?

I think that in the majority of cases, what people actually want to predict is the expected value, so we should default to MSE as our choice of loss function for training or hyperparameter searches, evaluating models, etc.

EDIT: Just to be clear, business objectives always come first, and the business objective should be what determines the quantity you want to predict and, therefore, the loss function you should choose.

Lastly, this should be the final optimization metric that you use to evaluate your models. But that doesn't mean you can't report on other metrics to stakeholders, and it doesn't mean you can't use a modified loss function for training.",Ty4Readin,1743354336.0,93,119,0.85,text
1jnupvt,"Weekly Entering & Transitioning - Thread 31 Mar, 2025 - 07 Apr, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1743393703.0,8,27,0.91,text
1jn9p9e,Should I invest time learning a language other than Python?,"I finished my PhD in CS three years ago, and I've been working as a data scientist for the past two years, exclusively using Python. I love it, especially the statistical side and scripting capabilities, but lately, I've been feeling a bit constrained by only using one language.

I'm debating whether it's worthwhile to branch out and learn another language to broaden my horizons. R seems appealing given my interests in stats, but I'm also curious about languages like Julia, Scala, or even something completely different.

Has anyone here faced a similar decision? Did learning another language significantly boost your career, or was it just a nice-to-have skill? Or maybe this is just a waste of time?

Thanks for any insights!

Update: I'm not completely sure about my long term goals, tbh. I do like statistics and stuff like causal inference, and Bayesian inference looks appealing. At the same time I feel that doing some DL might also be great and practical as they are the most requested in the industry (took some courses about NLP but at my work we mostly do tabular data with classical ML). Those are the main direction, but I'm aware that they might be too broad.",meni_s,1743331054.0,116,89,0.89,text
1jnkxza,Use of Generative AI,"I'm averse to generative AI, but is this one of those if you can't beat em, join em type of things? Is it possible to market myself by making projects (nowadays) without shoehorning LLMs, or wrappers?",Big-Acanthaceae-9888,1743364382.0,15,25,0.72,text
1jogiud,PSA: Largest Airbnb Datasets available for free at AirROI,"I came across this as I was looking to analyze some trends for my data science project. It covers more than a million listings and has high-quality data for many of the biggest rental markets.

[https://www.airroi.com/data-portal](https://www.airroi.com/data-portal)

",poorpeon,1743461174.0,0,17,0.47,text
1jma2jy,How to suck less in math?,"My masters wasn't math heavy but the focus was R and application. I want to understand some theory without going back to study calculus 1-3 and linear algebra not because I'm lazy, but because it is busy at work and I'm at loss of what to prioritize, I feel like I suck at coding too so I give it the priority at work since I spend lots of time data cleaning.

Is there a shortcut course/book for math specific to data science/staistical methods used in research?",None,1743208187.0,153,83,0.94,text
1jm4yzm,"If you are the one who says you want curious and motivated person, then do you actually hire them? Or it‚Äôs just a formality and decide based on tech skills?","I often see hiring managers and job posts saying they want someone who‚Äôs curious and motivated. I genuinely am I ask a lot of questions on projects, whether I‚Äôm working with data or just walking down the street thinking about things. I‚Äôve even shared work that shows this curiosity and drive, like how deeply I explore projects or how I published research papers just because I wanted to dive deeper into topics not because I had to for grades. I also often think about ways to improve the products we use.

But I rarely get a response or acknowledgment of these examples. So I was wondering how do you actually evaluate curiosity and motivation in a candidate? Or does it not matter that much, and the decision mostly comes down to whether someone meets the coding criteria once the recruiter passes the resume along?

I personally feel that curiosity is one of the most important traits for a data scientist but I‚Äôm not sure how often that really gets noticed or valued in the process.",Starktony11,1743194205.0,21,37,0.8,text
1jlnhg1,"‚ÄúGood at practical ML, weak on theory‚Äù ‚Äî getting the same feedback everywhere. How do I fix this?","Recently got this feedback after a machine learning engineer interview:

‚ÄúYou clearly understand how to make ML algorithms work in practice and have solid experience with real-world projects. But your explanations of the theoretical concepts behind the algorithms were vague or imprecise. We recommend taking a few months to review the fundamentals before reapplying.‚Äù

This isn‚Äôt the first time I‚Äôve heard this ‚Äî in fact, it‚Äôs a pattern I‚Äôm seeing across multiple interviews with tech-focused companies. And it‚Äôs getting in the way of landing the kinds of roles I‚Äôm really interested in.

Some context:
I‚Äôve been working for 2‚Äì3 years as an ML engineer at a large non-tech company. My experience is pretty diverse ‚Äî from traditional supervised learning to computer vision, with a recent shift toward GenAI (LLMs, embeddings, prompting, RAG, etc.). I‚Äôve built end-to-end pipelines, deployed models, and shipped ML to production. But because the work is so applied ‚Äî and lately very GenAI-oriented ‚Äî I‚Äôve honestly drifted away from the theoretical side of ML.

Now I‚Äôm trying to move into roles at more ML-mature companies, and I‚Äôm getting stuck at the theory part of the interviews.

My question is: how would you recommend brushing up on ML theory in a structured, deep way ‚Äî after being in the field for a while?
I‚Äôm not starting from zero, but I clearly need to tighten up my understanding and explanations.

Would love any advice, resources, or even personal stories from others who made the leap from applied/practical ML to more theory-heavy roles.

Thanks in advance!
",Difficult_Number4688,1743137983.0,177,103,0.92,text
1jlu60y,Options for a DS with 2 YOE,"I have been working as a data scientist for 2 years now in a consulting firm. I have experience with classical ML models, deep learning models, and some experience with GENAI. But my daily tasks revolve mostly around doing ad-hoc analytics. I am a CS grad. 

I am not very interested in analytics and consulting firm. So, what are the available options for me? Should I consider SDE (I don't have the experience though), MLE, or DS (in a product based company with more focus on model building)? 

I want growth and compensation and more interested in product based companies. What are my options? What's your advice? To be honest, working in consulting firm, it's too much frustrating due to long working hour and daily adhoc requests.",alpha_centauri9889,1743165783.0,31,15,0.86,text
1jlr7wo,Need Career Guidance - Ambiguity due to rising GenAI,"Hey Everyone, 

I have 6+ YOE in DS and my primary expertise is problem solving, classic ML (regression, classification etc.), Azure ML/Cognitive resources. Have worked on 20+ actual Manufacturing + Finance Industry use cases... 

I have dipped my hands a bit in GenAI, Neural nets, Vision models etc. But felt they are not my cup of tea. I mean I know the basics but don't feel like a natural with those tech. Primary reason not to prefer GenAI is because unless you are training/building LLMs (rare opportunity) all you are doing is software development using pre-trained models rather than any Data Science work.

So my question is to any Industry leaders/experts here.. where should I focus more on? 

Path 1: Stick to my skills and continue with the same (concerned if this sub segment becomes redundant in future)

Path 2: Diversify and focus on Gen AI or other sub segments.

Path 3: Others",_The_Numbers_Guy,1743154551.0,16,20,0.9,text
1jl6otm,Not getting calls for a month now. What can I do better?,What can I do better in this resume? I‚Äôve also worked on more projects but I have only listed high impact projects in my experience. ,IMightBYourDad,1743089551.0,231,112,0.92,image
1jlh6ae,Got a technical interview for data science intern at Capital One ‚Äì anyone been through it?,"Hey y‚Äôall,  
  
Just got an invite for a technical interview for a data science internship at Capital One, Wasn‚Äôt expecting to get this far tbh lol  
  
Anyone here been through it? Would love to hear about your experience ‚Äì what kind of stuff do they ask? Any curveballs or stuff I should brush up on? I‚Äôve done some Leetcode/stats/prep but not sure what¬†Capital One¬†specifically leans into.  
  
Any advice (or horror stories lol) welcome.",No-Brilliant6770,1743118295.0,38,26,0.89,text
1jlmc8t,Roast my freelancing website,"Hey fellow data scientists.

I am attempting to start my own business as a freelancer. I am at the very beginning of my journey. I have 0 experience as a free lancer, but I do have 5 years of career experience as a data analyst. 

For anyone willing, I need constructive criticism on the website I‚Äôve made. I realize it‚Äôs not great. I made it with a free square space trial. Feel free to be brutally honest, but if you can offer any improvement advice, that would be very appreciated 

password for the website: roast
",Typical-Macaron-1646,1743133877.0,14,21,0.75,other
1jl1ldy,Leaving data science - what are my options?,"This doesn't seem to be within the scope of the transitioning thread, so asking in my own post.

I have 10 YoE and am in the US. Was laid off in January. Was an actuarial analyst back in 2015 (I have four exams passed) using VBA and Excel, worked my way up to data analyst doing SQL + dashboarding (Shiny, Tableau, Power BI, D3), statistician using R and SQL and Python, and ended up at a lead DS. Minus things like Qlik, Databricks, Spark, and Snowflake, I have probably used that technology in a professional setting (yes, I have used all three major cloud services). I have a MS in statistics (my thesis was on time series) and am currently enrolled in OMSCS, but I am considering ending my enrollment there after having taken CV, DL, and RL.

I am very disappointed by how I observe the field has changed since ChatGPT came out. In the jobs I have had since that time as well as with interviews, the general impression I get is that people expect models to do both causal discovery and prediction optimally through mere data ingestion and algorithmic processing, without any sort of thought as to what data are available, what research questions there are, and for what purpose we are doing modeling. I did not enter this field to become a software engineer and just watch the process get automated away due to others' expectations of how models work only to find that expectations don't match reality. And then aside from that, I want nothing to do with generative AI. That is a whole other can of worms I won't get into. 

Very long story short, due to my mental health and due to me pushing through GenAI hype for job security, I did end up losing my memory in the process. I'm taking good care of myself (as mentioned in the comments, I've been 21 weeks into therapy). But I'm at a point right now where I'm not willing to just take any job without recognizing my mental limits. 

I am looking for data roles tied to actual business operations that have some aspect of requirements gathering (analyst, engineering, scientist, manager roles that aren't screaming AI all over them) and statistician roles, but especially given the layoff situation with the federal employees and contractors as well as entry-level saturation, this seems to be an uphill battle. I also think I'm in a situation where I have too much experience for an IC role and too little for a managerial role. The most extreme option I am considering is just dropping everything to become an electrician or HVAC person (not like I'm particularly attached to due to my memory loss anyway).

I want to ask this community for two things: suggestions for other things to pursue, and how to tailor my resume given the current situation. I have paid for a resume service and I've had my resume reviewed by tons of people. I have done a ton of networking. I just don't think that my mindset is right for this field.
",clarinetist001,1743074531.0,256,107,0.94,text
1jks145,What the fuck is happening on LinkedIn and reddit with LLMs?!,"Hi, I'm a very regular data scientist, really, very regular, finding good time applying statistics and linear algebra and machine learning to problems, with some optimization sometimes. End the week with a good PRD and call it a day. 

I swore to god I'd never learn about LLMs, I'm simply not interested, I'll never find a thrill learning it, let alone absorbing it on my timeline, everything now must talk about something, every time I open LinkedIn something dies. 

Do any of you guys see an out of this? How? How can one be a data scientist without having to deal with this every now and then? What fields rely on data scientists actually doing data science? Like work on numbers, apply some model, create a good pipeline or optimize some process and some storytelling and stuff? 

TBH, I've always been interested in ranching or plumbing, I guess that's my way out",Careful_Engineer_700,1743037457.0,500,153,0.91,text
1jl6tt4,Does anyone else lose interest during maintenance mode?,You've built a cool thing. It works great. Now it needs to be maintained with updates. Now I'm bored.,Trick-Interaction396,1743089910.0,32,12,0.94,text
1jkzb3c,"I built an AI-powered outreach system that automates job applications to CEOs, Data Heads, and Tech Recruiters","Hey guys, 

I‚Äôve been applying for a lot of jobs lately (hahaha, yeah the market sucks in the states). So I decided to build an AI system to make it a little less painful. It scrapes LinkedIn to find CEOs, Data Heads, and recruiters, predicts and verifies their emails, writes personalized messages using Mistral via Ollama, picks the best resume from a few versions I have, and sends it out automatically. I even set up a dashboard to keep track of everything. I‚Äôm getting a 17% response rate so far, which is way better than the usual black hole experience. Let me know if you're curious about how it works or if you have any ideas to make it even better!",Historical-Egg-2422,1743064554.0,27,21,0.7,text
1jl5tjk,Causal inference given calls,"I have been working on a usecase for causal modeling. How do we handle an observation window when treatment is dynamic. Say we have a 1 month observation window and treatment can occur every day or every other day. 

1) Given this the treatment is repeated or done every other day.
2) Experimentation is not possible. 
3) Because of this observation window can have overlap from one time point to another.

Ideally i want to essentially create a playbook of different strategies by utilizing say a dynamicDML but that seems pretty complex. Is that the way to go?

Note that treatment can also have a mediator but that requires its own analysis. I was thinking of a simple static model but we cant just aggregate it. 
For example we do treatment day 2 had an immediate effect. We the treatment window of 7 days wont be viable.  
Day 1 will always have treatment day 2 maybe or maybe not.  My main issue is reverse causality.

Is my proposed approach viable if we just account for previous information for treatments as a confounder such as a sliding window or aggregate windows. Ie # of times treatment has been done?


If we model the problem its essentially this

treatment -> response -> action

However it can also be
 treatment -> action 

As response didnt occur.
",JobIsAss,1743087343.0,6,8,0.76,text
1jlxfhj,EDA is Useless,"Hey folks! Yes, that is unpopular opinion. EDA is useless.

I've seen a lot notebooks on Kaggle in which people make various plots, histograms, density functions, scatter plots etc. But there is no point in doing it since at the end of the day just some sort of catboost or lightgbm is used. And still, such garbage is encouraged as usual, ""Great work!"".

All that EDA is done for the sake of EDA, and doesn't lead to any kind of decision making. ",Suspicious_Jacket463,1743174941.0,0,33,0.14,text
1jkjs7y,Isn't this solution overkill?,"I'm working at a startup and someone one my team is working on a binary text classifier to, given the transcript of an online sales meeting, detect who is a prospect and who is the sales representative. Another task is to classify whether or not the meeting is internal or external (could be framed as internal meeting vs sales meeting).

We have labeled data so I suggested using two tf-idf/count vectorizers + simple ML models for these tasks, as I think both tasks are quite easy so they should work with this approach imo... My team mates, who have never really done or learned about data science suggested, training two separate Llama3 models for each task. The other thing they are going to try is using chatgpt.

Am i the only one that thinks training a llama3 model for this task is overkill as hell? The costs of training + inference are going to be so huge compared to a tf-idf + logistic regression for example and because our contexts are very large (10k+) this is going to need a a100 for training and inference.

I understand the chatgpt approach because it's very simple to implement, but the costs are going to add up as well since there will be quite a lot of input tokens. My approach can run in a lambda and be trained locally.

**Also, I should add: for 80% of meetings we get the true labels out of meetings metadata, so we wouldn't need to run any model. Even if my tf-idf model was 10% worse than the llama3 approach, the real difference would really only be 2%, hence why I think this is good enough...**",AdministrativeRub484,1743015938.0,95,66,0.94,text
1jka8tt,Time-series forecasting: ML models perform better than classical forecasting models?,"This article demonstrated that ML models are better performing than classical forecasting models for time-series forecasting - https://doi.org/10.1016/j.ijforecast.2021.11.013

However, it has been my opinion, also the impression I got from the DS community, that classical forecasting models are almost always likely to yield better results. Anyone interested to have a take on this?",AMGraduate564,1742991035.0,103,70,0.95,text
1jkvbkv,Design/Planning tools and workflows?,"Interested in the tools, workflows, and general approaches other practitioners use to research, design, and document their ML and analytics solutions.

 My current workflow looks something like this:

Initial requirements gathering and research in a markdown document or confluence page. 

ETL, EDA in one or more notebooks with inline markdown documentation.

Solution/model candidate design back in confluence/markdown.

And onward to model experimentation, iteration, deployment, documenting as we go.


I feel like I‚Äôm at the point where my approach to the planning/design portions are bottlenecking my efficiency, particularly for managing complex projects. In particular:

- I haven‚Äôt found a satisfactory diagramming tool. I bounce around between mermaid diagrams and drawing in powerpoint.

- Braindumping in a markdown document feels natural, but I suspect I can be more efficient than just starting with a blank canvas and hammering away. 

- My team usually uses mlflow to manage experiments, but tends to present results by copy pasting into confluence.

How do you and/or your colleagues approach these elements of the DS workflow?",Smarterchild1337,1743047776.0,6,2,0.8,text
1jkfv0p,"First DS interview next week, just informed ""it will be very data engineering focused"".  Advice?","Hi all,  I'm going through the interview process for the first time.  I was informed that I got to the technical round, but that I should expect the questions to be very DE/ETL pipeline development focused.

I have decent experience with data-cleaning/transformation for analysis, and modelling from my PhD, but much less with the data ingestion part of the pipeline.  What suggestions would you give for me to brush up on/tools I should be able to talk fluently about?

The job is going to be dealing with a lot of real-time market data, time-series data heavy etc. I'm kinda surprised as there was no mention until now that it would be the DE side of the team (they specifically asked for predictive modelling with time-series data in description), but it's definitely something I'm interested in regardless.

Side note do people find that many DS-titled jobs these days are actually DE, or is the field so overlapping that the distinct titles aren't super relevant?",JarryBohnson,1743006372.0,28,22,0.9,text
1jl7q5h,How the fuck do I even get started in this field?,"Tiny bit of background, I have my master's in biostatistics and my undergrad in math, and did learn some ML modeling methods during grad school. Working as a data analyst currently but my day-to-day work involves very little actual analysis or even statistics.

On the other hand, reading all the posts and resumes here and current job openings for data scientists, I have honest to god no idea how I would ever even get one of these jobs or work towards it. I understand that having a statistics background can help in some vague, hand-wavey way, but I genuinely don't think I have any of the hard skills needed to work in DS and don't even know where to start.",NotTheTrueKing,1743092175.0,0,12,0.43,text
1jkmgo7,"data scientists in France, how do I improve my hiring chance?","I am a freelancer in France. 
I did √©cole ing√©nieur in statistics
my cv is a bit chaotic with short missions in data science, then spent 4 year just doing sql, R and some power bi, no ML. 
I did a gcp, tensorflow learning but they won t hire me for these cuz I don t have many projects.or even data science cuz I have a few experience.

Do you have some good projects I can work on since I am unemployed now, is it useful to learn something ( what?) cuz anyway they ll be like oh u dont have any projects or 5yr experience in this? what are your advice gor me please?",Due-Duty961,1743022562.0,7,8,0.66,text
1jkhe6u,Will working in insurance help me eventually become a data analyst?,"
I‚Äôve been applying to on-site roles for about a month now to get my foot in the door. Anything ‚Äúdata adjacent‚Äù or a large company where I think I can do (hopefully) an internal transfer. I‚Äôll be leaving a remote (niche role). I just got contacted for an interview for an ‚ÄúAnalyst‚Äù position at an insurance company. It pays almost $10,000 less than I get paid now and it‚Äôs hybrid. 

It‚Äôs not really an analyst role but I‚Äôll be analyzing insurance applications, learn the proper classifications, and pricing. It‚Äôs more of clerical role. They do have a data analyst team, and based on my limited research on LinkedIn, many of them start off in the ‚ÄúAnalyst‚Äù role and then pivot internally to a Data Analyst. They don‚Äôt expect you to have experience in insurance and are willing to completely train you. They also have great benefits as well. 

Would accepting this role be good for me? I know I‚Äôll be making much less because I‚Äôm now going to be hybrid and making almost $10,000 less but this is the best I can do. Even if I don‚Äôt internally pivot, would having an insurance industry background help me out in the long run when I apply to data analyst roles? ",Kati1998,1743010111.0,2,18,0.63,text
1jkjee5,Navigating the team in vested interest,"I have recent joined as an associate data scientist with previous background of swe. This is definitely my dream role and totally love the problems the team are solving. But it is kind of an ideal world scenario where the deployment is being done by DE team, pipelines as well. No containerisation or in short no MLOps practices. I do not like DE and the ever changing landscape of swe in general but I am wary of the stuff that this situation might set me back in the near future as all DS job postings do ask for some kind of DE, cloud, containerisation etc. How do I get my hands on these things or rather convince the team to move towards these tech stacks ?",NervousVictory1792,1743015018.0,0,1,0.33,text
1jj82n6,"""It's not you, it's me""?",,RecognitionSignal425,1742866507.0,387,134,0.94,other
1jiql82,"""Hey, you have a second for a quick call? It will just take a minute""",,ElectrikMetriks,1742822691.0,1242,43,0.99,image
1jk8klq,First Position Job Seeker and DS/MLE/AI Landscape,"Armed to the teeth with some projects and a few bootcamp certifications, Im soon to start applying at anything that moves.

Assuming you dont know how to code all that much, what have been your experiences when it comes to the use of LLM's in the workplace? Are you allowed to use them? Did you mention it during the interview?",trouble_sleeping_,1742984799.0,0,18,0.38,text
1jiui1n,Name your Job Title and What you do at a company (Wrong answers only),Basically what title says ,Terrible_Dimension66,1742832796.0,27,73,0.78,text
1jivc2n,Data Science Thesis on Crypto Fraud Detection ‚Äì Looking for Feedback!,"Hey r/datascience,

I'm about to start my Master‚Äôs thesis in DS, and I‚Äôm planning to focus on financial fraud detection in cryptocurrency. I believe crypto is an emerging market with increasing fraud risks, making it a high impact area for applying ML and anomaly detection techniques.

Original Plan:

\- Handling Imbalanced Datasets from Open-sources (Elliptic Dataset, CipherTrace) ‚Äì Since fraud cases are rare, techniques like SMOTE might be the way to go.  
\- Anomaly Detection Approaches:

* Autoencoders ‚Äì For unsupervised anomaly detection and feature extraction.
* Graph Neural Networks (GNNs) ‚Äì Since financial transactions naturally form networks, models like GCN or GAT could help detect suspicious connections.
* (Maybe both?)

Why This Project?

* I want to build an attractive portfolio in fraud detection and fintech as I‚Äôd love to contribute to fighting financial crime while also making a living in the field and I believe AML/CFT compliance and crypto fraud detection could benefit from AI-driven solutions.

My questions to you:

¬∑¬†¬†¬†¬†¬†¬† Any thoughts or suggestions on how to improve the approach?

¬∑¬†¬†¬†¬†¬†¬† Should I explore other ML models or techniques for fraud detection?

¬∑¬†¬†¬†¬†¬†¬† Any resources, datasets, or papers you'd recommend?

I'm still new to the DS world, so I‚Äôd appreciate any advice, feedback and critics.  
Thanks in advance!",Crokai,1742834838.0,18,13,0.84,text
1jii855,"Weekly Entering & Transitioning - Thread 24 Mar, 2025 - 31 Mar, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1742788904.0,11,41,0.92,text
1jhdo7j,"Management at my company claims to want coders / innovation, but rejects deliverables which aren't Excel","I work at a large financial firm. 
We have a ton of legacy Excel processes which require manual work, buggy add-ons or VBA code that takes several minutes to load. Spreadsheets that chug like hell to open or need to be operated with formula calculation off just to work in them.

Management will hype up ""innovation"" and will try to hire people with technical skills. They will send official communication talking about how the company is adopting AI and hyping up our internal chatbot (which is just some enterprise agreement with ChatGPT).

I've tried using python to automate some of our old processes. For example for adhoc deliverables, I'll use pandas and then style my work using great-tables, I'll plot stuff in plotly, etc.

I spend a lot of time styling my tables and plots to make them look professional. I use the company color scheme when creating them so that they look ""right"".

However, when I send stuff to my boss or his boss, they'll either complain that:

1) This doesn't look like the stuff that other people are doing

2) Will say ""I don't like the formatting"" but won't give specific examples on what to improve, won't provide examples of what constitutes good work

Independently of this, I recently spoke with a colleague who made attempts to move towards BI software such as Tableau for their processes. Even they have mentioned that the higher ups will ask for these types of solutions but ultimately prefer Excel's visuals for the deliverables.

I'm at a loss. I personally find Excel tables and graphs to be ugly, including the ones that my colleagues send. They look like something that a college student put together. If that's what the management wants, I'm inclined to stop complaining and just give it to them. But how would I actually do that in Python? 

In past jobs I've seen people do stuff like save ""Templates"" in Excel and have python spit the DF into the template. I've also heard there are packages that can create an excel file and then mark it up from within the code. At the end of the day this sounds like a recipe for me to create shitty code and unsustainable processes, which we already have plenty of. I want to be able to use a ""real"" plotting and table packages and perhaps just make something that is just good enough. 

Does anyone have any suggestions for me?

Edit: 

This post seems to have gained traction. I just wanted to clarify:
I think some people read this post as if my boss asked me to send an xlsx or csv file and I refused or am unwilling. That is not what happened. This is a post about visuals and formatting, i.e. sending emails or reports with inline tables and graphs/charts. If attaching an excel file with a raw DF were sufficient, obviously I would do that. 

Anyway I will look into using python/excel packages to mark up my stuff. Thanks",Fennecfox9,1742664359.0,266,70,0.97,text
1jhbqxn,Admission requirements of applied statistics /DS master,I‚Äôm looking at some schools within and outside of US for a master degree study in areas in the subject line . Just my past college education didn‚Äôt involve much algebra/calculus/ programming course . Have acquired some skills thru MITx online courses . How can I validate that my courses have met the requirements of such graduate programs and potentially showcase them to the admission committee ?,clooneyge,1742659327.0,19,36,0.86,text
1jgnzw6,Harnham - professional ghosts?,"Has anyone else been contacted by a recruiter from Harnham, conducted a 30min informational call, been told that their resume would be sent to the hiring manager, and then subsequently get ghosted by the recruiter? It‚Äôs happened to me 4 or 5 (or maybe more) times now. ",AnalyticNick,1742582130.0,74,46,0.96,text
1jgnhn7,"Deep learning industry Practitioners, how do you upskill yourself from the intermediate level?","I've been recently introduced to GPU-MODE, which is a great resource for kernels/gpu utilisation, I wondered what else is out there which is not pure research?",StillWastingAway,1742580850.0,22,9,0.92,text
1jhbvbg,Tips for migrating R-based ETL workflows to Python using LLM assistant?,"My team uses R heavily for production ETL workflows. This has been very effective, but I would prefer to be doing this in Python. Anyone with experience migrating R codebases to Python using LLM assistant? Our systems can be complex (multiple functions, SQL scripts, nested folders, config files, etc). We use RStudio Server for an IDE. I‚Äôve been using Gemini for ideation and some initial translation, but it‚Äôs tedious.",dmorris87,1742659647.0,0,22,0.45,text
1jgjuhz,Deep-ML (Leetcode for machine learning) New Feature: Break Down Problems into Simpler Steps!,"**New Feature: Break Down Problems into Simpler Steps!**

We've just rolled out a new feature to help you tackle challenging problems more effectively!

If you're ever stuck on a tough problem, you can now break it down into smaller, simpler sub-questions. These bite-sized steps guide you progressively toward the main solution, making even the most intimidating problems manageable.

Give it a try and let us know how it helps you solve those tricky challenges!  
its free for everyone on the daily question

[https://www.deep-ml.com/problems/39](https://www.deep-ml.com/problems/39)

https://preview.redd.it/00sa8vhoc2qe1.png?width=733&format=png&auto=webp&s=549d8786cfc4961c8f777968401e966331fe16ed

",mosef18,1742571823.0,16,10,0.68,text
1jgmsj0,"MoshiVis : New Conversational AI model, supports images as input, real-time latency","Kyutai labs (released Moshi last year) open-sourced MoshiVis, a new Vision Speech model which talks in real time and supports images as well in conversation. Check demo : https://youtu.be/yJiU6Oo9PSU?si=tQ4m8gcutdDUjQxh",mehul_gupta1997,1742579105.0,7,1,0.89,text
1jgkdwa,Scheduling Optimization with Genetic Algorithms and CP,"Hi,

I have a problem for my thesis project, I will receive data soon and wanted to ask for opinions before i went into a rabbit hole. 

I have a metal sheet pressing scheduling problems with  

* n jobs for varying order sizes, orders can be split 
* m machines, 
* machines are identical in pressing times but their suitability for mold differs.
*  every job can be done with a list of suitable subset of molds that fit in certain molds 
* setup times are sequence dependant, there are differing setup times for changing molds, subset of molds, 
* changing of metal sheets, pressing each type of metal sheet differs so different processing times
*  there is only one of each mold certain machines can be used with certain molds 
* I need my model to run under 1 hour. the company that gave us this project could only achieve a feasible solution with cp within a couple hours.

My objectives are to decrease earliness, tardiness and setup times

I wanted to achieve this with a combination of Genetic Algorithms, some algorithm that can do local searches between iterations of genetic algorithms and constraint programming. My groupmate has suggested simulated anealing, hence the local search between ga iterations. 

My main concern is handling operational constraints in GA. I have a lot of constraints and i imagine most of the childs from the crossovers will be infeasible. This[ chromosome encoding ](https://dergipark.org.tr/tr/download/article-file/218376)solves a lot of my problems but I still have to handle the fact that i can only use one mold at a time and the fact that this encoding does not consider idle times. We hope that constraint programming can add those idle times if we give the approximate machine, job allocations from the genetic algorithm. 

To handle idle times we also thought we could add 'dummy jobs' with no due dates, and no setup, only processing time so there wont be any earliness and tardiness cost. We could punish simultaneous usage of molds heavily in the fitness function. We hoped that optimally these dummy jobs could fit where we wanted there to be idle time, implicitly creating idle time. Is this a viable approach? How do people handle these kinds of stuff in genetic algorithms? Thank you for reading and giving your time.",NotMyRealName778,1742573165.0,9,11,0.91,text
1jfv15y,Breadth vs Depth and gatekeeping in our industry,"Why is it very common when people talk about analytics there is often a nature of people dismissing predictive modeling saying it‚Äôs not real data science or how people gate-keeping causal inference?

I remember when I first started my career and asked on this sub some person was adamant that you must know Real analysis. Despite the fact in my 3 years of working i never really saw any point of going very deep into a single algorithm or method? Often not I found that breadth is better than depth especially when it‚Äôs our job to solve a problem as most of the heavy lifting is done.


Wouldn‚Äôt this mindset then really be toxic in workplaces but also be the reason why we have these unrealistic take-homes where a manager thinks a candidate should for example build a CNN model with 0 data on forensic bullet holes to automate forensic analytics.

Instead it‚Äôs better for the work geared more about actionability more than anything.

Id love to hear what people have to say. Good coding practice, good fundamental understanding of statistics, and some solid understanding of how a method would work is good enough.",Tarneks,1742493344.0,76,41,0.84,text
1jg8inp,Really interesting ML use case from Strava,,jgmz-,1742530646.0,7,9,0.63,link
1jftqor,"I simulated 100,000 March Madness brackets",,Typical-Macaron-1646,1742490169.0,4,5,0.67,other
1jf8uwm,How exactly people are getting contacted by recruiters on LinkedIn?,"I have been applying for jobs for almost an year now and I have varied approach like applying directly on the websites, cold emailing, referral, only applying for jobs posted in last 24 hours and with each application been customized for that job description.

I have got 4 interviews in total and unfortunately no offer, but never a recruiter contacted me through LinkedIn, even it's regularly updated filled with skills, projects and experiences. I have made posts regarding various projects and topics but not a single recruiter contacted. 

Please share your input if you have received messages from recruiters.",SillyDude93,1742421022.0,65,62,0.84,text
1jeg1xn,Setting Expectations with Management & Growing as a Professional,"I am a data scientist at a F500 (technically just changed to MLE with the same team, mostly a personal choice for future opportunities).

Most of the work involves meeting with various clients (consulting) and building them ‚ÄúAI/ML‚Äù solutions. The work has already been sold by people far above me, and it‚Äôs on my team to implement it.

The issue is something that is probably well understood by everyone here. The data is horrific, the asks are unrealistic, and expectations are through the roof. 

The hard part is, when certain problems feel unsolvable given the setup (data quality, availability of historical data, etc), I often feel doubt that I am just not smart and not seeing some obvious solution. The leadership isn‚Äôt great from a technical side, so I don‚Äôt know how to grow. 

We had a model that we worked on for ages on a difficult problem that we got down to ~6% RMSE, and the client told us that much error is basically useless. I was so proud of it! It was months of work of gathering sources and optimizing.

At the same time, I don‚Äôt want to say ‚Äòthis is the best you will get‚Äô, because the work has already been sold. It feels like I have to be a snake oil salesmen to succeed, which I am good at but feels wrong. Plus, maybe I‚Äôm just missing something obvious that could solve these things‚Ä¶

Anyone who has significant experience in DS, specifically generating actual, tangible value with ML/predictive analytics? Is it just an issue with my current role? How do you set expectations with non-technical management without getting yourself let go in the process?

Apologies for the long post. Any general advice would be amazing. Thanks :)",TheFinalUrf,1742332241.0,56,21,0.9,text
1je46q1,"I made a Snowflake native app that generates synthetic card transaction data without inputs, and quickly",,matt-ice,1742302243.0,3,19,0.6,other
1jdotmw,What is financial fraud prevention data science like as a career path?,"How are the hours, the progression, the income, and the overall stress and work-life balance for this career path?  What are the pivots from here?

Edit: I'm most interested in learning about fraud prevention careers for banks and credit cards.",penpapermouse,1742248050.0,40,27,0.88,text
1jddkvq,Golden GIGO,,ElectrikMetriks,1742220749.0,139,6,0.97,image
1je02lx,Spending and demographics dataset,"Is there any free dataset out there that contains spending data at customer level, and any demographic info attached? I figure this is highly valuable and perhaps privacy sensitive, so a good dataset unlikely freely available.
In case there is some (anonymized) toy dataset out there, please do tell",Adorable-Emotion4320,1742285740.0,0,3,0.5,text
1jdv7ui,What‚Äôs your expectation from Jensen Huang‚Äôs keynote today in NVIDIA GTC? Some AI breakthrough round the corner?,"Today, Jensen Huang, NVIDIA‚Äôs CEO (and my favourite tech guy) is taking the stage for his famous Keynote at 10.30 PM IST in NVIDIA GTC‚Äô2025. Given the track record, we might be in for a treat and some major AI announcements might be coming. I strongly anticipate a new Agentic framework or some Multi-modal LLM. What are your thoughts? 

Note: You can tune in for free for the Keynote by registering at NVIDIA GTC‚Äô2025 [here](https://www.nvidia.com/gtc/?ncid=ref-inpa-722552).",mehul_gupta1997,1742266074.0,0,9,0.42,text
1jd5wub,Movies/Shows. Who gets it right? Who gets it SO wrong?,"Got a fun one for ya. Which moments in movies/shows have you cringed over, and which have you been impressed with, in regard to how they discuss the field? I feel like the term ‚Äúdata hard drive‚Äù has been thrown around since the 80s, the spy-related flicks always have some kind of weird geolocating/tracking animation that doesn‚Äôt exist. But who did it relatively well? Who did it the worst?",Thiseffingguy2,1742191131.0,10,28,0.62,text
1jclsn8,Seeking Advice: How to Effectively Develop advanced ML skills,"**About me** \- I am a DS with currently 3.5 YoE under my belt with experience in BFSI and FMCG.

In the past couple of months, I‚Äôve spoken with several mid-level data scientists working at my target companies. After reviewing my resume, they all pointed out the same gaps:

1. I lack NLP, Deep Learning, and LLM experience.
2. I don‚Äôt have any projects demonstrating these skills.
3. Feedback on my resume format varied from person to person.

Given this, I‚Äôd like advice on the following:

* How can I develop an intermediate-level understanding of NLP, DL, and LLMs enough to score a new job?
* Courses provide a high-level overview, but they often lack depth‚Äîwhat‚Äôs the best way to go deeper?
* I feel like **I‚Äôm being stretched too thin** by trying to learn these topics in different ways (courses, projects etc.). How would you approach this to stay focused and maximize learning?
* How do you gauge depth of your knowledge for interview?

Would appreciate any insights or strategies that worked for you!",JayBong2k,1742132812.0,180,51,0.94,text
1jd44gj,"Weekly Entering & Transitioning - Thread 17 Mar, 2025 - 24 Mar, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1742184104.0,10,57,0.87,text
1jcpd28,How to proceed with large work gap given competitive DS market?,"I‚Äôve been out of work for over a year now and don‚Äôt get much traction with job applications. I imagine the employment gap has rendered me basically unemployable in this market, despite having a master‚Äôs degree and a few years of subsequent work experience (plus some unrelated work experience prior to the master‚Äôs). I‚Äôve even applied to volunteer DS roles just to build my resume and been rejected. I recognize that I will likely need to find other means of employment before I can re-enter the DS space. Any advice on how to proceed and become employable again would be greatly appreciated. ",galactictock,1742142646.0,27,27,0.85,text
1jd2kgg,Is RPA a feasible way for Data Scientists to access data siloes?,"Basically, I'm debating whether I should make a case for my boss to learn my company's RPA tool (i.e. robot process automation) and invest a not insignificant amount of my time into implementing data pipelines.

We have an RPA tool already available, and we have a number of use cases that would benefit from it. I haven't systematically quantified their value (but I do have a rough idea).

Personally, I think I'm overqualified/overpaid for this type of data extraction. Plus, it's a technically inferior workaround to access siloed data. Lastly, I'm not sure what that deep dive into ""business analyst""/""data engineer light"" territory would mean for my career as a data scientist. It might limit me in some ways and it might create opportunities in others.

On the other side, it's only way too access some sources now. That may (or may not!) change in two years time, when a major software system is updated. And that depends on IT governance two years down the road (at a large company).

Long rambling, I know. My question: do you have experience with RPA bots within your data teams or within your departments? How and how well does it work for you? How sustainable a data pipeline can RPAs be? Do you have any advice for me?",norfkens2,1742178915.0,2,16,0.56,text
1jbqeyy,Solar panel installation rate and energy yield estimation from houses in the neighborhood using aerial imagery and solar radiation maps,,gagarin_kid,1742026380.0,38,3,1.0,other
1jcnell,3 Reasons Why Data Science Projects Fail,Have you ever seen any data science or analytics projects crash and burn? Why do you think it happened? Let‚Äôs hear about it! ,Thatshelbs,1742137394.0,0,14,0.26,link
1jbhjmx,Advice on building a data team,"I‚Äôm currently the ‚Äúchief‚Äù (i.e., only) data scientist at a maturing start up. The CEO has asked me to put together a proposal for expanding our data team. For the past 3 years I‚Äôve been doing everything from data engineering, to model development, and mlops. I‚Äôve been working 60+ hour weeks and had to learn a lot of things on the fly. But somehow I‚Äôve have managed to build models that meet our benchmark requirements, pushed them into production, and started to generate revenue. I feel like a jack of all trades and a master of none (with the exception of time-series analysis which was the focus of my PhD in a non-related STEM field). I‚Äôm tired, overworked and need to be able to delegate some of my work.

We‚Äôre getting to the point where we are ready to hire and grow our team, but I have no experience with transitioning from a solo IC to a team leader. Has anybody else made this transition in a start up? Any advice on how to build a team?

PS. Please DO NOT send me dm‚Äôs asking for a job. We do not do Visa sponsorships and we are only looking to hire locally. ",PsychicSeaCow,1741994965.0,163,82,0.95,text
1jbetth,"Chain restaurant data scientists, what do you do, and what kind of data do you work with?","Is it mostly just marketing? Do y‚Äôall ever work on pricing models or wholesale/supply chain analysis? Is your data internal or external? This is all out of academic curiosity, I am not currently looking to get into the industry!",kater543,1741987615.0,35,21,0.9,text
1jb0i8y,How much of the ML pipeline am I expected to know as DS?,"I'm prepping for an L4 level DS interview at big tech. The interview description is that we'll be doing ML case studies.

Does anyone have a good framework for how to outline how to answer these questions (how much you predict customer LTV?, how would you classify searches on the site?, how would you predict if the ad will be successful?, etc.) similar to the STAR framework for behavioral interviews?

How much of the pipeline am I supposed to know from the start to the end? Some of my interviews in the past have caught me off guard about some part in the pipeline I didn't think was the DS's job.",LeaguePrototype,1741946981.0,69,13,0.91,text
1jbdpuh,Contract For Hire Work,"Anybody have experience with contract for hire ds work? Did you convert? Did you get fired halfway through? Was it W2 or 1099? Were you forced to do the annoying stuff that full timers didn‚Äôt want to touch?

I‚Äôve been ignoring these types of jobs for a while now, but am interested in hearing how they are. Seems like a lack of security and benefits is traded for a high wage, but idk. 

Should I continue ignoring?",Fit-Employee-4393,1741984707.0,8,19,0.9,text
1javfus,Do you deal with unrealistic expectations from non-technical people frequently?,"I've been working at my job for a year and in data itself for several years. I'm willing to admit my shortcomings, willing to admit mistakes and learn.

However, there are several times where I feel like I've been in situations where there is 'no-winning'. Recently, I've inherited a task from a colleague who has left. There is no documentation. My only way of understanding this task is through the colleague who assigned it to me, who is not really a technical person. I've inherited code which is repetitive/redundant, difficult to follow and understand. What I REALLY want to do is spend time cleaning up this code so that debugging is easier and this code can run better but I'm not given a chance to do this b/c everytime I get a request related to this project, I'm asked to churn something out in less than a day. This feels unrealistic b/c I don't even have time to understand the outcome and whenever I do exactly as my collague asks, it has times broken something downstream, forcing me to undo this as soon as possible. This has put a strain on other tasks and so when I put this task to the side to do other tasks, there's been frustration expressed on me for not doing this task sooner.

The same colleague who assigned me this task initially told me that if I need help in understanding the requirements, he can help with that. When I've gone to him to ask questions or send updates, he himself looks like he doesn't have time to answer my questions because of back to back meetings. When he doesn't respond, then he expresses frustration to my boss and other senior colleagues when I haven't done something b/c I'm still waiting for a response b/c 'it's taking too long'. My boss has expressed to me he feels I don't ask enough questions that could be 'holding up the process'. So I have tried to ask more questions, but when colleagues can't get back to me on time, I'm told I'm not asking the right people or if I ask a question, I'm told I'm not 'asking the right question'. For example, this same colleague wanted me to fix a bug and wrote that this bug is causing ""unexpected results"". A senior colleague asked me if the requirements to fix this bug are clear to me and I thought to just clarify with the colleague who put in the bug fix request ""do you want me to remove these records or figure out how to best include them in the end result"". My boss saw my response and said ""you're not asking the right question! you're not supposed to ask people to do YOUR work for you"". From my point of view, I wasn't asking anybody to do my work b/c I'm the one ultimately who will dive into the code to fix things.

I'm at a loss tbh....I'm trying to do all the right things, trying to also improve my 'people skills' and understand what people want and how to streamline things. I know there's more room for improvement for me, but I am struggling with conflicting advice and lack of direction. I'm not sure if others can relate to this.",thro0away12,1741925269.0,104,33,0.96,text
1jally0,Does anyone have a job which doesn't use LLM/NLP/Computer Vision?,"I am looking for a new job and everything I see is LLM/NLP/Computer Vision. That stuff doesn't really interest me. Seems very computer science and my background is stats/analytics. I do linear regression and xgboost. Do these jobs still exist? If so, where?",Trick-Interaction396,1741897051.0,145,96,0.92,text
1jajnyq,Has anybody taken the DataMasked Course?,"Is it worth 3 grand? [https://datamasked.com/](https://datamasked.com/)

A data science coach (influencer?) on LinkedIn highly recommended it.

I'm 3 years post MS from a non-impressive state school. I'm working in compliance in the banking industry and bored out of my mind.

I'd like to break into experimentation, marketing, causal inference, etc.

Would this course be a good use of my money and time?",duffs_dimes,1741892160.0,21,37,0.72,text
1j8kofx,"Free Registrations for NVIDIA GTC' 2025, one of the prominent AI conferences, are open now","https://preview.redd.it/46u3gvqma0oe1.jpg?width=1200&format=pjpg&auto=webp&s=8e99003ad3c9af8b3f825a142650ea4e8ccfbf07

NVIDIA GTC 2025 is set to take place from **March 17-21**, bringing together researchers, developers, and industry leaders to discuss the latest advancements in **AI, accelerated computing, MLOps, Generative AI, and more**.

One of the key highlights will be **Jensen Huang‚Äôs keynote**, where NVIDIA has historically introduced breakthroughs, including last year‚Äôs **Blackwell architecture**. Given the pace of innovation, this year‚Äôs event is expected to feature significant developments in **AI infrastructure, model efficiency, and enterprise-scale deployment**.

With **technical sessions, hands-on workshops, and discussions led by experts**, GTC remains one of the most important events for those working in AI and high-performance computing.

**Registration is free** and now open. You can register [here.](https://www.nvidia.com/gtc/?ncid=ref-inpa-722552)

I strongly feel NVIDIA will announce something really big around AI this time. What are your thoughts?

",mehul_gupta1997,1741675340.0,18,10,0.83,text
1j8g4w9,MySQL for DS interviews?,"Hi, I currently work as a DS at a AI company, we primarily use SparkSQL, but I believe most DS interviews are in MySQL (?). Any tips/reading material for a smooth transition. 

  
For my work, I use SparkSQL for EDA and featurization ",redKeep45,1741658958.0,13,22,0.81,text
1j80r9t,Happy 2025 Mar10 Day!,,ElectrikMetriks,1741619202.0,71,5,0.88,image
1j8iqpw,"MSBA with 5 years experience in DS looking to pivot to an MLE, should I get a master's in CS?",I feel it would help me bridge the gap in software development and would appeal to recruiters(I am unemployed rn),fridchikn24,1741667444.0,5,9,0.62,text
1j87x92,How do you deal with coworkers that are adamant about their ways despite it blowing up in the past.,"Was discussing with a peer and they are very adamant of using randomized splits as its easy despite the fact that I proved that data sampling is problematic for replication as the data will never be the same even with random_seed set up. Factors like environment and hardware play a role.

I been pushing for model replication is a bare minimum standard as if someone else cant replicate the results then how can they validate it? We work in a heavily regulated field and I had to save a project from my predecessor where the entire thing was on the verge of being pulled out because none of the results could be replicated by a third party.

My coworker says that the standard shouldn‚Äôt be set up but i personally believe that replication is a bare minimum regardless as models isnt just fitting and predicting with 0 validation. If anything we need to ensure that our model is stable. 

The person constantly challenges everything I say and refuses to acknowledge the merit of methodology.  I dont mind people challenging but constantly saying I dont see the point or it doesn‚Äôt matter when it does infact matter by 3rd party validators. 

This person when working with them I had to constantly slow them down and stop them from rushing Through the work as it literally contains tons of mistakes. This is like a common occurrence. 


Edit: i see a few comments in, My manager was in the discussion as my coworker brought it up in our stand up and i had to defend my position in-front of my bosses (director and above). Basically what they said is ‚Äúapparently we have to do this because I say this is what should be done now given the need to replicate‚Äù. So everyone is pretty much aware and my boss did approach me on this, specifically because we both saw the fallout of how bad replication is problematic.",JobIsAss,1741637037.0,7,33,0.69,text
1j7ln6i,What sort of things should I be doing in my personal time to make moving companies easier?,"I'm looking to move from my current company, but am aware thats tough right now. I'm not new to the field, but my company doesn't really measure impact of solutions outside a few places (that I haven't been able to get projects supporting) so a lot of my resume lacks impact metrics. What things can I do to show I have the hard and soft skills these roles are looking for and show I can succeed in a place that does measure impact? I'm too small of a fish to change my company culture to get measurement in place as well, and wouldn't want to stay and be the one to rise up to do that, if that makes sense. 

I assume personal projects are less impressive than work projects, but is there anything I can do to make up for the fact that nothing I do at work really seems impressive either? ",TaterTot0809,1741565201.0,133,21,0.96,text
1j7uxqq,Why is my MacBook M4 Pro faster than my RTX 4060 Desktop for LLM inference with Ollama?,"I've been running the `deepseek-coder-v2` model (8.9GB) using `ollama run` on two systems:

1. **MacBook M4 Pro** (latest model)
2. **Desktop** with **Intel i9-14900K**, **192GB RAM**, and an **RTX 4060 GPU**

Surprisingly, the MacBook M4 Pro is significantly faster when running a simple query like ""tell me a long story."" The desktop setup, which should be much more powerful on paper, is noticeably slower.

Both systems are running the same model with default Ollama configurations.

Why is the MacBook M4 Pro outperforming the desktop? Is it related to how Ollama utilizes hardware, GPU acceleration differences, or perhaps optimizations for Apple Silicon?

Would appreciate insights from anyone with experience in LLM inference on these platforms!

Note: I can observe my gpu usage spiking when running the same, and so assume the hardware access is happening without issue",Suspicious_Sector866,1741599831.0,18,10,0.73,text
1j7uh4f,Have you started using MCP (Model Context Protocol) with your agentic workflow and data storages? What is the experience?,"If you've used MCP in your workflow, how has the experience been? Do you use it on top of your current data storage as well to gather more data?",metalvendetta,1741597724.0,9,2,0.77,text
1j7gch1,The kebab and the French train station: yet another data-driven analysis,,osm3000,1741550831.0,36,8,0.95,other
1j7q4w5,"Weekly Entering & Transitioning - Thread 10 Mar, 2025 - 17 Mar, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1741579302.0,9,24,0.86,text
1j7b4sg,Setting up AB test infra,"Hi, I‚Äôm a BI Analytics Manager at a SaaS company, focusing on the business side. The company wishes to scale A/B experimentation capabilities, but we‚Äôre currently limited by having only one data analyst who sets up all tests manually. This bottleneck restricts our experimentation capacity. 

Before hiring consultants, I want to understand the topic better. Could you recommend reliable resources (books, videos, courses) on building A/B testing infrastructure to automate test setup, deployment, and analysis. Any recommendations would be greatly appreciated!

Ps: there is no shortage on sources reiterating Kohavi book, but that‚Äôs not what I‚Äôm looking for. ",Alkanste,1741537261.0,22,21,0.85,text
1j5t5gp,Agent flow vs. data science,"I just wrapped up an experiment exploring how the number of agents (or steps) in an AI pipeline affects classification accuracy. Specifically, I tested four different setups on a movie review classification task. My initial hypothesis going into this was essentially, ""*More agents might mean a more thorough analysis, and therefore higher accuracy.""* But, as you'll see, it's not quite that straightforward.

# Results Summary

I have used the first 1000 reviews from IMDB  dataset to classify reviews into positive or negative. I used gpt-4o-mini as a model.

Here are the final results from the experiment:

|Pipeline Approach|Accuracy|
|:-|:-|
|Classification Only|0.95|
|Summary ‚Üí Classification|0.94|
|Summary ‚Üí Statements ‚Üí Classification|0.93|
|Summary ‚Üí Statements ‚Üí Explanation ‚Üí Classification|0.94|

Let's break down each step and try to see what's happening here.

# Step 1: Classification Only

*(Accuracy: 0.95)*

This simplest approach‚Äîsimply reading a review and classifying it as positive or negative‚Äîprovided the highest accuracy of all four pipelines. The model was straightforward and did its single task exceptionally well without added complexity.

# Step 2: Summary ‚Üí Classification

*(Accuracy: 0.94)*

Next, I introduced an extra agent that produced an emotional summary of the reviews before the classifier made its decision. Surprisingly, accuracy slightly dropped to 0.94. It looks like the summarization step possibly introduced abstraction or subtle noise into the input, leading to slightly lower overall performance.

# Step 3: Summary ‚Üí Statements ‚Üí Classification

*(Accuracy: 0.93)*

Adding yet another step, this pipeline included an agent designed to extract key emotional statements from the review. My assumption was that added clarity or detail at this stage might improve performance. Instead, overall accuracy dropped a bit further to 0.93. While the statements created by this agent might offer richer insights on emotion, they clearly introduced complexity or noise the classifier couldn't optimally handle.

# Step 4: Summary ‚Üí Statements ‚Üí Explanation ‚Üí Classification

*(Accuracy: 0.94)*

Finally, another agent was introduced that provided human readable explanations alongside the material generated in prior steps. This boosted accuracy slightly back up to 0.94, but didn't quite match the original simple classifier's performance. The major benefit here was increased interpretability rather than improved classification accuracy.

# Analysis and Takeaways

Here are some key points we can draw from these results:

# More Agents Doesn't Automatically Mean Higher Accuracy.

Adding layers and agents can significantly aid in interpretability and extracting structured, valuable data‚Äîlike emotional summaries or detailed explanations‚Äîbut each step also comes with risks. Each guy in the pipeline can introduce new errors or noise into the information it's passing forward.

# Complexity Versus Simplicity

The simplest classifier, with a single job to do (direct classification), actually ended up delivering the top accuracy. Although multi-agent pipelines offer useful modularity and can provide great insights, they're not necessarily the best option if raw accuracy is your number one priority.

# Always Double Check Your Metrics.

Different datasets, tasks, or model architectures could yield different results. Make sure you are consistently evaluating tradeoffs‚Äîinterpretability, extra insights, and user experience vs. accuracy.

In the end, ironically, the simplest methodology‚Äîjust directly classifying the review‚Äîgave me the highest accuracy. For situations where richer insights or interpretability matter, multiple-agent pipelines can still be extremely valuable even if they don't necessarily outperform simpler strategies on accuracy alone.

I'd love to get thoughts from everyone else who has experimented with these multi-agent setups. Did you notice a similar pattern (the simpler approach being as good or slightly better), or did you manage to achieve higher accuracy with multiple agents?

Full code on [GitHub](https://github.com/Pravko-Solutions/FlashLearn/tree/main/examples/agent_patterns)

# TL;DR

Adding multiple steps or agents can bring deeper insight and structure to your AI pipelines, but it won't always give you higher accuracy. Sometimes, keeping it simple is actually the best choice.",No_Information6299,1741366018.0,21,10,0.8,text
1j4v2ee,Google Collab now provides native support for Julia üéâü•≥,,Suspicious-Oil6672,1741266526.0,155,15,0.99,image
1j4wn4d,Failing final round interviews,I've been applying to DS internships all year and just got rejected from my 4th final round. Does anyone have any advice for these interviews? And is it bad practice for me to ask the hiring managers where I went wrong in the interviews?,Careless-Tailor-2317,1741271234.0,5,13,0.78,text
1j5hrrn,Thinking of selling my M2 Air to buy an M4 Pro - is it worth the upgrade for Machine Learning?,"Hey everybody, I need some advice. I‚Äôm a 3rd year CS undergrad and currently have a MacBook M2 Air with 16GB RAM and 256GB storage. I bought it in 2022 for about $2000 CAD, but I‚Äôve been running into issues. When I open multiple apps like Docker, Ollama, PyCharm, and run training models, the laptop quickly runs out of RAM and gets heat up and starts swapping, which isn‚Äôt great for the SSD.

I‚Äôm leaning towards selling it to upgrade to an M4 Pro, especially for machine learning and data science tasks. However, Apple‚Äôs trade-in value is only around $585 CAD, and I just recently had the motherboard, chassis, and display replaced (everything except the battery), so my laptop is basically new in most parts. I was planning to sell it on Facebook Marketplace, but I‚Äôm not sure what price I should target now that the M4 has been released.

On the flip side, I‚Äôve also considered keeping the laptop and using a Google Colab subscription for ML work. But running many applications still leads to heavy swap usage, which could harm the SSD in the long run. Given that I just renewed some parts, it might be the best time to sell for a higher resale value.

If I decide to upgrade to the M4, I‚Äôm thinking of getting a model with at least 24GB RAM and a 10-core CPU and GPU combination. Do you guys think that would be enough to future-proof it? What are your thoughts on selling now versus sticking with the current setup and using cloud resources?",No-Brilliant6770,1741331290.0,0,16,0.38,text
1j3r9qh,Best Industry-Recognized Certifications for Data Science?,"I‚Äôm looking to boost my university applications for a Data Science-related degree and want to take industry-recognized certifications that are valued by employers . Right now, I‚Äôm considering:

*  Google Advanced Data Analytics Professional Certificate 
*  Deep Learning Specialization  
*  TensorFlow Developer Certificate 
*  AWS Certified Machine Learning 

Are these the best certifications from an industry perspective, or are there better ones that hiring managers and universities prefer? I want to focus on practical, job-relevant skills rather than just general knowledge.",LimpInvite2475,1741137561.0,136,83,0.9,text
1j3hq4r,Whats your favourite AI tool so far?,Its hard for me too keep up - please enlighten me on what I am currently missing out on :),FirefoxMetzger,1741112915.0,123,102,0.91,text
1j3etuj,Favorite Data Science Books and Authors?,"I enjoy O‚ÄôReilly books for data science. I like how they build a topic progressively throughout the chapters. I‚Äôm looking for recommendations on great books or authors you‚Äôve found particularly helpful in learning data science, analytics, or machine learning.

What do you like about your recommendation? Do they have a unique way of explaining concepts, great real-world examples, or a hands-on approach?",Proof_Wrap_2150,1741105952.0,113,49,0.99,text
1j47h7z,Help with pyspark and bigquery,"Hi everyone. 

I'm creating a pyspark df that contains arrays for certain columns. 

But when I move it to a bigqquery table all the columns containing arrays are empty (they contains a message that says 0 rows)

Any suggestions? 

Thanks ",Zeoluccio,1741193182.0,1,3,0.57,text
1j30hfs,"HuggingFace free certification course for ""LLM Reasoning"" is live","HuggingFace has launched a new free course on ""LLM Reasoning"" for explaining how to build models like DeepSeek-R1. The course has a special focus towards Reinforcement Learning. Link : https://huggingface.co/reasoning-course",mehul_gupta1997,1741055622.0,194,12,0.95,image
1j3hx9m,Google's Data Science Agent (free to use in Colab): Build DS pipelines with just a prompt,"Google launched Data Science Agent integrated in Colab where you just need to upload files and ask any questions like build a classification pipeline, show insights etc. Tested the agent, looks decent but has errors and was unable to train a regression model on some EV data.  Know more here : https://youtu.be/94HbBP-4n8o",mehul_gupta1997,1741113392.0,8,5,0.63,text
1j39e2e,Workflow with Spark & large datasets,"Hi, I‚Äôm a beginner DS working at a company that handles huge datasets (>50M rows, >100 columns) in databricks with Spark. 

The most discouraging part of my job is the eternal waiting times when I want to check the current state of my EDA, say, I want the null count in a specific column, for example. 

I know I could sample the dataframe in the beginning to prevent processing the whole data but that doesn‚Äôt really reduce the execution time, even if I .cache() the sampled dataframe. 

I‚Äôm waiting now for 40 minutes for a count and I think this can‚Äôt be the way real professionals work, with such waiting times (of course I try to do something productive in those times but sometimes the job just needs to get done. 

So, I ask the more experienced professionals in this group: how do you handle this part of the job? Is .sample() our only option? I‚Äôm eager to learn ways to be better at my job. ",Davidat0r,1741090364.0,23,33,0.93,text
1j3hc7l,Would someone with a BBA Fintech make a good data scientist?,"Given they: Demonstrate fluency in Data Science programs/models such as Python, R, Blockchain, Al etc. and be able to recommend technological solutions to such problems as imperfect or asymmetric data

*(Deciding on a course to pursue with my limited regional options)*

Thank you ",Cool-Ad-3878,1741111976.0,0,12,0.33,text
1j2fd49,Soft skills: How do you make the rest of the organization contribute to data quality?,"I've been in six different data teams in my career, two of them as an employee and four as a consultant. Often we run into a wall when it comes to data quality where the quality will not improve unless the rest of the organization works to better it.

For example, if the dev team doesn't test the event measuring and deploy a new version, you don't get any data until you figure out what the problem is, ask them to fix it, and they deploy the fix. They say that they will test it next time, but it doesn't become a priority and happens a few months later again.

Or when a team is supposed to reach a certain KPI they will cut corners and do a weird process to reach it, making the measurement useless. For example, when employees on the ground are rewarded for the ""order to deliver"" time, they might check something as delivered once it's completed but not actually delivered, because they don't get rewarded for completing the task quickly only delivering it.

How do you engage with the rest organization to make them care about the data quality and meet you half way?

One thing I've kept doing at new organizations is trying to build an internal data product for the data producing teams, so that they can become a stakeholder in the data quality. If they don't get their processes in order, their data product stops working. This has had mixed results, form completely transformning the company to not having any impact at all. I've also tried holding workshops, and they seem to work for a while, but as people change departments and other stuff happens, this knowledge gets lost or deprioritized again.

What are your tried and true ways to make the organization you work for take the data quality seriously?",pimmen89,1740996142.0,72,14,0.95,text
1j2b2ng,"Weekly Entering & Transitioning - Thread 03 Mar, 2025 - 10 Mar, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1740978084.0,5,39,1.0,text
1j1nr34,Alternatives for Streamlit,"For my most pet projects like creating dashboards of voting charts for songs or planning a trip with altitude chart and maps along with some proof of concept for LLM or ML projects at work my first to go is Streamlit. I got accustomed to this tool but looking for some alternatives mostly because of the visual part. I tried dash with plotly but missing the coherence of the Streamlit. 

What is the tool that can do the same for the front end part (which can be uploaded in the simple way similar to Streamlit) as Streamlit but is not Streamlit. What are your favorite similar frameworks?",Marek_Vsk,1740910436.0,36,23,0.88,text
1j271od,Chain of Drafts : Improvised Chain of Thoughts prompting,CoD is an improvised Chain Of Thoughts prompt technique producing similarly accurate results with just 8% of tokens hence faster and cheaper. Know more here : https://youtu.be/AaWlty7YpOU,mehul_gupta1997,1740964935.0,1,0,0.56,text
1j17yok,Meta E5 ML Experience - Cleared,"Learned a lot form this subreddit so sharing my experience so people can learn from it too.

**Coding rounds** \- It is going to be 2 mids or 1 easy and 1 hard. For me biggest shock was the interviewer asked questions to see if I understand what I am saying or just saying it because I saw on leetcode that is the best option. So try to understand why the solution is working the way it is working and how is the space and time complexity calculated for that solution

**Behavioral** \- I created a story for every meta vision and mission. That covers all meta questions. The main difference I found in meta compared to other companies is the depth of follow ups. The questions were very specific and there were follow up questions on my answer to previous follow ups. I don't think one can lie in this round, they would be caught in the follow up questions easily. Also there was no why meta or tell me about yourself.

**MLSD** \- Alex Xu book is all you need for structure and what ML models to read about. The interviewer will ask technical questions including formula and how the particular thing actually work. So my suggest use Alex Xu ML SD book to understand the format, structure and solutions. Then google/chatgpt the technical part of each step in deep.",NumerousYam4243,1740858327.0,195,76,0.93,text
1j1crux,Any examples of GenAI in the value chain?,"Does anyone have some no-bullshit examples of how the generative part of AI has actually added value to the business?

I come across a lot of chat interfaces ... but those often are more hype and fomo than value adds. Curious if you know something serious.",FirefoxMetzger,1740871337.0,52,26,0.85,text
1j0y4xv,Influential Time-Series Forecasting Papers of 2023-2024: Part 2,"This article explores some of the latest advancements in time-series forecasting.

You can find the article¬†[here](https://aihorizonforecast.substack.com/p/influential-time-series-forecasting-8c3).

If you know of any other interesting TS papers, please share them in the comments.",nkafr,1740831156.0,106,12,0.96,text
1j0x07s,Data Science Web App Project: What Are Your Best Tips?,"I'm aiming to create a ***data science project*** that demonstrates my full skill set, including ***web app*** deployment, for my resume. I'm in search of well-structured **demo projects** that I can use as a template for my own work.



I'd also appreciate any guidance on the best tools and practices for deploying a data science project as a web app. What are the key elements that hiring managers look for in a project that's hosted online? Any suggestions on how to effectively present the project on my portfolio website and source code in GitHub profile would be greatly appreciated.",Aftabby,1740826613.0,72,33,0.85,text
1j0mkfs,Textbook Recommendations,"Because of my background in ML I was put in charge of the design and implementation of a project involving using synthetic data to make classification predictions. I am not a beginner and very comfortable with modeling in python with sklearn, pytorch, xgboost, etc and the standard process of scaling data, imputing, feature selection and running different models on hyperparameters. But I've never worked professionally doing this, only some research and kaggle projects. 

At the moment I'm wondering if anyone has any recommendations for textbooks or other documents detailing domain adaptation in the context of synthetic to real data for when the sets are not aligned 

and any on feature engineering techniques for non-time series, tabular numeric data beyond crossing, interactions, and taking summary statistics.


I feel like there's a lot I don't know but somehow I know the most where I work. So are there any intermediate to advanced resources on navigating this space?",Gravbar,1740788322.0,12,6,0.79,text
1j0lhyr,Presentation resources,I am looking for any resources helpful for creating good slide decks for presenting our work. I have seen some really fancy decks created by fellow DS at my company and I always wonder how are they creating these without any help. These folks do tend to have consulting backgrounds so could be something learnt there. Is it possible to learn this skill as it seems like good ppt skills create more impact on business stakeholders.,magooshseller,1740785290.0,6,4,0.72,text
1izmkfd,DS is becoming AI standardized junk,"Hiring is a nightmare. The majority of applicants submit the same prepackaged solutions. basic plots, default models, no validation, no business reasoning. EDA has been reduced to prewritten scripts with no anomaly detection or hypothesis testing. Modeling is just feeding data into GPT-suggested libraries, skipping feature selection, statistical reasoning, and assumption checks. Validation has become nothing more than blindly accepting default metrics. Everybody‚Äôs using AI and everything looks the same. It‚Äôs the standardization of mediocrity. Data science is turning into a low quality, copy-paste job.",KindLuis_7,1740680515.0,877,210,0.78,text
1j03efx,Fwd - NAME & SHAME: PACIFIC LIFE INSURANCE - sharing cuz reading this pissed me off. Similar experience with them last year.,,imisskobe95,1740732812.0,50,29,0.84,other
1j04aqa,Medium Blog post on EDA,"Hi all,
Started my own blog with the aim of providing guidance to beginners and reinforcing some concepts for those more experienced. 

Essentially trying to share value. Link is attached. Hope there‚Äôs something to learn for everyone. Happy to receive any critiques as well",joshamayo7,1740736876.0,37,12,0.82,link
1j029yl,"Sales forecasting advice, multiple out put","Hi All,

So I'm forecasting some sales data.  Mainly units sold.  They want a daily forecast (I tried to push them towards weekly but here we are).

I have a decades worth of data, I need to model out the effects of lockdowns obviously as well as like a bazillion campaigns they run throughout the year.

I've done some feature engineering and I've tried running it through multiple regression but that doesn't seem to work there are just so many parameters.  I computed a PCA on the input sales data and I'm feeding the lagged scores into the model which helps to reduce the number of features.

I am currently trying Gaussian Process Regression, the results are not generalizing well at all.  Definitely getting overfitting.  It gives 90% R2 and incredibly low rmse on training data, then garbage on validation.  The actual predictions do not track the real data as well at all.  Honestly was getting better just reconstruction from the previous day's PCA.  Considering doing some cross validation and hyper parameter tuning, any general advice on how to proceed?  I'm basically just throwing models at the wall to see what sticks would appreciate any advice.

",Unhappy_Technician68,1740727755.0,13,48,0.77,text
1j0eptr,AI File Convention Detection/Learning,"I have an idea for a project and trying to find some information online as this seems like something someone would have already worked on, however I'm having trouble finding anything online.  So I'm hoping someone here could point me in the direction to start learning more.

So some background.  In my job I help monitor the moving and processing of various files as they move between vendors/systems.

So for example we may a file that is generated daily named customerDataMMDDYY.rpt  where MMDDYY is the month day year.  Yet another file might have a naming convention like genericReport394MMDDYY492.csv

So what I would like to is to try and build a learning system that monitors the master data stream of file transfers that does two things

1) automatically detects naming conventions  
2) for each naming convention/pattern found in step 1, detect the ""normal"" cadence of the file movement.  For example is it 7 days a week, just week days, once a month?  
3) once 1,2 are set up, then alert if a file misses it's cadence.

Now I know how to get 2 and 3 set up.  However I'm having a hard time building a system to detect the naming conventions.  I have some ideas on how to get it done but hitting dead ends so hoping someone here might be able to offer some help.

Thanks",DanielBaldielocks,1740767519.0,0,6,0.5,text
1j0291u,question on GPT2 from scratch of Andrej Karpathy,"I was watching his video (Let's reproduce GPT-2 (124M)) where he implemented GPT-2. At around 3:15:00, it says that the initial token is the `endoftext` token. Can someone explain why that is?

Also, it seems to me that, with his code, three sentences of length 500, 524, and 2048 tokens, respectively, will fit into a (3, 1024) tensor (ignoring any excess tokens), with the first two sentences being adjacent. This would be appropriate if the three sentences come from, let's say, the same book or article; otherwise, it could be detrimental during training. Is my reasoning correct?",Alarmed-Reporter-230,1740727637.0,5,1,0.78,text
1j0ikva,Check out our AI data science tool,"Demo video: [https://youtu.be/wmbg7wH\_yUs](https://youtu.be/wmbg7wH_yUs)

Try out our beta here: [datasci.pro](https://datasci.pro/) (Note: The site isn‚Äôt optimized for mobile yet)

Our tool lets you upload datasets and interact with your data using conversational AI. You can prompt the AI to clean and preprocess data, generate visualizations, run analysis models, and create pdf reports‚Äîall while seeing the python scripts running under the hood.

We‚Äôre shipping updates daily so your feedback is greatly appreciated!",coke_and_coldbrew,1740777451.0,0,1,0.18,text
1j0abwb,How would I recreate this page (other data inputs and topics) on my Squarespace website?,"Hello All,

New Hear i have a youtube channel and social brand I'm trying to build, and I want to create pages like this:

[https://www.cnn.com/markets/fear-and-greed](https://www.cnn.com/markets/fear-and-greed)

or the data snapshots here:

[https://knowyourmeme.com/memes/loss](https://knowyourmeme.com/memes/loss)

I want to repeatedly create pages that would encompass a topic and have graphs and visuals like the above examples.

Thanks for any help or suggestions!!!",Triplebeambalancebar,1740756662.0,0,0,0.33,text
1iyr85i,How blessed/fucked-up am I?,"My manager gave me this book because I will be working on TSP and Vehicle Routing problems.

Says it's a good resource, is it really a good book for people like me ( pretty good with coding, mediocre maths skills, good in statistics and machine learning ) your typical junior data scientist.

I know I will struggle and everything, that's present in any book I ever read, but I'm pretty new to optimization and very excited about it.
But will I struggle to the extent I will find it impossible to learn something about optimization and start working?

",Careful_Engineer_700,1740586574.0,926,101,0.97,image
1iz76dr,[Unsupervised Model failure] Instagram Algorithm is Broken Every Year on Feb 26,,TheLastWhiteKid,1740629347.0,26,2,0.91,link
1iygj98,Is there a large pool of incompetent data scientists out there?,"Having moved from academia to data science in industry, I've had a strange series of interactions with other data scientists that has left me very confused about the state of the field, and I am wondering if it's just by chance or if this is a common experience? Here are a couple of examples:

I was hired to lead a small team doing data science in a large utilities company. Most senior person under me, who was referred to as the senior data scientists had no clue about anything and was actively running the team into the dust. Could barely write a for loop, couldn't use git. Took two years to get other parts of business to start trusting us. Had to push to get the individual made redundant because they were a serious liability. It was so problematic working with them I felt like they were a plant from a competitor trying to sabotage us.

Start hiring a new data scientist very recently. Lots of applicants, some with very impressive CVs, phds, experience etc. I gave a handful of them a very basic take home assessment, and the work I got back was mind boggling. The majority had no idea what they were doing, couldn't merge two data frames properly, didn't even look at the data at all by eye just printed summary stats. I was and still am flabbergasted they have high paying jobs in other places.  They would need major coaching to do basic things in my team. 

So my question is: is there a pool of ""fake"" data scientists out there muddying the job market and ruining our collective reputation, or have I just been really unlucky?",AnUncookedCabbage,1740548664.0,840,402,0.94,text
1izapxk,Have you used data heatmap in your workflows? If yes then how and what tools did you use?,"One specific use case would be:

  
\- LLM training/finetuning datasets could use heatmap to assess what records of a dataset have been mostly used across multiple models.



What else do you need data heatmap in your workflow, and did you write your own code or external tools to assess this for yourself?",metalvendetta,1740643084.0,1,8,0.53,text
1iy3eqq,Microsoft CEO Admits That AI Is Generating Basically No Value,,OverratedDataScience,1740511546.0,598,105,0.83,link
1iy6v4d,I get the impression that traditional statistical models are out-of-place with Big Data. What's the modern view on this?,"I'm a Data Scientist, but not good enough at Stats to feel confident making a statement like this one. But it seems to me that:

* Traditional statistical tests were built with the expectation that sample sizes would generally be around 20 - 30 people
* Applying them to Big Data situations where our groups consist of millions of people and reflect nearly 100% of the population is problematic

Specifically, I'm currently working on a A/B Testing project for websites, where people get different variations of a website and we measure the impact on conversion rates. Stakeholders have complained that it's very hard to reach statistical significance using the popular A/B Testing tools, like Optimizely and have tasked me with building a A/B Testing tool from scratch.

To start with the most basic possible approach, I started by running a z-test to compare the conversion rates of the variations and found that, using that approach, you can reach a statistically significant p-value with about 100 visitors. Results are about the same with chi-squared and t-tests, and you can usually get a pretty great effect size, too.

Cool -- but all of these data points are absolutely wrong. If you wait and collect weeks of data anyway, you can see that these effect sizes that were classified as statistically significant are completely incorrect.

It seems obvious to me that the fact that popular A/B Testing tools take a long time to reach statistical significance is a feature, not a flaw.

But there's a lot I don't understand here:

* What's the theory behind adjusting approaches to statistical testing when using Big Data? How are modern statisticians ensuring that these tests are more rigorous?
* What does this mean about traditional statistical approaches? If I can see, using Big Data, that my z-tests and chi-squared tests are calling inaccurate results significant when they're given small sample sizes, does this mean there are issues with these approaches in all cases?

The fact that so many modern programs are already much more rigorous than simple tests suggests that these are questions people have already identified and solved. Can anyone direct me to things I can read to better understand the issue?",takenorinvalid,1740520105.0,96,66,0.8,text
1iyn2u6,"Wan2.1 : New SOTA model for video generation, open-sourced, can run on consumer grade GPU","Alibabba group has released Wan2.1, a SOTA model series which has excelled on all benchmarks and is open-sourced. The 480P version can run on just 8GB VRAM only. Know more here : https://youtu.be/_JG80i2PaYc",mehul_gupta1997,1740575184.0,4,0,0.67,text
1iy5rks,Shitty debugging job taught me the most,"I was always a losey developer and just started working on large codebases the past year (first real job after school). I have a strong background in stats but never had to develop the ""backend"" of data intensive applications.

  
At my current job we took over a project from an outside company who was originally developing it. This was the main reason the company hired us, trying to in-house the project for cheaper than what they were charging. The job is pretty shit tbh, and I got 0 intro into the code or what we are doing. They figuratively just showed me my seat and told me to get at it.

  
I've been using a mix of AI tools to help me read through the code and help me understand what is going on in a macro level. Also when some bug comes up I let it read through the code for me to point me towards where the issue is and insert the neccesary print statements or potential modifications.

  
This excersize of ""something is constantly breaking"" is helping me to become a better data scientist in a shorter amount of time than anything else has. The job is still shit and pays like shit so I'll be switching soon, but I learned a lot by having to do this dirty work that others won't. Unfortunately, I don't think this opportunity is avaiable to someone fresh out of school in HCOL countries since they put this type of work where the labor is cheap.",LeaguePrototype,1740517387.0,44,3,0.93,text
1iy5ud3,Do you dev local or in the cloud?,"Like the question says -- by this I also think ssh'd into a stateful machine where you can basically do whatever you want counts as 'local.'

My company has tried many different things for us to have development enviornments in the cloud -- jupyter labs, aws sagemaker etc. However, I find that for the most part it's such a pain working with these system that any increase in compute speed I'd gain would be washed out by the clunkiness of these managed development systems.

I'm sure there's times when your data get's huge -- but tbh I can handle a few trillion rows locally if I batch. And my local GPU is so much easier to use than trying to download CUDA on an AWS system.

For me, just putting a requirments.txt in the rep, and using either a venv or a docker container is just so much easier and, in practice, more ""standard"" than trying to grok these complicated cloud setups. Yet it seems like every company thinks data scientists ""need"" a cloud setup. ",Any-Fig-921,1740517579.0,14,27,0.75,text
1ixy90o,Data Scientist Tasked with Building Interactive Client-Facing Product‚ÄîWhere Should I Start?,"Hi community,

I‚Äôm a data scientist with little to no experience in front-end engineering, and I‚Äôve been tasked with developing an interactive, client-facing product. My previous experience with building interactive tools has been limited to Streamlit and Plotly, but neither scales well for this use case.

I‚Äôm looking for suggestions on where to start researching technologies or frameworks that can help me create a more scalable and robust solution. Ideally, I‚Äôd like something that:

	1. Can handle larger user loads without performance issues. 	2. Is relatively accessible for someone without a front-end background.
        3.Integrates well with Python and backend services.

If you‚Äôve faced a similar challenge, what tools or frameworks did you use? Any resources (tutorials, courses, documentation) would also be much appreciated!",NoteClassic,1740498922.0,13,21,0.88,text
1ix350m,What‚Äôs the best business book you‚Äôve read?,"I came across this question on a job board. After some reflection, I realized that some of the best business books helped me understand the strategy behind the company‚Äôs growth goals, better empathizing with others, and getting them to care about impactful projects like I do.

What are some useful business-related books for a career in data science?",Symmberry,1740406898.0,250,65,0.97,text
1ix4ile,"We are back with many Data science jobs in Soccer, NFL, NHL, Formula1 and more sports! 2025","Hey guys,

I've been silent here lately but many opportunities keep appearing and being posted. 

These are a few from the last 10 days or so

* [Part-Time Data Scientist Assistant - NHL](http://www.sportsjobs.online/jobs/7449)
* [Senior Data Scientist - Epic Games](http://www.sportsjobs.online/jobs/7484)
* [Data Scientist - Atlanta United](https://www.linkedin.com/jobs/view/4158678051/)
* [Quantitative Analyst (Mid-Senior)](http://www.sportsjobs.online/jobs/7431)
* [Basketball Data Analyst - Washington Mystics](http://www.sportsjobs.online/jobs/7395)
* [Senior Data Scientist, NFL - The Score](http://www.sportsjobs.online/jobs/7365)

I run¬†www.sportsjobs(.)online, a job board in that niche. In the last month I added around 300 jobs.

For the ones that already saw my posts before, I've added more sources of jobs lately. I'm open to suggestions to prioritize the next batch.

It's a niche, there aren't thousands of jobs as in Software in general but my commitment is to¬†**keep improving a simple metric, jobs per month.**

We always need some metric in DS..

I've created also a¬†[reddit community](https://www.reddit.com/r/sports_jobs/)¬†where I post recurrently the openings if that's easier to check for you.

I hope this helps someone!",fark13,1740410548.0,115,11,0.95,text
1ixlnua,"If AI were used to evaluate employees based on self-assessments, what input might cause unintended results?",Have fun with this one. ,anecdotal_yokel,1740454738.0,10,11,0.6,text
1ix80i6,What are some good suggestions to learn route optimization and data science in supply chains?,As titled.,sonicking12,1740419118.0,30,19,0.86,text
1ixctdh,Improving Workflow: Managing Iterations Between Data Cleaning and Analysis in Jupyter Notebooks?,"I use Jupyter notebooks for projects, which typically follow a structure like this:
	1. Load Data
	2. Clean Data
	3. Analyze Data

What I find challenging is this iterative cycle:

I clean the data initially, move on to analysis, then realize during analysis that further cleaning or transformations could enhance insights. I then loop back to earlier cells, make modifications, and rerun subsequent cells.

2 ‚û°Ô∏è 3 ‚û°Ô∏è 2.1 (new cell embedded in workflow) ‚û°Ô∏è 3.1 (new cell ‚Ä¶.

This process quickly becomes convoluted and difficult to manage clearly within Jupyter notebooks. It feels messy, bouncing between sections and losing track of the logical flow.

My questions for the community:

How do you handle or structure your notebooks to efficiently manage this iterative process between data cleaning and analysis?

Are there best practices, frameworks, or notebook structuring methods you recommend to maintain clarity and readability?

Additionally, I‚Äôd appreciate book recommendations (I like books from O‚ÄôReilly) that might help me improve my workflow or overall approach to structuring analysis.

Thanks in advance‚ÄîI‚Äôm eager to learn better ways of working!",Proof_Wrap_2150,1740430751.0,14,12,0.82,text
1ix7iu9,Best books to learn Reinforcement learning?,same as title,mihirshah0101,1740417947.0,13,6,0.93,text
1ixd4jm,Amazon AS interviews starting in 2 weeks,"Hi, I was recently contacted by an Amazon recruiter. I will be interviewing for an Applied Scientist position. I am currently a DS with 5 years of experience. The problem is that the i terview process involves 1 phone screen and 1 onsite round which will have leetcode style coding. I am pretty bad at DSA. 
Can anyone please suggest me how to prepare for this part in a short duration? 
What questions to do and how to target? 
Any advice will be appreciated. TIA",MikeSpecterZane,1740431498.0,3,7,0.58,text
1iwemoj,Gym chain data scientists?,Just had a thought-any gym chain data scientists here can tell me specifically what kind of data science you‚Äôre doing? Is it advanced or still in nascency?  Was just curious since I got back into the gym after a while and was thinking of all the possibilities data science wise.,kater543,1740329827.0,53,115,0.78,text
1iwu845,"Weekly Entering & Transitioning - Thread 24 Feb, 2025 - 03 Mar, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1740373305.0,7,48,1.0,text
1ix3ymj,roast my cv,basically the title. any advice? ,ditchdweller13,1740409119.0,0,38,0.45,image
1ivsn10,Publishing a Snowflake native app to generate synthetic financial data - any interest?,,matt-ice,1740257196.0,2,5,0.59,other
1ivolsz,DeepSeek new paper : Native Sparse Attention for Long Context LLMs,Summary for DeepSeek's new paper on improved Attention mechanism (NSA) : https://youtu.be/kckft3S39_Y?si=8ZLfbFpNKTJJyZdF,mehul_gupta1997,1740246805.0,7,1,0.65,text
1ivgrnb,Are LLMs good with ML model outputs?,"The vision of my product management is to automate the root cause analysis of the system failure by deploying a multi-reasoning-steps LLM agents that have a problem to solve, and at each reasoning step are able to call one of multiple, simple ML models (get_correlations(X[1:1000], look_for_spikes(time_series(T1,...,T100)).

I mean, I guess it could work because LLMs could utilize domain specific knowledge and process hundreds of model outputs way quicker than human, while ML models would take care of numerically-intense aspects of analysis.

Does the idea make sense? Are there any successful deployments of machines of that sort? Can you recommend any papers on the topic?
",Ciasteczi,1740223865.0,13,29,0.77,text
1iv8cbv,Was the hype around DeepSeek warranted or unfounded?,"Python DA here whose upper limit is sklearn, with a bit of tensorflow.

The question: how innovative was the DeepSeek model? There is so much propaganda out there, from both sides, that‚Äôs it‚Äôs tough to understand what the net gain was.

From what I understand, DeepSeek essentially used reinforcement learning on its base model, was sucked, then trained mini-models from Llama and Qwen in a ‚Äúdistillation‚Äù methodology, and has data go thru those mini models after going thru the RL base model, and the combination of these models achieved great performance. Basically just an ensemble method. But what does ‚Äúdistilled‚Äù mean, they imported the models ie pytorch? Or they cloned the repo in full? And put data thru all models in a pipeline?

I‚Äôm also a bit unclear on the whole concept of synthetic data. To me this seems like a HUGE no no, but according to my chat with DeepSeek, they did use synthetic data.

So, was it a cheap knock off that was overhyped, or an innovative new way to architect an LLM? And what does that even mean?",SingerEast1469,1740191298.0,66,113,0.79,text
1iuw9ow,"To the avid fans of R, I respect your fight for it but honestly curious what keeps you motivated?","I started my career as an R user and loved it! Then after some years in I started looking for new roles and got the slap of reality that no one asks for R. Gradually made the switch to Python and never looked back. I have nothing against R and I still fend off unreasonable attacks on R by people who never used it calling it only good for adhoc academic analysis and bla bla. But, is it still worth fighting for?",Difficult-Big-3890,1740158571.0,349,199,0.95,text
1iun6jy,"AI isn‚Äôt evolving, it‚Äôs stagnating","AI was supposed to revolutionize intelligence, but all it‚Äôs doing is shifting us from discovery to dependency. Development has turned into a cycle of fine-tuning and API calls, just engineering.
Let‚Äôs be real, the power isn‚Äôt in the models it‚Äôs in the infrastructure. If you don‚Äôt have access to massive compute, you‚Äôre not training anything foundational. Google, OpenAI, and Microsoft own the stack, everyone else just rents it. This isn‚Äôt decentralizing intelligence it‚Äôs centralizing control.
Meanwhile, the viral hype is wearing thin. Compute costs are unsustainable, inference is slow and scaling isn‚Äôt as seamless as promised. We are deep in Amara‚Äôs Law, overestimating short-term effects and underestimating long-term ones.",KindLuis_7,1740130910.0,832,164,0.87,text
1ivavo1,Large Language Diffusion Models (LLDMs) : Diffusion for text generation,"A new architecture for LLM training is proposed called LLDMs that uses Diffusion (majorly used with image generation models ) for text generation. The first model, LLaDA 8B looks decent and is at par with Llama 8B and Qwen2.5 8B. Know more here : https://youtu.be/EdNVMx1fRiA?si=xau2ZYA1IebdmaSD",mehul_gupta1997,1740199742.0,4,0,0.83,text
1iuf85f,"What's are the top three technical skills or platforms to learn, NOT named R, Python, SQL, or any of the BI platforms (eg Tableau, PowerBI)?","E.g. Alteryx, OpenAI, etc?",jarena009,1740102146.0,122,105,0.86,text
1iuib2z,Uncensored DeepSeek-R1 by Perplexity AI,"Perplexity AI has released R1-1776, a post tuned version of DeepSeek-R1 with 0 Chinese censorship and bias. The model is free to use on perplexity AI and weights are available on Huggingface. For more info : https://youtu.be/TzNlvJlt8eg?si=SCDmfFtoThRvVpwh",mehul_gupta1997,1740111639.0,72,18,0.85,text
1iuivf9,How Would You Clean & Categorize Job Titles at Scale?,"I have a dataset with 50,000 unique job titles and want to standardize them by grouping similar titles under a common category. 

My approach is to:

1. Take the top 20% most frequently occurring titles (~500 unique).
2. Use these 500 reference titles to label and categorize the entire dataset.
3. Assign a match score to indicate how closely other job titles align with these reference titles.

I‚Äôm still working through it, but I‚Äôm curious‚Äîhow would you approach this problem? Would you use NLP, fuzzy matching, embeddings, or another method?

Any insights on handling messy job titles at scale would be appreciated!

TL;DR: I have 50k unique job titles and want to group similar ones using the top 500 most common titles as a reference set. How would you do it? Do you have any other ways of solving this?",Proof_Wrap_2150,1740113557.0,24,18,0.8,text
1itn1zg,How do you organize your files?,"In my current work I mostly do one-off scripts, data exploration, try 5 different ways to solve a problem, and do a lot of testing. My files are a hot mess. Someone asks me to do a project and I vaguely remember something similar I did a year ago that I could reuse but I cannot find it so I have to rewrite it. How do you manage your development work and ‚Äúrough drafts‚Äù before you have a final cleaned up version? 

Anything in production is on GitHub, unit tested, and all that good stuff. I‚Äôm using a windows machine with Spyder if that matters. I also have a pretty nice Linux desktop in the office that I can ssh into so that‚Äôs a whole other set of files that is not a hot mess‚Ä¶..yet.",big_data_mike,1740016276.0,68,46,0.93,text
1ito4a5,Help analyzing Profit & Loss statements across multiple years?,"Has anyone done work analyzing Profit & Loss statements across multiple years? I have several years of records but am struggling with standardizing the data. The structure of the PDFs varies, making it difficult to extract and align information consistently.

Rather than reading the files with Python, I started by manually copying and pasting data for a few years to prove a concept. I‚Äôd like to start analyzing 10+ years once I am confident I can capture the pdf data without manual intervention. I‚Äôd like to automate this process. If you‚Äôve worked on something similar, how did you handle inconsistencies in PDF formatting and structure?",Proof_Wrap_2150,1740019435.0,7,10,0.71,text
1itqkyj,help for unsupervised learning on transactions dataset.,"i have a transactions dataset and it has too much excessive info in it to detect a transactions as fraud currently we are using rules based for fraud detection but we are looking for different options a ml modle or something.... i tried a lot but couldn't get anywhere.

can u help me or give me any ideas.

i tried to generate synthetic data using ctgan no help\
did clean the data kept few columns those columns were regarding is the trans flagged or not, relatively flagged or not, history of being flagged no help\
tried dbscan, LoF, iso forest, kmeans. no help

i feel lost. ",1_plate_parcel,1740027436.0,4,16,0.67,text
1iu0skr,Build demo pipelines 100x faster,"Every time I start a new project I have to collect the data and guide clients through the first few weeks before I get some decent results to show them. This is why I created a collection of classic data science pipelines built with LLMs you can use to quickly demo any data science pipeline and even use it in production in some cases.

All of the examples are using opensource library FlashLearn that was developed for exactly this purpose.

# Examples by use case

* **Customer service**
   * [Classifying customer tickets](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Customer%20service/classify_tickets.md)
* **Finance**
   * [Parse financial report data](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Finance/parse_financial_report_data.md)
* **Marketing**
   * [Customer segmentation](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Marketing/customer_segmentation.md)
* **Personal assistant**
   * [Research assistant](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Personal%20asistant/research_assistant.md)
* **Product Intelligence**
   * [Discover trends in product\_reviews](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/discover_trends_in_prodcut%20_reviews.md)
   * [User behaviour analysis](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/user_behaviour_analysis.md)
* **Sales**
   * [Personalized cold emails](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/personalized_emails.md)
   * [Sentiment classification](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/sentiment_classification.md)

Feel free to use it and adapt it for your use cases!

P.S: The quality of the result should be 2-5% off the specialized model -> I expect this gap will close with new development. ",No_Information6299,1740064542.0,0,1,0.31,text
1it3ed9,Data Science Entrepreneur,"Anyone in this group running a consultancy or trying to build a start-up? Or even an early employee at a startup?

I feel like data science lends itself mainly to large corps and without much transferability to SMEs
",Longjumping-Will-127,1739964790.0,28,22,0.78,text
1itpmil,Upping my Generative AI game,"I'm a pretty big user of AI on a consumer level. I'd like to take a deeper dive in terms of what it could do for me in Data Science. I'm not thinking so much of becoming an expert on building LLMs but more of an expert in using them. I'd like to learn more about 
- Prompt engineering 
- API integration 
- Light overview on how LLMs work
- Custom GPTs

Can anyone suggest courses, books, YouTube videos, etc that might help me achieve that goal?",Tamalelulu,1740024207.0,0,6,0.4,text
1itr8ub,Who would contribute more to a company?,"2 fresh graduates, Graduate A and B.

Graduate A has a data science bachelors, has completed various projects and research and stays up to date with industry skills. (Internships completed too)

Graduate B has a statistics bachelors, has actively pursued academic research and applies learned skills to a startup after some projects. (No internships, but lots of self initiation)

Would Graduate A or B make the cut for the data scientist and/or ML/AI role? ",Cool-Ad-3878,1740029776.0,0,42,0.38,text
1isdrmn,I created CV copilot for Data Scientists,,Grapphie,1739888535.0,126,40,0.82,image
1is21el,Yes Business Impact Matters,"This is based on another post that said ds has lost its soul because all anyone cared about was short term ROI and they didn't understand that really good ds would be a gold mine but greedy short-term business folks ruin that.


First off let me say I used to agree when I was a junior. But now that I have 10 yoe I have the opposite opinion. I've seen so many boondoggles promise massive long-term ROI and a bunch of phds and other ds folks being paid 200k+/year would take years to develop a model that barely improved the bottom line, whereas a lookup table could get 90% of the way there and have practically no costs.


The other analogy I use is pretend you're the customer. The plumbing in your house broke and your toilets don't work. One plumber comes in and says they can fix it in a day for $200. Another comes and says they and their team needs 3 months to do a full scientific study of the toilet and your house and maximize ROI for you, because just fixing it might not be the best long-term ROI. And you need to pay them an even higher hourly than the first plumber for months of work, since they have specialized scientific skills the first plumber doesn't have. Then when you go with the first one the second one complains that you're so shortsighted and don't see the value of science and are just short-term greedy. And you're like dude I just don't want to have to piss and shit in my yard for 3 months and I don't want to pay you tens of thousands of dollars when this other guy can fix it for $200.",None,1739845645.0,205,53,0.94,text
1isi8u2,Building a Reliable Text-to-SQL Pipeline: A Step-by-Step Guide pt.2,,phicreative1997,1739899840.0,6,1,0.75,link
1irq8e0,[OC] There's far better ways to work with larger sets of data... and there's also more fun ways to overheat your computer than a massive Excel book.,,ElectrikMetriks,1739815437.0,238,33,0.93,image
1isd4tz,Time series data loading headaches? Tell us about them!,"Hi r/datascience,

I am revamping time series data loading in PyTorch and want your input!  We're working on a open-source data loader with a unified API to handle all sorts of time series data quirks ‚Äì different formats, locations, metadata, you name it.

The goal? Make your life easier when working with pytorch, forecasting, foundation models, and more. No more wrestling with Pandas, polars, or messy file formats! we are planning to expand the coverage and support all kinds of time series data formats. 

We're exploring a flexible two-layered design, but we need your help to make it truly awesome.

**Tell us about your time series data loading woes:**

* What are the biggest challenges you face?
* What formats and sources do you typically work with?
* Any specific features or situations that are a real pain?
* What would your dream time series data loader do?

Your feedback will directly shape this project, so share your thoughts and help us build something amazing!",xandie985,1739886739.0,4,1,0.6,text
1is56xt,"System design, OOPs, APIs, Security etc in Data science interviews?","System design, OOPs concepts and other things for DS interviews?

As a data scientist I know how to train a model, how to build data pipelines, how to create API and then deploy it on the server (maybe not extensively but I know how to deploy it on say EC2 with a docker etc). Also I know basics of OOPs and pretty good with solving leetcode type problems (ie optimising scripts). 

But now with a 4 years of exp, do I need to know the system design as well? That too extensive system design with everything that comes under the software pipeline? A client(a software engineer) just interviewed me for only such topics, API end points, scalability, etc. which I had zero idea about. I know only the basics of these things and feels like this isn‚Äôt something I should be looking at (as data science itself is huge to learn how am I supposed to learn entire software stack?) 

Am I right? Or I‚Äôm just living under a rock all this time? ",Amazing_Life_221,1739855830.0,21,9,0.92,text
1irs5de,What app making framework do you recommend to data scientists?,"Communicating findings from data analysis is important for people who work with data. One aspect of that is making web apps. For someone with no/little experience with web development, what app making framework would you recommend? Shiny for python/R, FastHTML, Django, Flask, or something else? And why? 

The goal is to make robust apps that work well with multiple concurrent users. Should support asynchronous operations for long running calculations.

Edit:
It seems that for simple to intermediate level complex apps, Shiny for R/Python or FastHTML are great options. The main advantage is that you can write all frontend and backend code in a single language. FastAPI authors developed FastHTML and they say it can replace FastAPI + JS frontend. So, FastHTML is probably a good option for complicated apps also.",yaymayhun,1739819907.0,68,54,0.91,text
1ish90u,Anyone do TestGorilla tests for a job app?,I recently did some technical assessments from TestGorilla. I'm wondering what other people thought of these.,Will_Tomos_Edwards,1739897496.0,1,4,0.57,text
1irr33g,How to actually apply Inferential Statistics on analyses/to help business?,"Hi guys I'm a Data analyst with like 3-4 years of experience. I feel like in my last jobs I got too relaxed and have been doing too much SQL, building dashboards, reporting and python automation without going into advanced analyses. I just got lucky and had a great job offer from a company with millions of active users. I don't want to waste this opportunity to learn and therefore am looking into more advanced topics, namely inferential statistics, to make my time here worthwhile.

As far as I know Inferential statistics should be mostly about defining hypotheses, doing statistical tests and drawing conclusions. However what I'm not sure is when/how can you make use of these tests to benefit a business.

Could you please share a case, just briefly is enough, where you used inferential/advanced statistics/analysis to help your org/business?

Any other skills a great Data analyst should have?

Thank you very much! Any comment could help me a lot!",Trungyaphets,1739817406.0,41,17,0.96,text
1irkor6,ROC vs PRC - Not what I expected,"https://preview.redd.it/eb419hrdipje1.png?width=1484&format=png&auto=webp&s=ab92bc43b20b2919a991fe6ca8ad1839f91ddf33

Interviewee started to talk about China and Taiwan when asked this question. Watch out for chatgpt abuse. ",tootieloolie,1739801465.0,84,16,0.87,text
1ir26jt,Starting a Data Consultancy,Hey everyone. Was wondering if anyone here has successfully started their own data science/analytics/governance consultancy firm before. What was the experience like and has it been worth it so far?,ParfaitRude229,1739738914.0,52,41,0.84,text
1irc50e,"Weekly Entering & Transitioning - Thread 17 Feb, 2025 - 24 Feb, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1739768504.0,11,28,1.0,text
1irlf7b,Leverage my skills,"I work in automotive as a embedded developer (C++, Python ) in sensor processing and state estimation like sensor fusion. Also started to work in edge AI. I really like to analyse signals, think about models. Its not data science per se, but i want to leverage my skills to find data science jobs.

How can i upskill? What to learn? Is my skills valuable for data science?",Huge-Leek844,1739803548.0,1,6,0.53,text
1ir3o2h,Dataflow Diagrams and Other Planning?,"Recently I have been thinking a lot about the project planning needed for good Data Science practices. Having intelligent conversations and defining clear goals is like half the battle for any job, Data Science not being an exception. 

One thing that my team has historically done towards the beginning of a project (that I quite enjoy) is to gather everyone together to discuss our Dataflow Diagrams.

For those of you who may not know what that is, here is a link: [https://www.geeksforgeeks.org/what-is-dfddata-flow-diagram/](https://www.geeksforgeeks.org/what-is-dfddata-flow-diagram/)

Some people may think that this is solely the domain of the Data Architect or Engineer (neither of which I do on an official basis), but I believe that getting the opinions of my teammates early on can reduce problems down the line. I have even incorporated this practice at the place that I volunteer at.

On to the point of this post: have any of you found the design of these quite helpful or not? What are some practices that you do to maybe improve designing these? Any other planning tips or advice to share?

P.S. I usually lurk here, so I guess it is time that I make a post. Lol!",NerdyMcDataNerd,1739742729.0,7,3,0.69,text
1iq0gwj,Data Science is losing its soul,"DS teams are starting to lose the essence that made them truly groundbreaking. their mixed scientific and business core. What we‚Äôre seeing now is a shift from deep statistical analysis and business oriented modeling to quick and dirty engineering solutions. Sure, this approach might give us a few immediate wins but it leads to low ROI projects and pulls the field further away from its true potential. One size-fits-all programming just doesn‚Äôt work. it‚Äôs not the whole game.
",KindLuis_7,1739623050.0,890,245,0.92,text
1iq9hcp,What is your daily/weekly routine if you have a WFH position?,"I'm asking this here since data science/analytics is a very remote industry. I'm honestly trying to figure out a good cadence of when to make breakfast and get coffee, when to meal prep, when to get a 15 minute walk in, when to work out, do my hobbies etc., without driving myself insane. Especially when it comes to meal prepping and cooking. When I was unemployed I was able to cook and meal prep for myself every day. I'm trying to figure out how often to cook and meal prep and grocery shop so I'm not cooking as soon as I log off. 

What is your routine for keeping up with life while you're working remotely?",lemonbottles_89,1739648599.0,62,54,0.88,text
1iq2oo0,Give clients & bosses what they want,"Every time I start a new project I have to collect the data and guide clients through the first few weeks before I get some decent results to show them. This is why I created a collection of classic data science pipelines built with LLMs you can use to quickly demo any data science pipeline and even use it in production for non-critical use cases.

# Examples by use case

* **Customer service**
   * [Classifying customer tickets](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Customer%20service/classify_tickets.md)
* **Finance**
   * [Parse financial report data](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Finance/parse_financial_report_data.md)
* **Marketing**
   * [Customer segmentation](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Marketing/customer_segmentation.md)
* **Personal assistant**
   * [Research assistant](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Personal%20asistant/research_assistant.md)
* **Product Intelligence**
   * [Discover trends in product\_reviews](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/discover_trends_in_prodcut%20_reviews.md)
   * [User behaviour analysis](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/user_behaviour_analysis.md)
* **Sales**
   * [Personalized cold emails](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/personalized_emails.md)
   * [Sentiment classification](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/sentiment_classification.md)



Feel free to use it and adapt it for your use cases! ",No_Information6299,1739630311.0,14,3,0.68,text
1iqfhq5,Most trusted sources of AI news,"What is your most trusted source of AI news?
",Queasy_Commission316,1739664782.0,0,6,0.39,text
1ios31c,What companies/industries are ‚Äúslow-paced‚Äù/low stress?,"I‚Äôve only ever worked in data science for consulting companies, which are inherently fast-paced and quite stressful. The money is good but I don‚Äôt see myself in this field forever. ‚ÄúFast-pace‚Äù in my experience can be a code word for ‚Äúburn you out‚Äù. 

Out of curiosity, do any of you have lower stress jobs in data science? My guess would be large retailers/corporations that are no longer in growth stage and just want to fine tune/maintain their production models, while also dedicating some money to R&D with more reasonable timelines",_hairyberry_,1739476942.0,222,138,0.95,text
1ipcfkm,Third-party Tools,"Hey Everyone,

Curious to other‚Äôs experiences with business teams using third-party tools?

 I keep getting asked to build dashboards and algorithms for specific processes that just get compared against third-party tools like MicroStrategy and others. We‚Äôve even had a long-standing process get transitioned out for a third-party algorithm that cost the company a few million to buy (way more than it cost in-house by like 20-30x). Even though we seem to have a large part of the same functionalities.

What‚Äôs the point of companies having internal data teams if they just compare and contrast to third-party software? So many of our team‚Äôs goals are to outdo these softwares but the business would rather trust the software instead. Super frustrating.",Ill-Ad-9823,1739545153.0,5,6,0.73,text
1ip9y2l,Looking for resources on Interrupted time series analysis,"As the title says, I am looking for sources on the topic. It can go from basics to advanced use cases. I need them both. Thanks!",chomoloc0,1739537732.0,1,5,0.56,text
1ior7cf,Mcafee data scientist,Anyone has gone through Mcafee data science coding assessment? Looking for some insights on the assessment. ,lostmillenial97531,1739474730.0,10,19,0.7,text
1ioxz48,FCC Text data?,"I'm looking to do some project(s) regarding telecommunications. Would I have to build an ""FCC_publications"" dataset from scratch? I'm not finding one on their site or others.


Also, what's the standard these days for storing/sharing a dataset like that? I can't imagine it's CSV. But is it just a zip file with folders/documents inside?",ib33,1739492790.0,3,4,0.67,text
1inytsd,AI Influencers will kill IT sector,"Tech-illiterate managers see AI-generated hype and think they need to disrupt everything: cut salaries, push impossible deadlines and replace skilled workers with AI that barely functions. 
Instead of making IT more efficient, they drive talent away, lower industry standards and create burnout cycles. The results? Worse products, more tech debt and a race to the bottom where nobody wins except investors cashing out before the crash.",KindLuis_7,1739387137.0,612,159,0.95,text
1iogppw,Data Team Benchmarks,"I put together some charts to help benchmark data teams:¬†[http://databenchmarks.com/](http://databenchmarks.com/)

For example

* Average data team size as % of the company (hint: 3%)
* Median salary across data roles for 500 job postings in Europe
* Distribution of analytics engineers, data engineers, and analysts
* The data-to-engineer ratio at top tech companies

The data comes from LinkedIn, open job boards, and a few other sources.",Different_Eggplant97,1739444855.0,6,1,0.75,text
1iobbu2,What Are the Common Challenges Businesses Face in LLM Training and Inference?,"Hi everyone, I‚Äôm relatively new to the AI field and currently exploring the world of LLMs. I‚Äôm curious to know what are the main challenges businesses face when it comes to training and deploying LLMs, as I‚Äôd like to understand the challenges beginners like me might encounter.

Are there specific difficulties in terms of data processing or model performance during inference? What are the key obstacles you‚Äôve encountered that could be helpful for someone starting out in this field to be aware of?

Any insights would be greatly appreciated! Thanks in advance!",jameslee2295,1739421908.0,5,12,0.59,text
1iolgcd,Is Managing Unstructured Data a Pain Point for the AI/RAG Ecosystem? Can It Be Solved by Well-Designed Software?,"Hey Redditors,

I've been brainstorming about a software solution that could potentially address a significant gap in the AI-enhanced information retrieval systems, particularly in the realm of Retrieval-Augmented Generation (RAG). While these systems have advanced considerably, there's still a major production challenge: managing the real-time validity, updates, and deletion of documents forming the knowledge base.

Currently, teams need to appoint managers to oversee the governance of these unstructured data, similar to how structured databases like SQL are managed. This is a complex task that requires dedicated jobs and suitable tools.

Here's my idea: develop a unified user interface (UI) specifically for document ingestion, advanced data management, and transformation into synchronized vector databases. The final product would serve as a single access point per document base, allowing clients to perform semantic searches using their AI agents. The UI would encourage data managers to keep their information up-to-date through features like notifications, email alerts, and document expiration dates.

The project could start as open-source, with a potential revenue model involving a paid service to deploy AI agents connected to the document base.

Some technical challenges include ensuring the accuracy of embeddings and dealing with chunking strategies for document processing. As technology advances, these hurdles might lessen, shifting the focus to the quality and relevance of the source document base.

Do you think a well-designed software solution could genuinely add value to this industry? Would love to hear your thoughts, experiences, and any suggestions you might have.

Do you know any existing open source software ?

Looking forward to your insights!",Weird_ftr,1739460206.0,0,8,0.14,text
1inofnb,Kimi k-1.5 (o1 level reasoning LLM) Free API,"So Moonshot AI just released free API for Kimi k-1.5, a reasoning multimodal LLM which even beat OpenAI o1 on some benchmarks. The Free API gives access to 20 Million tokens. Check out how to generate : https://youtu.be/BJxKa__2w6Y?si=X9pkH8RsQhxjJeCR",mehul_gupta1997,1739358307.0,17,0,0.71,text
1inl1gw,Challenges with Real-time Inference at Scale,"Hello! We‚Äôre implementing an AI chatbot that supports real-time customer interactions, but the inference time of our LLM becomes a bottleneck under heavy user traffic. Even with GPU-backed infrastructure, the scaling costs are climbing quickly. Has anyone optimized LLMs for high-throughput applications or found any company provides platforms/services that handle this efficiently? Would love to hear about approaches to reduce latency without sacrificing quality.",jameslee2295,1739342927.0,7,13,0.7,text
1imkowl,Evaluating the thinking process of reasoning LLMs,"So I tried using Deepseek R1 for a classification task. Turns out it is awful. Still, my boss wants me to evaluate it's thinking process and he has now told me to search for ways to do so.

I tried looking on arxiv and google but did not manage to find anything about evaluating the reasoning process of these models on subjective tasks.

What else can I do here?",AdministrativeRub484,1739230742.0,23,22,0.76,text
1imf5q9,"Takehomes, how do you approach them and how to get better?","As the title says, I have about 1 year of data science experience, mostly as junior DS. My previous work consisted of month long ML projects so I am familiar with how to get each step done (cleaning, modeling, feature engineering etc.). However, I always feel like with take homes my approach is just bad. I spent about 15 hours (normally 6-10 seems to is expected afail), but then the model is absolute shit. If I were to break it down, I would say 10 hours on pandas wizardry of cleaning data, EDA (basic plots) and feature engineering, 5 on modeling, usually I try several models and end up with one that works best. HOWEVER, when I say best I do not mean it works well, it almost always behaved like shit, even something good like random forest with few features is typically giving bad predictions in most metrics. So the question is, if anyone has good examples / tutorials on how the process should look like, I would appreciate",neural_net_ork,1739216896.0,30,22,0.87,text
1im9ipe,Building an app. Help,"I work as a data analyst. I have been asked to create an app that can be used by employees to track general updates in the company. The app must be able to be accessed on employees mobile phones. The app needs to be separate to any work login information, ideally using a personal phone number to gain access or a code.

I tried using power apps but that requires login through Microsoft.

I've never built an app before I was wondering if anyone knew any low code applications to use to built it and if not any other relatively simple application to use? Thanks.",Careful-Ingenuity674,1739203389.0,12,33,0.66,text
1ilyfhk,"Weekly Entering & Transitioning - Thread 10 Feb, 2025 - 17 Feb, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1739163703.0,7,61,0.77,text
1ilb54i,Effort/Time needed for Data Science not recognized/valued,"I conduct many data analysis projects to improve processes and overall performance at my company. I am not employed as a data analyst or data scientist but fill the job as manager for a manufacturing area. 

I have the issue that top management just asks for analysis or insights but seems not to be aware of the effort and time I need to conduct these things. To gather all data, preprocess them, make the analysis, and then process the findings to nice visuals for them. 

Often it seems they think it takes one to two hours for an analysis although I need several days. 

I struggle because I feel they do not appreciate my work or recognize how much effort it takes; besides the knowledge and skills I have to put in to conduct the analysis. 

Is anyone else experiencing the same situation or have an idea how I can address this? 

",cognitivebehavior,1739093515.0,182,30,0.98,text
1ikgq0p,Data Analysis on AI Agent Token Flow,"Does anyone know of a particular tool or library that can simulate agent system before actually calling LLMs or APIs? Something that I can find the distribution of token generation by a tool or agent or the number of calls to a certain function by LLM etc., any thoughts?",FullStackAI-Alta,1738995013.0,6,3,0.72,text
1ik6xjh,What happens in managerial interviews?,"[I posted a few](https://www.reddit.com/r/datascience/s/x8WL1G8JRa) days ago that I had a technical meeting that I crushed.
The next one I'd be speaking with the senior SWE manager and the director, each are 30 minutes, referred that they will need to know about my skills and qualifications and for me to ask any questions I may have.

I'll read about the company and its industry and products and I'll come up with good questions I know but, I fall short in identifying what skills they are interested in knowing? Didn't they get the sense from the technical one?

Maybe there's something they need to know about my soft skills and work ethics or how much impact my projects had in my current and past jobs.

The job is for a Data Scientist 2.

Thanks.",Careful_Engineer_700,1738965467.0,14,9,0.78,text
1ijzgvg,PerpetualBooster outperformed AutoGluon on 10 out of 10 classification tasks,"PerpetualBooster is a GBM but behaves like AutoML so it is benchmarked against AutoGluon (v1.2, best quality preset), the current leader in [AutoML benchmark](https://automlbenchmark.streamlit.app/cd_diagram). Top 10 datasets with the most number of rows are selected from [OpenML datasets](https://www.openml.org/) for classification tasks. 

The results are summarized in the following table:

| OpenML Task | Perpetual Training Duration | Perpetual Inference Duration | Perpetual AUC | AutoGluon Training Duration | AutoGluon Inference Duration | AutoGluon AUC |
| -------------------------------------------------------- | ------- | ------ | ------------------- | -------- | ------ | ------------------ |
| [BNG(spambase)](https://www.openml.org/t/146163)         | 70.1    | 2.1   | 0.671  | 73.1     | 3.7    | 0.669              |
| [BNG(trains)](https://www.openml.org/t/208)              | 89.5    | 1.7   |  0.996 | 106.4    | 2.4    | 0.994              |
| [breast](https://www.openml.org/t/361942)                | 13699.3 | 97.7  |  0.991  | 13330.7  | 79.7   | 0.949              |
| [Click_prediction_small](https://www.openml.org/t/7291)  | 89.1    | 1.0   |  0.749  | 101.0    | 2.8    | 0.703              |
| [colon](https://www.openml.org/t/361938)                 | 12435.2 | 126.7 |  0.997  | 12356.2  | 152.3  | 0.997              |
| [Higgs](https://www.openml.org/t/362113)                 | 3485.3  | 40.9  |  0.843  | 3501.4   | 67.9   | 0.816              |
| [SEA(50000)](https://www.openml.org/t/230)               | 21.9    | 0.2   |  0.936  | 25.6     | 0.5    | 0.935              |
| [sf-police-incidents](https://www.openml.org/t/359994)   | 85.8    | 1.5   |  0.687  | 99.4     | 2.8    | 0.659              |
| [bates_classif_100](https://www.openml.org/t/361941)     | 11152.8 | 50.0  |  0.864  | OOM      | OOM    | OOM                |
| [prostate](https://www.openml.org/t/361945)              | 13699.9 | 79.8  |  0.987  | OOM      | OOM    | OOM                |
| average                                                  | 3747.0  | 34.0  | -                  | 3699.2   | 39.0   | -                  |

PerpetualBooster outperformed AutoGluon on 10 out of 10 classification tasks, training equally fast and inferring 1.1x faster. 

PerpetualBooster demonstrates greater robustness compared to AutoGluon, successfully training on all 10 tasks, whereas AutoGluon encountered out-of-memory errors on 2 of those tasks.

Github: https://github.com/perpetual-ml/perpetual",mutlu_simsek,1738946868.0,37,4,0.93,text
1ijs9gs,[UPDATE] Use LLMs like scikit-learn,"A week ago I posted that I created a very simple Python Open-source lib that allows you to integrate LLMs in your existing data science workflows.

I got a lot of DMs asking for some more real use cases in order for you to understand **HOW** and **WHEN** to use LLMs. This is why I created 10 more or less real examples split by use case/industry to get your brains going.

# Examples by use case

* **Customer service**
   * [Classifying customer tickets](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Customer%20service/classify_tickets.md)
* **Finance**
   * [Parse financial report data](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Finance/parse_financial_report_data.md)
* **Marketing**
   * [Customer segmentation](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Marketing/customer_segmentation.md)
* **Personal assistant**
   * [Research assistant](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Personal%20asistant/research_assistant.md)
* **Product intelligence**
   * [Discover trends in product\_reviews](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/discover_trends_in_prodcut%20_reviews.md)
   * [User behaviour analysis](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Product%20intelligence/user_behaviour_analysis.md)
* **Sales**
   * [Personalized cold emails](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/personalized_emails.md)
   * [Sentiment classification](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Sales/sentiment_classification.md)
* **Software development**
   * [Automated PR reviews](https://github.com/Pravko-Solutions/FlashLearn/blob/main/examples/Software%20development/automated_pr_reviews.md)

  
I really hope that this examples will help you deliver your solutions faster! If you have any questions feel free to ask!",No_Information6299,1738925131.0,15,10,0.62,text
1ijji6f,Looking for PyTorch practice sources,"The textbook tutorials are good to develop a basic understanding, but I want to be able to practice using PyTorch with multiple problems that use the same concept, with well-explained step-by-step solutions. Does anyone have a good source for this?

Datalemur does this well for their SQL tutorial.",None,1738892254.0,49,13,0.94,text
1ijptot,Anyone use uplift models?,How is your experience with uplift models? Are they easy to train and be used? Any tips and tricks? Do you re-train the model often? How do you decide if uplift model needs to be retrained?,stevofolife,1738914463.0,10,9,0.82,text
1ij5jp8,"Have anyone recently interviewed for Meta's Data Scientist, Product Analytics position?","I was recently contacted by a recruiter from Meta for the Data Scientist, Product Analytics (Ph.D.) position. I was told that the technical screening will be 45 minutes long and cover four areas:

1. Programming
2. Research Design
3. Determining Goals and Success Metrics
4. Data Analysis

I was surprised that all four topics could fit into a 45-minute since I always thought even two topics would be a lot for that time. This makes me wonder if areas 2, 3, and 4 might be combined into a single product-sense question with one big business case study.

Also, I‚Äôm curious‚Äîdoes this format apply to all candidates for the Data Scientist, Product Analytics roles, or is it specific to candidates with doctoral degrees?

If anyone has any idea about this, I‚Äôd really appreciate it if you could share your experience. Thanks in advance!",PhotographFormal8593,1738856769.0,174,94,0.91,text
1ijfjh6,What does prompt engineering entail in a Data Scientist role?,"I've seen postings for LLM-focused roles asking for experience with prompt engineering. I've fine-tuned LLMs, worked with transformers, and interfaced with LLM APIs, but what would prompt engineering entail in a DS role?",galactictock,1738881187.0,34,26,0.8,text
1ijfonh,Storing LLM/Chatbot Conversations On Cloud,"Hey, I was wondering if anyone has any recommendations for storing conversations from chatbot interactions on the cloud for downstream analytics. Currently I use postgres but the varying length of conversation and long bodies of text seem really inefficient. Any ideas for better approaches?",MenArePigs69,1738881563.0,2,6,0.75,text
1iid6zv,"Data Science Skills, Help Me Fill the Gaps!","I‚Äôm putting together a Data Science Knowledge Map to track key skills across different areas like Machine Learning, Deep Learning, Statistics, Cloud Computing, and Autonomy/RL. The goal is to make a structured roadmap for learning and improvement.

You can check it out here: https://docs.google.com/spreadsheets/d/1laRz9aftuN-kTjUZNHBbr6-igrDCAP1wFQxdw6fX7vY/edit

My goal is to make it general purpose so you can focus on skillset categories that are most useful to you. 

Would love your feedback. Are there any skills or topics you think should be added? Also, if you have great resources for any of these areas, feel free to share!",baileyarzate,1738770942.0,146,34,0.96,text
1iibksg,How do you all quantify the revenue impact of your work product?,"I'm (mostly) an academic so pardon my cluelessness.

A lot of the advice given on here as to how to write an effective resume for industry roles revolves around quantifying the revenue impact of the projects you and your team undertook in your current role. In that, it is not enough to simply discuss technical impact (increased accuracy of predictions, improved quality of data etc) but the impact a project had on a firm's bottom line.

But it seems to me that quantifying the \*causal\* impact of an ML system, or some other standard data science project, is itself a data science project. In fact, one could hire a data scientist (or economist) whose sole job is to audit the effectiveness of data science projects in a firm.  I bet you aren't running diff-in-diffs or estimating production functions, to actually ascertain revenue impact. So how are you guys figuring it out?",Ok_Composer_1761,1738766793.0,73,64,0.96,text
1iicldl,"Advice on Building Live Odds Model (ETL Pipeline, Database, Predictive Modeling, API)","I'm working on a side project right now that is designed to be a plugin for a Rocket League mod called BakkesMod that will calculate and display live odds win odds for each team to the player. These will be calculated by taking live player/team stats obtained through the BakkesMod API, sending them to a custom API that accepts the inputs, runs them as variables through predictive models, and returns the odds to the frontend. I have some questions about the architecture/infrastructure that would best be suited. Keep in mind that this is a personal side project so the scale is not massive, but I'd still like it to be fairly thorough and robust.

# Data Pipeline:

My idea is to obtain json data from [Ballchasing.com](http://ballchasing.com/) through their API from the last thirty days to produce relevant models (I don't want data from 2021 to have weight in predicting gameplay in 2025). My ETL pipeline doesn't need to be immediately up-to-date, so I figured I'd automate it to run weekly.

From here, I'd store this data in both AWS S3 and a PostgreSQL database. The S3 bucket will house parquet files assembled from the flattened json data that is received straight from Ballchasing to be used for longer term data analysis and comparison. Storing in S3 Infrequent Access (IA) would be $0.0125/GB and converting it to the Glacier Flexible Retrieval type in S3 after a certain amount of time with a lifecycle rule would be $0.0036/GB. I estimate that a single day's worth of Parquet files would be maybe 20MB, so if I wanted to keep, let's say 90 days worth of data in IA and the rest in Glacier Flexible, that would only be $0.0225 for IA (1.8GB) and I wouldn't reach $0.10/mo in Glacier Flexible costs until 3.8 years worth of data past 90 days old (~27.78GB). Obviously there are costs associated with data requests, but with the small amount of requests I'll be triggering, it's effectively negligible.

As for the Postgres DB, I plan on hosting it on AWS RDS. I will only ever retain the last thirty days worth of data. This means that every weekly run would remove the oldest seven days of data and populate with the newest seven days of data. Overall, I estimate a single day's worth of SQL data being about 25-30 MB, making my total maybe around 750-900 MB. Either way, it's safe to say I'm not looking to store a monumental amount of data.

During data extraction, each group of data entries for a specific day will be transformed to prepare it for loading into the Postgres DB (30 day retention) and writing to parquet files to be stored in S3 (IA -> Glacier Flexible). Afterwards, I'll perform EDA on the cleaned data with Polars to determine things like weights of different stats related to winning matches and what type of modeling library I should use (scikit-learn, PyTorch, XGBoost).

# API:

After developing models for different ranks and game modes, I'd serve them through a gRPC API written in Go. The goal is to be able to just send relevant stats to the API, insert them as variables in the models, and return odds back to the frontend. I have not decided where to store these models yet (S3?).

I doubt it would be necessary, but I did think about using Kafka to stream these results because that's a technology I haven't gotten to really use that interests me, and I feel it may be applicable here (albeit probably not necessary).

# Automation:

As I said earlier, I plan on this pipeline being run weekly. Whether that includes EDA and iterative updates to the models is something I will encounter in the future, but for now, I'd be fine with those steps being manual. I don't foresee my data pipeline being too overwhelming for AWS Lambda, so I think I'll go with that. If it ends up taking too long to run there, I could just run it on an EC2 instance that is turned on/off before/after the pipeline is scheduled to run. I've never used CloudWatch, but I'm of the assumption that I can use that to automate these runs on Lambda. I can conduct basic CI/CD through GitHub actions.

# Frontend

The frontend will not have to be hosted anywhere because it's facilitated through Rocket League as a plugin. It's a simple text display and the in-game live stats will be gathered using BakkesMod's API.

# Questions:

* Does anything seem ridiculous, overkill, or not enough for my purposes? Have I made any mistakes in my choices of technologies and tools?
* What recommendations would you give me for this architecture/infrastructure
* What should I use to transform and prep the data for load into S3/Postgres
* What would be the best service to store my predictive models?
* Is it reasonable to include Kafka in this project to get experience with it even though it's probably not necessary?

Thanks for any help!

Edit 1: Revised data pipeline section to better clarify the storage of Parquet files for long-term storage opposed to raw JSON.",FreddieKiroh,1738769404.0,10,6,0.86,text
1ihnvjz,Side Projects,"What are your side projects?

For me I have a betting model I‚Äôve been working on from time to time over the past few years. Currently profitable in backtesting, but too risky to put money into. It‚Äôs been a fun way to practice things like ranking models and web scraping which I don‚Äôt get much exposure to at work. Also could make money with it one day which is cool. I‚Äôm wondering what other people are doing for fun on the side. Feel free to share.",Fit-Employee-4393,1738691938.0,100,61,0.92,text
1ihl43y,"For a take-home performance project that's meant to take 2 hours, would you actually stay under 2 hours?","I've completed a take home project for an analyst role I'm applying for. The project asked that I spend no more than 2 hours to complete the task, and that it's okay if not all questions are answered, as they want to get a sense of my data story telling skills. But they also gave me a week to turn this in. 

I've finished and I spent way more than 2 hours on this, as I feel like in this job market, I shouldn't take the risk of turning in a sloppier take home task. I've looked around and seen that others who were given 2 hour take homes also spent way more time on their tasks as well. It just feels like common sense to use all the time I was actually given, especially since other candidates are going to do so as well, but I'm worried that a hiring manager and recruiter might look at this and think ""They obviously spent more than 2 hours"". ",lemonbottles_89,1738685182.0,119,67,0.9,text
1ii5swa,XI (Œæ) Correlation Coefficient in Postgres,,Florents,1738745293.0,3,0,0.67,link
1ihsbrs,ML System Design Mock,"I have ML system design interview coming up and wanted to see if anyone here has website, group,discord or want to mock together?",NumerousYam4243,1738702758.0,3,5,0.64,text
1ih7qk4,Guidance for New Professionals,"Hey everyone, I worked at this company last summer and I am coming back as a graduate in March as a Data Scientist.

Altough the title is Data Scientist, projects with actual modelling are rare. The focus is more on BI, and creating new solutions for the company in its different operations.

I worked there and liked the people and environment but I really aim to stand out, to try and give my best, to learn the most.

I would love to get some tips and experiences from you guys, thanks!",Most_Panic_2955,1738637535.0,44,14,0.89,text
1igo4dh,What areas does synthetic data generation has usecases?,"There are synthetic data generation libraries from tools such as Ragas, and I‚Äôve heard some even use it for model training. What are the actual use case examples of using synthetic data generation?",metalvendetta,1738586328.0,81,54,0.91,text
1igpi6w,TabPFN v2: A pretrained transformer outperforms existing SOTA for small tabular data and outperforms Chronos for time-series,"Have any of you tried TabPFN v2? It is a pretrained transformer which outperforms existing SOTA for small tabular data. You can read it in üîó [**Nature**](https://www.nature.com/articles/s41586-024-08328-6).

Some key highlights:

* It outperforms an ensemble of strong baselines tuned for 4 hours in 2.8 seconds for classification and 4.8 seconds for regression tasks, for datasets up to 10,000 samples and 500 features
* It is robust to uninformative features and can natively handle numerical and categorical features as well as missing values.
* Pretrained on 130 million synthetically generated datasets, it is a generative transformer model which allows for fine-tuning, data generation and density estimation.
* TabPFN v2 performs as well with half the data as the next best baseline (CatBoost) with all the data.
* TabPFN v2 can be used for forecasting by featurizing the timestamps. It ranks #1 on the popular time-series GIFT-Eval [benchmark](https://huggingface.co/spaces/Salesforce/GIFT-Eval) and outperforms Chronos.

TabPFN v2 is available under an [open license](https://github.com/PriorLabs/TabPFN): a derivative of the Apache 2 license with a single modification, adding an enhanced attribution requirement inspired by the Llama 3 license. You can also try it via [API](https://github.com/PriorLabs/tabpfn-client).",rsesrsfh,1738590662.0,20,8,0.85,text
1igpsrg,"About data processing, data science, tiger style and assertions","I recently came across a video in youtube mentioning this [tiger coding style](https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/TIGER_STYLE.md) and the assertions part is quite interesting.

> Assertions detect programmer errors. Unlike operating errors, which are expected and which must be handled, assertion failures are unexpected. The only correct way to handle corrupt code is to crash. Assertions downgrade catastrophic correctness bugs into liveness bugs. Assertions are a force multiplier for discovering bugs by fuzzing.

This style only reinforces that the practice that I already used to is relevant in other fields and I try to use that as much as I can BUT it seems to be only plausible to use for metadata and function parameters, and not the actual data we work with. I say that because if the dataset is large enough, then any assertion would take a lot of time and slow the actual program execution.

**Should I do a lot of assertions that reduce performance or should I ignore the need for error detection and not use any assertions during data processing?**

Do you do anything similar to this? How would you approach this performance / error detection trade-off? Is there any middle ground that could be found?",Silent-Sunset,1738591497.0,5,2,0.73,text
1ighhad,"Weekly Entering & Transitioning - Thread 03 Feb, 2025 - 10 Feb, 2025"," 

Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:

* Learning resources (e.g. books, tutorials, videos)
* Traditional education (e.g. schools, degrees, electives)
* Alternative education (e.g. online courses, bootcamps)
* Job search questions (e.g. resumes, applying, career prospects)
* Elementary questions (e.g. where to start, what next)

While you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and Resources pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).",AutoModerator,1738558904.0,10,39,0.92,text
1ifub7j,[AI Tools] What AI Tools do you use as a copilot when working on your data science coding?,There are coding platforms like v0 and cursor that are very helpful for doing frontend/backend related coding work. What's the one you use for data science?,limedove,1738491384.0,66,35,0.84,text
1ig62ea,"any one here built a recommender system before , i need help understanding the architecture","I am building a RS based on a Neo4j database 

I struggle with the how the data should flow between the database, recommender system and the website 

I did some research and what i arrived on is that i should make the RS as an API to post the recommendations to the website 

but i really struggle to understand how the backend of the project work ",Emotional-Rhubarb725,1738526340.0,3,11,0.56,text
